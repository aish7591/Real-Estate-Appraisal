{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ZDoeuUgq0t1"
   },
   "source": [
    "#Step 0 - Setup the learning environment\n",
    "\n",
    "To begin, we import some library modules and functions that we will use.   \n",
    "\n",
    "Numpy is a collection of math functions including various matrix operations - see http://www.numpy.org/.   Matplotlib is a 2D plotting library  - see https://matplotlib.org/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MucjMNbIhdgD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The environment is ready.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "mlp_bp_ann.py\n",
    "\n",
    "Backpropagation tutorial using a two layer ANN, without\n",
    "the aid of additional NN libraries. \n",
    "This two-layer ANN has one layer of hidden nodes that allows the model to develop\n",
    "non-linear functions\n",
    "\n",
    "The program uses  Numpy for faster matrix operations and \n",
    "Matplotlib for plotting the training error per epoch\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore')\n",
    "\n",
    "print(\"The environment is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rO-ApF4yzD4A"
   },
   "source": [
    "### Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DWLUN</th>\n",
       "      <th>RDOS</th>\n",
       "      <th>YRBLT</th>\n",
       "      <th>TOTFIXT</th>\n",
       "      <th>HEATING</th>\n",
       "      <th>WBFPSTK</th>\n",
       "      <th>BMNTGAR</th>\n",
       "      <th>ATTFRGAR</th>\n",
       "      <th>TOTLIVAR</th>\n",
       "      <th>DECKOFP</th>\n",
       "      <th>ENCLPOR</th>\n",
       "      <th>NBHDGRP</th>\n",
       "      <th>RECROOM</th>\n",
       "      <th>FINBSMT</th>\n",
       "      <th>GRADE</th>\n",
       "      <th>CDU</th>\n",
       "      <th>TOTOBY</th>\n",
       "      <th>SALEPRIC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1900</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1098</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>103000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1900</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2112</td>\n",
       "      <td>232</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.08</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>162000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1959</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1110</td>\n",
       "      <td>20</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>288</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4</td>\n",
       "      <td>420</td>\n",
       "      <td>160500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>1910</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1634</td>\n",
       "      <td>120</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.08</td>\n",
       "      <td>4</td>\n",
       "      <td>5530</td>\n",
       "      <td>170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1900</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1808</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1955</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1290</td>\n",
       "      <td>72</td>\n",
       "      <td>174</td>\n",
       "      <td>1</td>\n",
       "      <td>216</td>\n",
       "      <td>0</td>\n",
       "      <td>1.08</td>\n",
       "      <td>4</td>\n",
       "      <td>13020</td>\n",
       "      <td>160500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1900</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1886</td>\n",
       "      <td>24</td>\n",
       "      <td>84</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4</td>\n",
       "      <td>3250</td>\n",
       "      <td>175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>1930</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2300</td>\n",
       "      <td>346</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>1885</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2936</td>\n",
       "      <td>247</td>\n",
       "      <td>184</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.08</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>236000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1967</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1430</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.08</td>\n",
       "      <td>4</td>\n",
       "      <td>2770</td>\n",
       "      <td>155000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     DWLUN  RDOS  YRBLT  TOTFIXT  HEATING WBFPSTK  BMNTGAR  ATTFRGAR  \\\n",
       "0        1    21   1900        5        2      no        0         0   \n",
       "1        2     0   1900       10        2      no        0         0   \n",
       "2        1     1   1959        7        2      no        0         0   \n",
       "3        2    19   1910       10        2      no        0         0   \n",
       "4        1    10   1900        8        2      no        0         0   \n",
       "..     ...   ...    ...      ...      ...     ...      ...       ...   \n",
       "185      1    15   1955        7        2     yes        0         0   \n",
       "186      1    17   1900        5        2      no        0         0   \n",
       "187      2    19   1930       12        2      no        0         0   \n",
       "188      3    20   1885       15        2      no        0         0   \n",
       "189      1    17   1967        7        2      no        0         0   \n",
       "\n",
       "     TOTLIVAR  DECKOFP  ENCLPOR  NBHDGRP  RECROOM  FINBSMT  GRADE  CDU  \\\n",
       "0        1098       58        0        2        0        0   0.92    5   \n",
       "1        2112      232       72        1        0        0   1.08    4   \n",
       "2        1110       20       77        1      288        0   1.00    4   \n",
       "3        1634      120       98        1        0        0   1.08    4   \n",
       "4        1808        0        0        1        0        0   1.00    4   \n",
       "..        ...      ...      ...      ...      ...      ...    ...  ...   \n",
       "185      1290       72      174        1      216        0   1.08    4   \n",
       "186      1886       24       84        1        0        0   1.00    4   \n",
       "187      2300      346        0        1        0        0   1.00    4   \n",
       "188      2936      247      184        1        0        0   1.08    4   \n",
       "189      1430        0        0        1        0        0   1.08    4   \n",
       "\n",
       "     TOTOBY  SALEPRIC   \n",
       "0         0     103000  \n",
       "1         0     162000  \n",
       "2       420     160500  \n",
       "3      5530     170000  \n",
       "4         0     170000  \n",
       "..      ...        ...  \n",
       "185   13020     160500  \n",
       "186    3250     175000  \n",
       "187       0     200000  \n",
       "188       0     236000  \n",
       "189    2770     155000  \n",
       "\n",
       "[190 rows x 18 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('RealEstateLabelled.csv',header=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       no\n",
       "1       no\n",
       "2       no\n",
       "3       no\n",
       "4       no\n",
       "      ... \n",
       "185    yes\n",
       "186     no\n",
       "187     no\n",
       "188     no\n",
       "189     no\n",
       "Name: WBFPSTK, Length: 190, dtype: object"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.WBFPSTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "185    1\n",
      "186    0\n",
      "187    0\n",
      "188    0\n",
      "189    0\n",
      "Name: WBFPSTK, Length: 190, dtype: object\n",
      "     DWLUN  RDOS  YRBLT  TOTFIXT  HEATING WBFPSTK  BMNTGAR  ATTFRGAR  \\\n",
      "0        1    21   1900        5        2       0        0         0   \n",
      "1        2     0   1900       10        2       0        0         0   \n",
      "2        1     1   1959        7        2       0        0         0   \n",
      "3        2    19   1910       10        2       0        0         0   \n",
      "4        1    10   1900        8        2       0        0         0   \n",
      "..     ...   ...    ...      ...      ...     ...      ...       ...   \n",
      "185      1    15   1955        7        2       1        0         0   \n",
      "186      1    17   1900        5        2       0        0         0   \n",
      "187      2    19   1930       12        2       0        0         0   \n",
      "188      3    20   1885       15        2       0        0         0   \n",
      "189      1    17   1967        7        2       0        0         0   \n",
      "\n",
      "     TOTLIVAR  DECKOFP  ENCLPOR  NBHDGRP  RECROOM  FINBSMT  GRADE  CDU  \\\n",
      "0        1098       58        0        2        0        0   0.92    5   \n",
      "1        2112      232       72        1        0        0   1.08    4   \n",
      "2        1110       20       77        1      288        0   1.00    4   \n",
      "3        1634      120       98        1        0        0   1.08    4   \n",
      "4        1808        0        0        1        0        0   1.00    4   \n",
      "..        ...      ...      ...      ...      ...      ...    ...  ...   \n",
      "185      1290       72      174        1      216        0   1.08    4   \n",
      "186      1886       24       84        1        0        0   1.00    4   \n",
      "187      2300      346        0        1        0        0   1.00    4   \n",
      "188      2936      247      184        1        0        0   1.08    4   \n",
      "189      1430        0        0        1        0        0   1.08    4   \n",
      "\n",
      "     TOTOBY  SALEPRIC   \n",
      "0         0     103000  \n",
      "1         0     162000  \n",
      "2       420     160500  \n",
      "3      5530     170000  \n",
      "4         0     170000  \n",
      "..      ...        ...  \n",
      "185   13020     160500  \n",
      "186    3250     175000  \n",
      "187       0     200000  \n",
      "188       0     236000  \n",
      "189    2770     155000  \n",
      "\n",
      "[190 rows x 18 columns]\n"
     ]
    }
   ],
   "source": [
    "length_wood = len(df.WBFPSTK)\n",
    "\n",
    "for y in range(length_wood):\n",
    "        if df.WBFPSTK[y] == 'yes':\n",
    "             df.WBFPSTK[y] = 1\n",
    "        else: \n",
    "             df.WBFPSTK[y] = 0\n",
    "        \n",
    "print(df.WBFPSTK) \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "RealEstate = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 21, 1900, ..., 5, 0, 103000],\n",
       "       [2, 0, 1900, ..., 4, 0, 162000],\n",
       "       [1, 1, 1959, ..., 4, 420, 160500],\n",
       "       ...,\n",
       "       [2, 19, 1930, ..., 4, 0, 200000],\n",
       "       [3, 20, 1885, ..., 4, 0, 236000],\n",
       "       [1, 17, 1967, ..., 4, 2770, 155000]], dtype=object)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RealEstate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = RealEstate[:,0:17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = RealEstate[:,17] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 21, 1900, ..., 0.92, 5, 0],\n",
       "       [2, 0, 1900, ..., 1.08, 4, 0],\n",
       "       [1, 1, 1959, ..., 1.0, 4, 420],\n",
       "       ...,\n",
       "       [2, 19, 1930, ..., 1.0, 4, 0],\n",
       "       [3, 20, 1885, ..., 1.08, 4, 0],\n",
       "       [1, 17, 1967, ..., 1.08, 4, 2770]], dtype=object)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([103000, 162000, 160500, 170000, 170000, 145000, 182000, 165000,\n",
       "       240000, 160000, 154000, 151000, 161000, 220000, 202500, 137000,\n",
       "       135000, 198000, 175000, 160000, 200000, 158000, 125000, 170000,\n",
       "       205000, 155000, 145000, 250000, 155000, 128000, 128800, 175000,\n",
       "       170000, 155000, 135000, 180000, 190000, 170000, 205000, 160000,\n",
       "       178000, 140000, 172000, 175000, 132000, 160000, 136000, 155000,\n",
       "       155000, 153000, 180000, 139900, 205000, 146000, 146000, 147000,\n",
       "       155000, 160000, 162000, 175000, 180000, 178000, 130000, 195000,\n",
       "       185000, 130000, 185000, 162500, 117500, 202500, 150000, 160000,\n",
       "       145000, 145575, 179900, 142000, 165000, 180000, 212500, 145000,\n",
       "       140000, 165000, 165000, 165000, 142000, 125000, 157000, 155000,\n",
       "       166000, 162500, 215000, 170000, 160000, 120000, 206750, 177000,\n",
       "       172000, 155000, 160000, 155000, 190000, 180000, 173000, 150800,\n",
       "       125000, 134000, 220000, 140000, 172000, 166666, 206000, 140000,\n",
       "       150000, 187000, 187000, 175000, 225000, 157000, 150000, 162000,\n",
       "       174500, 220000, 168000, 155000, 223000, 179900, 232000, 225000,\n",
       "       136000, 178000, 145000, 170000, 200000, 155000, 230000, 128000,\n",
       "       115000, 193000, 150000, 187900, 192000, 118000, 155000, 190000,\n",
       "       155000, 136500, 153000, 147500, 185000, 157500, 208000, 183500,\n",
       "       228000, 140000, 175000, 150000, 199900, 239900, 210000, 230000,\n",
       "       230000, 121000, 156000, 195000, 235000, 220000, 171000, 187000,\n",
       "       141000, 195000, 155000, 222000, 149500, 165900, 155000, 140000,\n",
       "       138500, 155000, 169000, 200000, 186500, 200000, 173000, 120000,\n",
       "       200000, 160500, 175000, 200000, 236000, 155000], dtype=object)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the target variable into binary variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 0 0 1 1\n",
      " 1 1 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 0 0\n",
      " 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 1 1 0 0 0 1 0 1 0 1\n",
      " 0 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0\n",
      " 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1\n",
      " 0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "length = len(target)\n",
    "\n",
    "for x in range(length):\n",
    "    if target[x] >= 170000:\n",
    "        target[x] = 1\n",
    "    else: \n",
    "        target[x] = 0\n",
    "        \n",
    "target = np.array(target, dtype='int32')\n",
    "print(target)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "data_in = min_max_scaler.fit_transform(data_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(133, 17) (28, 17) (29, 17) (133,) (28,) (29,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(data_in, target, test_size=0.3)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)\n",
    "print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.7826087 , 0.36764706, ..., 0.65217391, 0.5       ,\n",
       "        0.        ],\n",
       "       [0.        , 0.26086957, 0.36764706, ..., 1.        , 1.        ,\n",
       "        0.        ],\n",
       "       [0.5       , 0.08695652, 1.        , ..., 1.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [1.        , 0.39130435, 0.36764706, ..., 0.65217391, 0.5       ,\n",
       "        0.        ],\n",
       "       [0.        , 0.47826087, 0.36764706, ..., 1.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.56521739, 0.82352941, ..., 1.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.array([[1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1],\n",
    " [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1],\n",
    " [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1],\n",
    " [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133, 1)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        , 0.7826087 , ..., 0.65217391, 0.5       ,\n",
       "        0.        ],\n",
       "       [1.        , 0.        , 0.26086957, ..., 1.        , 1.        ,\n",
       "        0.        ],\n",
       "       [1.        , 0.5       , 0.08695652, ..., 1.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [1.        , 1.        , 0.39130435, ..., 0.65217391, 0.5       ,\n",
       "        0.        ],\n",
       "       [1.        , 0.        , 0.47826087, ..., 1.        , 0.        ,\n",
       "        0.        ],\n",
       "       [1.        , 0.        , 0.56521739, ..., 1.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = (np.column_stack((Z,X_train)))\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133, 18)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,\n",
       "       1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "       0])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model using training data partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N2Kea7YlfRMA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outout node initial weights:\n",
      "[[-6.40050979e-02 -4.74073768e-01  4.96624779e-02 -6.46776074e-02\n",
      "  -7.96321979e-02 -1.69665179e-01 -2.95351366e-01  1.19270966e-01\n",
      "  -2.00345326e-01 -2.33172725e-01  1.21133833e-01  2.91420943e-02\n",
      "  -3.65420055e-01  1.35781213e-02 -3.15560134e-01  2.85335148e-01\n",
      "   3.53975293e-01 -5.76316262e-03  3.46561485e-01 -4.20354523e-01\n",
      "   5.24609012e-03 -4.34713496e-01 -7.18776724e-02 -4.03469084e-01\n",
      "  -3.72840028e-01  9.67453090e-02 -2.73987999e-01 -3.93054316e-01\n",
      "  -2.79693793e-01 -1.50173715e-01 -3.22125154e-02 -2.98256774e-01\n",
      "   1.40406725e-01 -1.69301644e-02  5.23672002e-03 -1.13107349e-01\n",
      "   2.93637454e-01  8.00041789e-02 -3.37701401e-01  2.00752347e-01\n",
      "   4.64551080e-01  8.36117022e-06  3.89520064e-01 -1.58386347e-01\n",
      "   6.71441276e-02 -7.24540367e-02 -6.32527370e-02  2.76559185e-01\n",
      "   3.56041735e-02  4.53742227e-01  4.42081601e-02 -4.17905078e-01\n",
      "  -1.33657598e-01  3.50850504e-01 -9.37249570e-02 -4.72797634e-01\n",
      "  -2.52822761e-01 -4.32855629e-01  4.93852011e-01  4.70580313e-01\n",
      "   3.00258351e-01  1.01817121e-01  2.64959860e-01 -3.30774553e-01\n",
      "  -2.06976768e-01  2.40668753e-02 -1.43375719e-01 -4.54321035e-01\n",
      "   4.83153445e-01 -5.86450806e-02  4.00043938e-03 -1.76458682e-01\n",
      "  -2.40255247e-01 -1.13110115e-01  3.32016900e-01  2.36747056e-01\n",
      "  -1.20789433e-01 -4.86982663e-01  2.97404939e-01 -2.30611202e-01\n",
      "   8.26848885e-02 -4.74449058e-01  1.62202019e-01 -1.12476574e-01\n",
      "  -2.92620127e-03 -8.50941626e-02 -1.49128099e-01  5.09779053e-02\n",
      "   4.72910690e-01 -3.87223785e-01 -1.86741472e-01 -4.58202290e-01\n",
      "   2.38399759e-01  1.57512388e-01 -2.85364254e-01 -8.32465598e-02\n",
      "   1.43841934e-01  1.61481327e-01 -3.29522867e-01  3.81652236e-01\n",
      "   2.78008160e-01]]\n",
      "Hidden node initial weights:\n",
      "[[-0.36604579  0.36891663  0.24877788 ...  0.12356318 -0.32328784\n",
      "   0.09125735]\n",
      " [-0.01073383  0.04790778  0.19952062 ... -0.02859014  0.04949682\n",
      "   0.34511312]\n",
      " [ 0.4885097  -0.45113191 -0.26788175 ... -0.14286507  0.08978199\n",
      "  -0.44777727]\n",
      " ...\n",
      " [ 0.10317594  0.13978223  0.32183481 ... -0.2783459   0.03616054\n",
      "  -0.36133889]\n",
      " [-0.14693846 -0.14242811 -0.09744914 ... -0.10083784  0.03872644\n",
      "  -0.26230271]\n",
      " [-0.24670952 -0.42437322  0.04092962 ... -0.3882126   0.36502514\n",
      "  -0.04152997]]\n"
     ]
    }
   ],
   "source": [
    "# Set the learing parameters\n",
    "learning_rate = 0.01\n",
    "momemtum = 0.5\n",
    "num_epochs = 20000\n",
    "min_val_loss = np.inf\n",
    "\n",
    "# Set the dimensions of the input, hidden, and output layers\n",
    "size_in = 17\n",
    "size_hidden = 100  # Can be anything - larger will take longer\n",
    "size_out = target.ndim\n",
    "\n",
    "# Set the seed value of the random number generator\n",
    "random_seed = 2\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Initialize the network weights, and place to store previous epoch weights\n",
    "weight_out = np.random.rand(size_out, size_hidden + 1) - 0.5\n",
    "weight_hidden = np.random.rand(size_hidden, size_in + 1) - 0.5\n",
    "\n",
    "weight_hidden_prev = np.zeros(weight_hidden.shape)\n",
    "weight_out_prev = np.zeros(weight_out.shape)\n",
    "\n",
    "weight_hidden_val = np.zeros(weight_hidden.shape)\n",
    "weight_out_val = np.zeros(weight_out.shape)\n",
    "\n",
    "# Initialize a vector to store the train errors for each epoch\n",
    "error_log = np.zeros([num_epochs])\n",
    "errorv_log = np.zeros([num_epochs])\n",
    "\n",
    "print(\"Outout node initial weights:\")\n",
    "print(weight_out)\n",
    "print(\"Hidden node initial weights:\")\n",
    "print(weight_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 18)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133, 18)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = X_train.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 133)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,\n",
       "       1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "       0])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = Y_train\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133,)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_val= np.array([[1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = (np.column_stack((Z_val,X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 18)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_val.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aUsIPYqke-Lf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Training MSE: 0.1489739\n",
      "Iter: 0, Validation MSE: 0.0966812\n",
      "Iter: 50, Training MSE: 0.0539326\n",
      "Iter: 50, Validation MSE: 0.0367307\n",
      "Iter: 100, Training MSE: 0.0468385\n",
      "Iter: 100, Validation MSE: 0.0291946\n",
      "Iter: 150, Training MSE: 0.0438693\n",
      "Iter: 150, Validation MSE: 0.0270832\n",
      "Iter: 200, Training MSE: 0.0418978\n",
      "Iter: 200, Validation MSE: 0.0264756\n",
      "Iter: 250, Training MSE: 0.0403502\n",
      "Iter: 250, Validation MSE: 0.0266521\n",
      "Iter: 300, Training MSE: 0.0390482\n",
      "Iter: 300, Validation MSE: 0.0273884\n",
      "Iter: 350, Training MSE: 0.0379207\n",
      "Iter: 350, Validation MSE: 0.0285374\n",
      "Iter: 400, Training MSE: 0.0369312\n",
      "Iter: 400, Validation MSE: 0.0299447\n",
      "Iter: 450, Training MSE: 0.0360550\n",
      "Iter: 450, Validation MSE: 0.0314592\n",
      "Iter: 500, Training MSE: 0.0352739\n",
      "Iter: 500, Validation MSE: 0.0329582\n",
      "Iter: 550, Training MSE: 0.0345742\n",
      "Iter: 550, Validation MSE: 0.0343579\n",
      "Iter: 600, Training MSE: 0.0339455\n",
      "Iter: 600, Validation MSE: 0.0356119\n",
      "Iter: 650, Training MSE: 0.0333795\n",
      "Iter: 650, Validation MSE: 0.0367034\n",
      "Iter: 700, Training MSE: 0.0328693\n",
      "Iter: 700, Validation MSE: 0.0376357\n",
      "Iter: 750, Training MSE: 0.0324090\n",
      "Iter: 750, Validation MSE: 0.0384242\n",
      "Iter: 800, Training MSE: 0.0319931\n",
      "Iter: 800, Validation MSE: 0.0390895\n",
      "Iter: 850, Training MSE: 0.0316166\n",
      "Iter: 850, Validation MSE: 0.0396532\n",
      "Iter: 900, Training MSE: 0.0312752\n",
      "Iter: 900, Validation MSE: 0.0401352\n",
      "Iter: 950, Training MSE: 0.0309646\n",
      "Iter: 950, Validation MSE: 0.0405531\n",
      "Iter: 1000, Training MSE: 0.0306810\n",
      "Iter: 1000, Validation MSE: 0.0409217\n",
      "Iter: 1050, Training MSE: 0.0304211\n",
      "Iter: 1050, Validation MSE: 0.0412528\n",
      "Iter: 1100, Training MSE: 0.0301819\n",
      "Iter: 1100, Validation MSE: 0.0415563\n",
      "Iter: 1150, Training MSE: 0.0299605\n",
      "Iter: 1150, Validation MSE: 0.0418398\n",
      "Iter: 1200, Training MSE: 0.0297545\n",
      "Iter: 1200, Validation MSE: 0.0421096\n",
      "Iter: 1250, Training MSE: 0.0295617\n",
      "Iter: 1250, Validation MSE: 0.0423704\n",
      "Iter: 1300, Training MSE: 0.0293802\n",
      "Iter: 1300, Validation MSE: 0.0426261\n",
      "Iter: 1350, Training MSE: 0.0292082\n",
      "Iter: 1350, Validation MSE: 0.0428795\n",
      "Iter: 1400, Training MSE: 0.0290443\n",
      "Iter: 1400, Validation MSE: 0.0431328\n",
      "Iter: 1450, Training MSE: 0.0288873\n",
      "Iter: 1450, Validation MSE: 0.0433872\n",
      "Iter: 1500, Training MSE: 0.0287358\n",
      "Iter: 1500, Validation MSE: 0.0436433\n",
      "Iter: 1550, Training MSE: 0.0285893\n",
      "Iter: 1550, Validation MSE: 0.0439010\n",
      "Iter: 1600, Training MSE: 0.0284468\n",
      "Iter: 1600, Validation MSE: 0.0441592\n",
      "Iter: 1650, Training MSE: 0.0283081\n",
      "Iter: 1650, Validation MSE: 0.0444162\n",
      "Iter: 1700, Training MSE: 0.0281728\n",
      "Iter: 1700, Validation MSE: 0.0446695\n",
      "Iter: 1750, Training MSE: 0.0280409\n",
      "Iter: 1750, Validation MSE: 0.0449159\n",
      "Iter: 1800, Training MSE: 0.0279125\n",
      "Iter: 1800, Validation MSE: 0.0451524\n",
      "Iter: 1850, Training MSE: 0.0277877\n",
      "Iter: 1850, Validation MSE: 0.0453755\n",
      "Iter: 1900, Training MSE: 0.0276668\n",
      "Iter: 1900, Validation MSE: 0.0455826\n",
      "Iter: 1950, Training MSE: 0.0275500\n",
      "Iter: 1950, Validation MSE: 0.0457716\n",
      "Iter: 2000, Training MSE: 0.0274373\n",
      "Iter: 2000, Validation MSE: 0.0459414\n",
      "Iter: 2050, Training MSE: 0.0273289\n",
      "Iter: 2050, Validation MSE: 0.0460916\n",
      "Iter: 2100, Training MSE: 0.0272248\n",
      "Iter: 2100, Validation MSE: 0.0462228\n",
      "Iter: 2150, Training MSE: 0.0271249\n",
      "Iter: 2150, Validation MSE: 0.0463358\n",
      "Iter: 2200, Training MSE: 0.0270290\n",
      "Iter: 2200, Validation MSE: 0.0464321\n",
      "Iter: 2250, Training MSE: 0.0269370\n",
      "Iter: 2250, Validation MSE: 0.0465133\n",
      "Iter: 2300, Training MSE: 0.0268488\n",
      "Iter: 2300, Validation MSE: 0.0465810\n",
      "Iter: 2350, Training MSE: 0.0267641\n",
      "Iter: 2350, Validation MSE: 0.0466368\n",
      "Iter: 2400, Training MSE: 0.0266827\n",
      "Iter: 2400, Validation MSE: 0.0466820\n",
      "Iter: 2450, Training MSE: 0.0266044\n",
      "Iter: 2450, Validation MSE: 0.0467180\n",
      "Iter: 2500, Training MSE: 0.0265290\n",
      "Iter: 2500, Validation MSE: 0.0467460\n",
      "Iter: 2550, Training MSE: 0.0264563\n",
      "Iter: 2550, Validation MSE: 0.0467669\n",
      "Iter: 2600, Training MSE: 0.0263862\n",
      "Iter: 2600, Validation MSE: 0.0467817\n",
      "Iter: 2650, Training MSE: 0.0263184\n",
      "Iter: 2650, Validation MSE: 0.0467910\n",
      "Iter: 2700, Training MSE: 0.0262527\n",
      "Iter: 2700, Validation MSE: 0.0467957\n",
      "Iter: 2750, Training MSE: 0.0261890\n",
      "Iter: 2750, Validation MSE: 0.0467962\n",
      "Iter: 2800, Training MSE: 0.0261272\n",
      "Iter: 2800, Validation MSE: 0.0467930\n",
      "Iter: 2850, Training MSE: 0.0260671\n",
      "Iter: 2850, Validation MSE: 0.0467867\n",
      "Iter: 2900, Training MSE: 0.0260086\n",
      "Iter: 2900, Validation MSE: 0.0467775\n",
      "Iter: 2950, Training MSE: 0.0259516\n",
      "Iter: 2950, Validation MSE: 0.0467659\n",
      "Iter: 3000, Training MSE: 0.0258959\n",
      "Iter: 3000, Validation MSE: 0.0467521\n",
      "Iter: 3050, Training MSE: 0.0258415\n",
      "Iter: 3050, Validation MSE: 0.0467365\n",
      "Iter: 3100, Training MSE: 0.0257882\n",
      "Iter: 3100, Validation MSE: 0.0467192\n",
      "Iter: 3150, Training MSE: 0.0257360\n",
      "Iter: 3150, Validation MSE: 0.0467005\n",
      "Iter: 3200, Training MSE: 0.0256848\n",
      "Iter: 3200, Validation MSE: 0.0466805\n",
      "Iter: 3250, Training MSE: 0.0256345\n",
      "Iter: 3250, Validation MSE: 0.0466596\n",
      "Iter: 3300, Training MSE: 0.0255851\n",
      "Iter: 3300, Validation MSE: 0.0466377\n",
      "Iter: 3350, Training MSE: 0.0255364\n",
      "Iter: 3350, Validation MSE: 0.0466151\n",
      "Iter: 3400, Training MSE: 0.0254884\n",
      "Iter: 3400, Validation MSE: 0.0465919\n",
      "Iter: 3450, Training MSE: 0.0254411\n",
      "Iter: 3450, Validation MSE: 0.0465681\n",
      "Iter: 3500, Training MSE: 0.0253945\n",
      "Iter: 3500, Validation MSE: 0.0465440\n",
      "Iter: 3550, Training MSE: 0.0253483\n",
      "Iter: 3550, Validation MSE: 0.0465196\n",
      "Iter: 3600, Training MSE: 0.0253027\n",
      "Iter: 3600, Validation MSE: 0.0464950\n",
      "Iter: 3650, Training MSE: 0.0252576\n",
      "Iter: 3650, Validation MSE: 0.0464702\n",
      "Iter: 3700, Training MSE: 0.0252129\n",
      "Iter: 3700, Validation MSE: 0.0464453\n",
      "Iter: 3750, Training MSE: 0.0251686\n",
      "Iter: 3750, Validation MSE: 0.0464205\n",
      "Iter: 3800, Training MSE: 0.0251247\n",
      "Iter: 3800, Validation MSE: 0.0463956\n",
      "Iter: 3850, Training MSE: 0.0250812\n",
      "Iter: 3850, Validation MSE: 0.0463709\n",
      "Iter: 3900, Training MSE: 0.0250379\n",
      "Iter: 3900, Validation MSE: 0.0463463\n",
      "Iter: 3950, Training MSE: 0.0249950\n",
      "Iter: 3950, Validation MSE: 0.0463218\n",
      "Iter: 4000, Training MSE: 0.0249523\n",
      "Iter: 4000, Validation MSE: 0.0462975\n",
      "Iter: 4050, Training MSE: 0.0249098\n",
      "Iter: 4050, Validation MSE: 0.0462734\n",
      "Iter: 4100, Training MSE: 0.0248676\n",
      "Iter: 4100, Validation MSE: 0.0462495\n",
      "Iter: 4150, Training MSE: 0.0248255\n",
      "Iter: 4150, Validation MSE: 0.0462258\n",
      "Iter: 4200, Training MSE: 0.0247837\n",
      "Iter: 4200, Validation MSE: 0.0462024\n",
      "Iter: 4250, Training MSE: 0.0247420\n",
      "Iter: 4250, Validation MSE: 0.0461793\n",
      "Iter: 4300, Training MSE: 0.0247004\n",
      "Iter: 4300, Validation MSE: 0.0461564\n",
      "Iter: 4350, Training MSE: 0.0246589\n",
      "Iter: 4350, Validation MSE: 0.0461338\n",
      "Iter: 4400, Training MSE: 0.0246176\n",
      "Iter: 4400, Validation MSE: 0.0461115\n",
      "Iter: 4450, Training MSE: 0.0245763\n",
      "Iter: 4450, Validation MSE: 0.0460894\n",
      "Iter: 4500, Training MSE: 0.0245352\n",
      "Iter: 4500, Validation MSE: 0.0460675\n",
      "Iter: 4550, Training MSE: 0.0244940\n",
      "Iter: 4550, Validation MSE: 0.0460459\n",
      "Iter: 4600, Training MSE: 0.0244530\n",
      "Iter: 4600, Validation MSE: 0.0460246\n",
      "Iter: 4650, Training MSE: 0.0244119\n",
      "Iter: 4650, Validation MSE: 0.0460035\n",
      "Iter: 4700, Training MSE: 0.0243709\n",
      "Iter: 4700, Validation MSE: 0.0459827\n",
      "Iter: 4750, Training MSE: 0.0243300\n",
      "Iter: 4750, Validation MSE: 0.0459620\n",
      "Iter: 4800, Training MSE: 0.0242890\n",
      "Iter: 4800, Validation MSE: 0.0459416\n",
      "Iter: 4850, Training MSE: 0.0242480\n",
      "Iter: 4850, Validation MSE: 0.0459213\n",
      "Iter: 4900, Training MSE: 0.0242069\n",
      "Iter: 4900, Validation MSE: 0.0459013\n",
      "Iter: 4950, Training MSE: 0.0241659\n",
      "Iter: 4950, Validation MSE: 0.0458814\n",
      "Iter: 5000, Training MSE: 0.0241248\n",
      "Iter: 5000, Validation MSE: 0.0458617\n",
      "Iter: 5050, Training MSE: 0.0240837\n",
      "Iter: 5050, Validation MSE: 0.0458422\n",
      "Iter: 5100, Training MSE: 0.0240424\n",
      "Iter: 5100, Validation MSE: 0.0458228\n",
      "Iter: 5150, Training MSE: 0.0240012\n",
      "Iter: 5150, Validation MSE: 0.0458035\n",
      "Iter: 5200, Training MSE: 0.0239598\n",
      "Iter: 5200, Validation MSE: 0.0457843\n",
      "Iter: 5250, Training MSE: 0.0239184\n",
      "Iter: 5250, Validation MSE: 0.0457653\n",
      "Iter: 5300, Training MSE: 0.0238768\n",
      "Iter: 5300, Validation MSE: 0.0457463\n",
      "Iter: 5350, Training MSE: 0.0238352\n",
      "Iter: 5350, Validation MSE: 0.0457275\n",
      "Iter: 5400, Training MSE: 0.0237934\n",
      "Iter: 5400, Validation MSE: 0.0457087\n",
      "Iter: 5450, Training MSE: 0.0237515\n",
      "Iter: 5450, Validation MSE: 0.0456900\n",
      "Iter: 5500, Training MSE: 0.0237095\n",
      "Iter: 5500, Validation MSE: 0.0456713\n",
      "Iter: 5550, Training MSE: 0.0236674\n",
      "Iter: 5550, Validation MSE: 0.0456527\n",
      "Iter: 5600, Training MSE: 0.0236251\n",
      "Iter: 5600, Validation MSE: 0.0456342\n",
      "Iter: 5650, Training MSE: 0.0235826\n",
      "Iter: 5650, Validation MSE: 0.0456157\n",
      "Iter: 5700, Training MSE: 0.0235400\n",
      "Iter: 5700, Validation MSE: 0.0455972\n",
      "Iter: 5750, Training MSE: 0.0234972\n",
      "Iter: 5750, Validation MSE: 0.0455787\n",
      "Iter: 5800, Training MSE: 0.0234543\n",
      "Iter: 5800, Validation MSE: 0.0455602\n",
      "Iter: 5850, Training MSE: 0.0234111\n",
      "Iter: 5850, Validation MSE: 0.0455418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 5900, Training MSE: 0.0233678\n",
      "Iter: 5900, Validation MSE: 0.0455234\n",
      "Iter: 5950, Training MSE: 0.0233242\n",
      "Iter: 5950, Validation MSE: 0.0455049\n",
      "Iter: 6000, Training MSE: 0.0232805\n",
      "Iter: 6000, Validation MSE: 0.0454865\n",
      "Iter: 6050, Training MSE: 0.0232365\n",
      "Iter: 6050, Validation MSE: 0.0454681\n",
      "Iter: 6100, Training MSE: 0.0231923\n",
      "Iter: 6100, Validation MSE: 0.0454496\n",
      "Iter: 6150, Training MSE: 0.0231479\n",
      "Iter: 6150, Validation MSE: 0.0454312\n",
      "Iter: 6200, Training MSE: 0.0231032\n",
      "Iter: 6200, Validation MSE: 0.0454127\n",
      "Iter: 6250, Training MSE: 0.0230583\n",
      "Iter: 6250, Validation MSE: 0.0453942\n",
      "Iter: 6300, Training MSE: 0.0230131\n",
      "Iter: 6300, Validation MSE: 0.0453757\n",
      "Iter: 6350, Training MSE: 0.0229677\n",
      "Iter: 6350, Validation MSE: 0.0453572\n",
      "Iter: 6400, Training MSE: 0.0229220\n",
      "Iter: 6400, Validation MSE: 0.0453387\n",
      "Iter: 6450, Training MSE: 0.0228760\n",
      "Iter: 6450, Validation MSE: 0.0453201\n",
      "Iter: 6500, Training MSE: 0.0228298\n",
      "Iter: 6500, Validation MSE: 0.0453016\n",
      "Iter: 6550, Training MSE: 0.0227833\n",
      "Iter: 6550, Validation MSE: 0.0452830\n",
      "Iter: 6600, Training MSE: 0.0227364\n",
      "Iter: 6600, Validation MSE: 0.0452644\n",
      "Iter: 6650, Training MSE: 0.0226893\n",
      "Iter: 6650, Validation MSE: 0.0452458\n",
      "Iter: 6700, Training MSE: 0.0226418\n",
      "Iter: 6700, Validation MSE: 0.0452272\n",
      "Iter: 6750, Training MSE: 0.0225941\n",
      "Iter: 6750, Validation MSE: 0.0452086\n",
      "Iter: 6800, Training MSE: 0.0225460\n",
      "Iter: 6800, Validation MSE: 0.0451900\n",
      "Iter: 6850, Training MSE: 0.0224975\n",
      "Iter: 6850, Validation MSE: 0.0451714\n",
      "Iter: 6900, Training MSE: 0.0224488\n",
      "Iter: 6900, Validation MSE: 0.0451528\n",
      "Iter: 6950, Training MSE: 0.0223997\n",
      "Iter: 6950, Validation MSE: 0.0451342\n",
      "Iter: 7000, Training MSE: 0.0223503\n",
      "Iter: 7000, Validation MSE: 0.0451156\n",
      "Iter: 7050, Training MSE: 0.0223005\n",
      "Iter: 7050, Validation MSE: 0.0450970\n",
      "Iter: 7100, Training MSE: 0.0222504\n",
      "Iter: 7100, Validation MSE: 0.0450784\n",
      "Iter: 7150, Training MSE: 0.0221999\n",
      "Iter: 7150, Validation MSE: 0.0450599\n",
      "Iter: 7200, Training MSE: 0.0221490\n",
      "Iter: 7200, Validation MSE: 0.0450413\n",
      "Iter: 7250, Training MSE: 0.0220978\n",
      "Iter: 7250, Validation MSE: 0.0450228\n",
      "Iter: 7300, Training MSE: 0.0220463\n",
      "Iter: 7300, Validation MSE: 0.0450043\n",
      "Iter: 7350, Training MSE: 0.0219943\n",
      "Iter: 7350, Validation MSE: 0.0449858\n",
      "Iter: 7400, Training MSE: 0.0219420\n",
      "Iter: 7400, Validation MSE: 0.0449674\n",
      "Iter: 7450, Training MSE: 0.0218894\n",
      "Iter: 7450, Validation MSE: 0.0449490\n",
      "Iter: 7500, Training MSE: 0.0218363\n",
      "Iter: 7500, Validation MSE: 0.0449307\n",
      "Iter: 7550, Training MSE: 0.0217829\n",
      "Iter: 7550, Validation MSE: 0.0449124\n",
      "Iter: 7600, Training MSE: 0.0217292\n",
      "Iter: 7600, Validation MSE: 0.0448941\n",
      "Iter: 7650, Training MSE: 0.0216751\n",
      "Iter: 7650, Validation MSE: 0.0448759\n",
      "Iter: 7700, Training MSE: 0.0216206\n",
      "Iter: 7700, Validation MSE: 0.0448577\n",
      "Iter: 7750, Training MSE: 0.0215658\n",
      "Iter: 7750, Validation MSE: 0.0448396\n",
      "Iter: 7800, Training MSE: 0.0215106\n",
      "Iter: 7800, Validation MSE: 0.0448215\n",
      "Iter: 7850, Training MSE: 0.0214551\n",
      "Iter: 7850, Validation MSE: 0.0448035\n",
      "Iter: 7900, Training MSE: 0.0213993\n",
      "Iter: 7900, Validation MSE: 0.0447855\n",
      "Iter: 7950, Training MSE: 0.0213432\n",
      "Iter: 7950, Validation MSE: 0.0447676\n",
      "Iter: 8000, Training MSE: 0.0212867\n",
      "Iter: 8000, Validation MSE: 0.0447498\n",
      "Iter: 8050, Training MSE: 0.0212300\n",
      "Iter: 8050, Validation MSE: 0.0447320\n",
      "Iter: 8100, Training MSE: 0.0211729\n",
      "Iter: 8100, Validation MSE: 0.0447142\n",
      "Iter: 8150, Training MSE: 0.0211156\n",
      "Iter: 8150, Validation MSE: 0.0446966\n",
      "Iter: 8200, Training MSE: 0.0210580\n",
      "Iter: 8200, Validation MSE: 0.0446789\n",
      "Iter: 8250, Training MSE: 0.0210002\n",
      "Iter: 8250, Validation MSE: 0.0446614\n",
      "Iter: 8300, Training MSE: 0.0209422\n",
      "Iter: 8300, Validation MSE: 0.0446439\n",
      "Iter: 8350, Training MSE: 0.0208839\n",
      "Iter: 8350, Validation MSE: 0.0446265\n",
      "Iter: 8400, Training MSE: 0.0208255\n",
      "Iter: 8400, Validation MSE: 0.0446091\n",
      "Iter: 8450, Training MSE: 0.0207668\n",
      "Iter: 8450, Validation MSE: 0.0445918\n",
      "Iter: 8500, Training MSE: 0.0207081\n",
      "Iter: 8500, Validation MSE: 0.0445745\n",
      "Iter: 8550, Training MSE: 0.0206491\n",
      "Iter: 8550, Validation MSE: 0.0445573\n",
      "Iter: 8600, Training MSE: 0.0205901\n",
      "Iter: 8600, Validation MSE: 0.0445401\n",
      "Iter: 8650, Training MSE: 0.0205310\n",
      "Iter: 8650, Validation MSE: 0.0445230\n",
      "Iter: 8700, Training MSE: 0.0204718\n",
      "Iter: 8700, Validation MSE: 0.0445060\n",
      "Iter: 8750, Training MSE: 0.0204125\n",
      "Iter: 8750, Validation MSE: 0.0444890\n",
      "Iter: 8800, Training MSE: 0.0203532\n",
      "Iter: 8800, Validation MSE: 0.0444720\n",
      "Iter: 8850, Training MSE: 0.0202940\n",
      "Iter: 8850, Validation MSE: 0.0444551\n",
      "Iter: 8900, Training MSE: 0.0202347\n",
      "Iter: 8900, Validation MSE: 0.0444383\n",
      "Iter: 8950, Training MSE: 0.0201755\n",
      "Iter: 8950, Validation MSE: 0.0444215\n",
      "Iter: 9000, Training MSE: 0.0201164\n",
      "Iter: 9000, Validation MSE: 0.0444047\n",
      "Iter: 9050, Training MSE: 0.0200573\n",
      "Iter: 9050, Validation MSE: 0.0443880\n",
      "Iter: 9100, Training MSE: 0.0199984\n",
      "Iter: 9100, Validation MSE: 0.0443713\n",
      "Iter: 9150, Training MSE: 0.0199396\n",
      "Iter: 9150, Validation MSE: 0.0443546\n",
      "Iter: 9200, Training MSE: 0.0198810\n",
      "Iter: 9200, Validation MSE: 0.0443381\n",
      "Iter: 9250, Training MSE: 0.0198226\n",
      "Iter: 9250, Validation MSE: 0.0443215\n",
      "Iter: 9300, Training MSE: 0.0197644\n",
      "Iter: 9300, Validation MSE: 0.0443050\n",
      "Iter: 9350, Training MSE: 0.0197065\n",
      "Iter: 9350, Validation MSE: 0.0442885\n",
      "Iter: 9400, Training MSE: 0.0196488\n",
      "Iter: 9400, Validation MSE: 0.0442721\n",
      "Iter: 9450, Training MSE: 0.0195913\n",
      "Iter: 9450, Validation MSE: 0.0442557\n",
      "Iter: 9500, Training MSE: 0.0195342\n",
      "Iter: 9500, Validation MSE: 0.0442394\n",
      "Iter: 9550, Training MSE: 0.0194775\n",
      "Iter: 9550, Validation MSE: 0.0442231\n",
      "Iter: 9600, Training MSE: 0.0194210\n",
      "Iter: 9600, Validation MSE: 0.0442069\n",
      "Iter: 9650, Training MSE: 0.0193649\n",
      "Iter: 9650, Validation MSE: 0.0441907\n",
      "Iter: 9700, Training MSE: 0.0193092\n",
      "Iter: 9700, Validation MSE: 0.0441746\n",
      "Iter: 9750, Training MSE: 0.0192540\n",
      "Iter: 9750, Validation MSE: 0.0441585\n",
      "Iter: 9800, Training MSE: 0.0191991\n",
      "Iter: 9800, Validation MSE: 0.0441425\n",
      "Iter: 9850, Training MSE: 0.0191447\n",
      "Iter: 9850, Validation MSE: 0.0441266\n",
      "Iter: 9900, Training MSE: 0.0190907\n",
      "Iter: 9900, Validation MSE: 0.0441107\n",
      "Iter: 9950, Training MSE: 0.0190371\n",
      "Iter: 9950, Validation MSE: 0.0440949\n",
      "Iter: 10000, Training MSE: 0.0189841\n",
      "Iter: 10000, Validation MSE: 0.0440791\n",
      "Iter: 10050, Training MSE: 0.0189316\n",
      "Iter: 10050, Validation MSE: 0.0440635\n",
      "Iter: 10100, Training MSE: 0.0188795\n",
      "Iter: 10100, Validation MSE: 0.0440479\n",
      "Iter: 10150, Training MSE: 0.0188280\n",
      "Iter: 10150, Validation MSE: 0.0440324\n",
      "Iter: 10200, Training MSE: 0.0187770\n",
      "Iter: 10200, Validation MSE: 0.0440170\n",
      "Iter: 10250, Training MSE: 0.0187265\n",
      "Iter: 10250, Validation MSE: 0.0440016\n",
      "Iter: 10300, Training MSE: 0.0186765\n",
      "Iter: 10300, Validation MSE: 0.0439864\n",
      "Iter: 10350, Training MSE: 0.0186271\n",
      "Iter: 10350, Validation MSE: 0.0439713\n",
      "Iter: 10400, Training MSE: 0.0185783\n",
      "Iter: 10400, Validation MSE: 0.0439562\n",
      "Iter: 10450, Training MSE: 0.0185300\n",
      "Iter: 10450, Validation MSE: 0.0439413\n",
      "Iter: 10500, Training MSE: 0.0184823\n",
      "Iter: 10500, Validation MSE: 0.0439265\n",
      "Iter: 10550, Training MSE: 0.0184352\n",
      "Iter: 10550, Validation MSE: 0.0439118\n",
      "Iter: 10600, Training MSE: 0.0183886\n",
      "Iter: 10600, Validation MSE: 0.0438973\n",
      "Iter: 10650, Training MSE: 0.0183426\n",
      "Iter: 10650, Validation MSE: 0.0438828\n",
      "Iter: 10700, Training MSE: 0.0182972\n",
      "Iter: 10700, Validation MSE: 0.0438685\n",
      "Iter: 10750, Training MSE: 0.0182524\n",
      "Iter: 10750, Validation MSE: 0.0438543\n",
      "Iter: 10800, Training MSE: 0.0182082\n",
      "Iter: 10800, Validation MSE: 0.0438402\n",
      "Iter: 10850, Training MSE: 0.0181645\n",
      "Iter: 10850, Validation MSE: 0.0438263\n",
      "Iter: 10900, Training MSE: 0.0181214\n",
      "Iter: 10900, Validation MSE: 0.0438125\n",
      "Iter: 10950, Training MSE: 0.0180789\n",
      "Iter: 10950, Validation MSE: 0.0437989\n",
      "Iter: 11000, Training MSE: 0.0180370\n",
      "Iter: 11000, Validation MSE: 0.0437854\n",
      "Iter: 11050, Training MSE: 0.0179957\n",
      "Iter: 11050, Validation MSE: 0.0437720\n",
      "Iter: 11100, Training MSE: 0.0179549\n",
      "Iter: 11100, Validation MSE: 0.0437588\n",
      "Iter: 11150, Training MSE: 0.0179147\n",
      "Iter: 11150, Validation MSE: 0.0437458\n",
      "Iter: 11200, Training MSE: 0.0178751\n",
      "Iter: 11200, Validation MSE: 0.0437329\n",
      "Iter: 11250, Training MSE: 0.0178361\n",
      "Iter: 11250, Validation MSE: 0.0437202\n",
      "Iter: 11300, Training MSE: 0.0177976\n",
      "Iter: 11300, Validation MSE: 0.0437076\n",
      "Iter: 11350, Training MSE: 0.0177597\n",
      "Iter: 11350, Validation MSE: 0.0436951\n",
      "Iter: 11400, Training MSE: 0.0177224\n",
      "Iter: 11400, Validation MSE: 0.0436829\n",
      "Iter: 11450, Training MSE: 0.0176856\n",
      "Iter: 11450, Validation MSE: 0.0436708\n",
      "Iter: 11500, Training MSE: 0.0176494\n",
      "Iter: 11500, Validation MSE: 0.0436588\n",
      "Iter: 11550, Training MSE: 0.0176137\n",
      "Iter: 11550, Validation MSE: 0.0436470\n",
      "Iter: 11600, Training MSE: 0.0175785\n",
      "Iter: 11600, Validation MSE: 0.0436354\n",
      "Iter: 11650, Training MSE: 0.0175439\n",
      "Iter: 11650, Validation MSE: 0.0436240\n",
      "Iter: 11700, Training MSE: 0.0175098\n",
      "Iter: 11700, Validation MSE: 0.0436126\n",
      "Iter: 11750, Training MSE: 0.0174763\n",
      "Iter: 11750, Validation MSE: 0.0436015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 11800, Training MSE: 0.0174433\n",
      "Iter: 11800, Validation MSE: 0.0435905\n",
      "Iter: 11850, Training MSE: 0.0174107\n",
      "Iter: 11850, Validation MSE: 0.0435797\n",
      "Iter: 11900, Training MSE: 0.0173787\n",
      "Iter: 11900, Validation MSE: 0.0435690\n",
      "Iter: 11950, Training MSE: 0.0173472\n",
      "Iter: 11950, Validation MSE: 0.0435585\n",
      "Iter: 12000, Training MSE: 0.0173162\n",
      "Iter: 12000, Validation MSE: 0.0435481\n",
      "Iter: 12050, Training MSE: 0.0172857\n",
      "Iter: 12050, Validation MSE: 0.0435379\n",
      "Iter: 12100, Training MSE: 0.0172556\n",
      "Iter: 12100, Validation MSE: 0.0435279\n",
      "Iter: 12150, Training MSE: 0.0172261\n",
      "Iter: 12150, Validation MSE: 0.0435180\n",
      "Iter: 12200, Training MSE: 0.0171970\n",
      "Iter: 12200, Validation MSE: 0.0435082\n",
      "Iter: 12250, Training MSE: 0.0171684\n",
      "Iter: 12250, Validation MSE: 0.0434986\n",
      "Iter: 12300, Training MSE: 0.0171402\n",
      "Iter: 12300, Validation MSE: 0.0434892\n",
      "Iter: 12350, Training MSE: 0.0171125\n",
      "Iter: 12350, Validation MSE: 0.0434799\n",
      "Iter: 12400, Training MSE: 0.0170852\n",
      "Iter: 12400, Validation MSE: 0.0434707\n",
      "Iter: 12450, Training MSE: 0.0170584\n",
      "Iter: 12450, Validation MSE: 0.0434617\n",
      "Iter: 12500, Training MSE: 0.0170320\n",
      "Iter: 12500, Validation MSE: 0.0434528\n",
      "Iter: 12550, Training MSE: 0.0170061\n",
      "Iter: 12550, Validation MSE: 0.0434440\n",
      "Iter: 12600, Training MSE: 0.0169805\n",
      "Iter: 12600, Validation MSE: 0.0434354\n",
      "Iter: 12650, Training MSE: 0.0169554\n",
      "Iter: 12650, Validation MSE: 0.0434270\n",
      "Iter: 12700, Training MSE: 0.0169307\n",
      "Iter: 12700, Validation MSE: 0.0434186\n",
      "Iter: 12750, Training MSE: 0.0169064\n",
      "Iter: 12750, Validation MSE: 0.0434104\n",
      "Iter: 12800, Training MSE: 0.0168825\n",
      "Iter: 12800, Validation MSE: 0.0434024\n",
      "Iter: 12850, Training MSE: 0.0168589\n",
      "Iter: 12850, Validation MSE: 0.0433944\n",
      "Iter: 12900, Training MSE: 0.0168358\n",
      "Iter: 12900, Validation MSE: 0.0433866\n",
      "Iter: 12950, Training MSE: 0.0168130\n",
      "Iter: 12950, Validation MSE: 0.0433789\n",
      "Iter: 13000, Training MSE: 0.0167906\n",
      "Iter: 13000, Validation MSE: 0.0433713\n",
      "Iter: 13050, Training MSE: 0.0167686\n",
      "Iter: 13050, Validation MSE: 0.0433639\n",
      "Iter: 13100, Training MSE: 0.0167469\n",
      "Iter: 13100, Validation MSE: 0.0433566\n",
      "Iter: 13150, Training MSE: 0.0167256\n",
      "Iter: 13150, Validation MSE: 0.0433494\n",
      "Iter: 13200, Training MSE: 0.0167047\n",
      "Iter: 13200, Validation MSE: 0.0433423\n",
      "Iter: 13250, Training MSE: 0.0166840\n",
      "Iter: 13250, Validation MSE: 0.0433354\n",
      "Iter: 13300, Training MSE: 0.0166638\n",
      "Iter: 13300, Validation MSE: 0.0433285\n",
      "Iter: 13350, Training MSE: 0.0166438\n",
      "Iter: 13350, Validation MSE: 0.0433218\n",
      "Iter: 13400, Training MSE: 0.0166242\n",
      "Iter: 13400, Validation MSE: 0.0433152\n",
      "Iter: 13450, Training MSE: 0.0166049\n",
      "Iter: 13450, Validation MSE: 0.0433087\n",
      "Iter: 13500, Training MSE: 0.0165859\n",
      "Iter: 13500, Validation MSE: 0.0433024\n",
      "Iter: 13550, Training MSE: 0.0165672\n",
      "Iter: 13550, Validation MSE: 0.0432961\n",
      "Iter: 13600, Training MSE: 0.0165488\n",
      "Iter: 13600, Validation MSE: 0.0432899\n",
      "Iter: 13650, Training MSE: 0.0165307\n",
      "Iter: 13650, Validation MSE: 0.0432839\n",
      "Iter: 13700, Training MSE: 0.0165129\n",
      "Iter: 13700, Validation MSE: 0.0432780\n",
      "Iter: 13750, Training MSE: 0.0164954\n",
      "Iter: 13750, Validation MSE: 0.0432722\n",
      "Iter: 13800, Training MSE: 0.0164782\n",
      "Iter: 13800, Validation MSE: 0.0432665\n",
      "Iter: 13850, Training MSE: 0.0164612\n",
      "Iter: 13850, Validation MSE: 0.0432609\n",
      "Iter: 13900, Training MSE: 0.0164445\n",
      "Iter: 13900, Validation MSE: 0.0432554\n",
      "Iter: 13950, Training MSE: 0.0164281\n",
      "Iter: 13950, Validation MSE: 0.0432500\n",
      "Iter: 14000, Training MSE: 0.0164120\n",
      "Iter: 14000, Validation MSE: 0.0432447\n",
      "Iter: 14050, Training MSE: 0.0163961\n",
      "Iter: 14050, Validation MSE: 0.0432396\n",
      "Iter: 14100, Training MSE: 0.0163805\n",
      "Iter: 14100, Validation MSE: 0.0432345\n",
      "Iter: 14150, Training MSE: 0.0163651\n",
      "Iter: 14150, Validation MSE: 0.0432296\n",
      "Iter: 14200, Training MSE: 0.0163500\n",
      "Iter: 14200, Validation MSE: 0.0432247\n",
      "Iter: 14250, Training MSE: 0.0163351\n",
      "Iter: 14250, Validation MSE: 0.0432200\n",
      "Iter: 14300, Training MSE: 0.0163204\n",
      "Iter: 14300, Validation MSE: 0.0432154\n",
      "Iter: 14350, Training MSE: 0.0163060\n",
      "Iter: 14350, Validation MSE: 0.0432109\n",
      "Iter: 14400, Training MSE: 0.0162918\n",
      "Iter: 14400, Validation MSE: 0.0432065\n",
      "Iter: 14450, Training MSE: 0.0162778\n",
      "Iter: 14450, Validation MSE: 0.0432022\n",
      "Iter: 14500, Training MSE: 0.0162640\n",
      "Iter: 14500, Validation MSE: 0.0431980\n",
      "Iter: 14550, Training MSE: 0.0162505\n",
      "Iter: 14550, Validation MSE: 0.0431939\n",
      "Iter: 14600, Training MSE: 0.0162372\n",
      "Iter: 14600, Validation MSE: 0.0431899\n",
      "Iter: 14650, Training MSE: 0.0162240\n",
      "Iter: 14650, Validation MSE: 0.0431861\n",
      "Iter: 14700, Training MSE: 0.0162111\n",
      "Iter: 14700, Validation MSE: 0.0431823\n",
      "Iter: 14750, Training MSE: 0.0161984\n",
      "Iter: 14750, Validation MSE: 0.0431787\n",
      "Iter: 14800, Training MSE: 0.0161859\n",
      "Iter: 14800, Validation MSE: 0.0431751\n",
      "Iter: 14850, Training MSE: 0.0161736\n",
      "Iter: 14850, Validation MSE: 0.0431717\n",
      "Iter: 14900, Training MSE: 0.0161614\n",
      "Iter: 14900, Validation MSE: 0.0431684\n",
      "Iter: 14950, Training MSE: 0.0161495\n",
      "Iter: 14950, Validation MSE: 0.0431651\n",
      "Iter: 15000, Training MSE: 0.0161377\n",
      "Iter: 15000, Validation MSE: 0.0431620\n",
      "Iter: 15050, Training MSE: 0.0161261\n",
      "Iter: 15050, Validation MSE: 0.0431590\n",
      "Iter: 15100, Training MSE: 0.0161147\n",
      "Iter: 15100, Validation MSE: 0.0431562\n",
      "Iter: 15150, Training MSE: 0.0161034\n",
      "Iter: 15150, Validation MSE: 0.0431534\n",
      "Iter: 15200, Training MSE: 0.0160924\n",
      "Iter: 15200, Validation MSE: 0.0431507\n",
      "Iter: 15250, Training MSE: 0.0160814\n",
      "Iter: 15250, Validation MSE: 0.0431482\n",
      "Iter: 15300, Training MSE: 0.0160707\n",
      "Iter: 15300, Validation MSE: 0.0431457\n",
      "Iter: 15350, Training MSE: 0.0160601\n",
      "Iter: 15350, Validation MSE: 0.0431434\n",
      "Iter: 15400, Training MSE: 0.0160497\n",
      "Iter: 15400, Validation MSE: 0.0431412\n",
      "Iter: 15450, Training MSE: 0.0160394\n",
      "Iter: 15450, Validation MSE: 0.0431391\n",
      "Iter: 15500, Training MSE: 0.0160293\n",
      "Iter: 15500, Validation MSE: 0.0431371\n",
      "Iter: 15550, Training MSE: 0.0160193\n",
      "Iter: 15550, Validation MSE: 0.0431353\n",
      "Iter: 15600, Training MSE: 0.0160095\n",
      "Iter: 15600, Validation MSE: 0.0431335\n",
      "Iter: 15650, Training MSE: 0.0159998\n",
      "Iter: 15650, Validation MSE: 0.0431319\n",
      "Iter: 15700, Training MSE: 0.0159903\n",
      "Iter: 15700, Validation MSE: 0.0431303\n",
      "Iter: 15750, Training MSE: 0.0159809\n",
      "Iter: 15750, Validation MSE: 0.0431289\n",
      "Iter: 15800, Training MSE: 0.0159716\n",
      "Iter: 15800, Validation MSE: 0.0431277\n",
      "Iter: 15850, Training MSE: 0.0159625\n",
      "Iter: 15850, Validation MSE: 0.0431265\n",
      "Iter: 15900, Training MSE: 0.0159535\n",
      "Iter: 15900, Validation MSE: 0.0431254\n",
      "Iter: 15950, Training MSE: 0.0159446\n",
      "Iter: 15950, Validation MSE: 0.0431245\n",
      "Iter: 16000, Training MSE: 0.0159358\n",
      "Iter: 16000, Validation MSE: 0.0431237\n",
      "Iter: 16050, Training MSE: 0.0159272\n",
      "Iter: 16050, Validation MSE: 0.0431230\n",
      "Iter: 16100, Training MSE: 0.0159187\n",
      "Iter: 16100, Validation MSE: 0.0431224\n",
      "Iter: 16150, Training MSE: 0.0159103\n",
      "Iter: 16150, Validation MSE: 0.0431220\n",
      "Iter: 16200, Training MSE: 0.0159021\n",
      "Iter: 16200, Validation MSE: 0.0431216\n",
      "Iter: 16250, Training MSE: 0.0158939\n",
      "Iter: 16250, Validation MSE: 0.0431214\n",
      "Iter: 16300, Training MSE: 0.0158859\n",
      "Iter: 16300, Validation MSE: 0.0431213\n",
      "Iter: 16350, Training MSE: 0.0158779\n",
      "Iter: 16350, Validation MSE: 0.0431214\n",
      "Iter: 16400, Training MSE: 0.0158701\n",
      "Iter: 16400, Validation MSE: 0.0431215\n",
      "Iter: 16450, Training MSE: 0.0158624\n",
      "Iter: 16450, Validation MSE: 0.0431218\n",
      "Iter: 16500, Training MSE: 0.0158548\n",
      "Iter: 16500, Validation MSE: 0.0431222\n",
      "Iter: 16550, Training MSE: 0.0158473\n",
      "Iter: 16550, Validation MSE: 0.0431227\n",
      "Iter: 16600, Training MSE: 0.0158399\n",
      "Iter: 16600, Validation MSE: 0.0431234\n",
      "Iter: 16650, Training MSE: 0.0158326\n",
      "Iter: 16650, Validation MSE: 0.0431242\n",
      "Iter: 16700, Training MSE: 0.0158254\n",
      "Iter: 16700, Validation MSE: 0.0431251\n",
      "Iter: 16750, Training MSE: 0.0158183\n",
      "Iter: 16750, Validation MSE: 0.0431262\n",
      "Iter: 16800, Training MSE: 0.0158113\n",
      "Iter: 16800, Validation MSE: 0.0431274\n",
      "Iter: 16850, Training MSE: 0.0158044\n",
      "Iter: 16850, Validation MSE: 0.0431287\n",
      "Iter: 16900, Training MSE: 0.0157975\n",
      "Iter: 16900, Validation MSE: 0.0431301\n",
      "Iter: 16950, Training MSE: 0.0157908\n",
      "Iter: 16950, Validation MSE: 0.0431317\n",
      "Iter: 17000, Training MSE: 0.0157841\n",
      "Iter: 17000, Validation MSE: 0.0431334\n",
      "Iter: 17050, Training MSE: 0.0157776\n",
      "Iter: 17050, Validation MSE: 0.0431352\n",
      "Iter: 17100, Training MSE: 0.0157711\n",
      "Iter: 17100, Validation MSE: 0.0431372\n",
      "Iter: 17150, Training MSE: 0.0157647\n",
      "Iter: 17150, Validation MSE: 0.0431393\n",
      "Iter: 17200, Training MSE: 0.0157584\n",
      "Iter: 17200, Validation MSE: 0.0431416\n",
      "Iter: 17250, Training MSE: 0.0157521\n",
      "Iter: 17250, Validation MSE: 0.0431439\n",
      "Iter: 17300, Training MSE: 0.0157460\n",
      "Iter: 17300, Validation MSE: 0.0431465\n",
      "Iter: 17350, Training MSE: 0.0157399\n",
      "Iter: 17350, Validation MSE: 0.0431491\n",
      "Iter: 17400, Training MSE: 0.0157339\n",
      "Iter: 17400, Validation MSE: 0.0431519\n",
      "Iter: 17450, Training MSE: 0.0157280\n",
      "Iter: 17450, Validation MSE: 0.0431549\n",
      "Iter: 17500, Training MSE: 0.0157221\n",
      "Iter: 17500, Validation MSE: 0.0431580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 17550, Training MSE: 0.0157163\n",
      "Iter: 17550, Validation MSE: 0.0431612\n",
      "Iter: 17600, Training MSE: 0.0157106\n",
      "Iter: 17600, Validation MSE: 0.0431646\n",
      "Iter: 17650, Training MSE: 0.0157050\n",
      "Iter: 17650, Validation MSE: 0.0431681\n",
      "Iter: 17700, Training MSE: 0.0156994\n",
      "Iter: 17700, Validation MSE: 0.0431718\n",
      "Iter: 17750, Training MSE: 0.0156939\n",
      "Iter: 17750, Validation MSE: 0.0431756\n",
      "Iter: 17800, Training MSE: 0.0156884\n",
      "Iter: 17800, Validation MSE: 0.0431796\n",
      "Iter: 17850, Training MSE: 0.0156831\n",
      "Iter: 17850, Validation MSE: 0.0431837\n",
      "Iter: 17900, Training MSE: 0.0156778\n",
      "Iter: 17900, Validation MSE: 0.0431880\n",
      "Iter: 17950, Training MSE: 0.0156725\n",
      "Iter: 17950, Validation MSE: 0.0431924\n",
      "Iter: 18000, Training MSE: 0.0156673\n",
      "Iter: 18000, Validation MSE: 0.0431970\n",
      "Iter: 18050, Training MSE: 0.0156622\n",
      "Iter: 18050, Validation MSE: 0.0432017\n",
      "Iter: 18100, Training MSE: 0.0156571\n",
      "Iter: 18100, Validation MSE: 0.0432066\n",
      "Iter: 18150, Training MSE: 0.0156521\n",
      "Iter: 18150, Validation MSE: 0.0432117\n",
      "Iter: 18200, Training MSE: 0.0156471\n",
      "Iter: 18200, Validation MSE: 0.0432169\n",
      "Iter: 18250, Training MSE: 0.0156422\n",
      "Iter: 18250, Validation MSE: 0.0432223\n",
      "Iter: 18300, Training MSE: 0.0156374\n",
      "Iter: 18300, Validation MSE: 0.0432279\n",
      "Iter: 18350, Training MSE: 0.0156326\n",
      "Iter: 18350, Validation MSE: 0.0432336\n",
      "Iter: 18400, Training MSE: 0.0156279\n",
      "Iter: 18400, Validation MSE: 0.0432395\n",
      "Iter: 18450, Training MSE: 0.0156232\n",
      "Iter: 18450, Validation MSE: 0.0432455\n",
      "Iter: 18500, Training MSE: 0.0156186\n",
      "Iter: 18500, Validation MSE: 0.0432518\n",
      "Iter: 18550, Training MSE: 0.0156140\n",
      "Iter: 18550, Validation MSE: 0.0432582\n",
      "Iter: 18600, Training MSE: 0.0156094\n",
      "Iter: 18600, Validation MSE: 0.0432648\n",
      "Iter: 18650, Training MSE: 0.0156049\n",
      "Iter: 18650, Validation MSE: 0.0432715\n",
      "Iter: 18700, Training MSE: 0.0156005\n",
      "Iter: 18700, Validation MSE: 0.0432785\n",
      "Iter: 18750, Training MSE: 0.0155961\n",
      "Iter: 18750, Validation MSE: 0.0432856\n",
      "Iter: 18800, Training MSE: 0.0155918\n",
      "Iter: 18800, Validation MSE: 0.0432929\n",
      "Iter: 18850, Training MSE: 0.0155875\n",
      "Iter: 18850, Validation MSE: 0.0433004\n",
      "Iter: 18900, Training MSE: 0.0155832\n",
      "Iter: 18900, Validation MSE: 0.0433082\n",
      "Iter: 18950, Training MSE: 0.0155790\n",
      "Iter: 18950, Validation MSE: 0.0433160\n",
      "Iter: 19000, Training MSE: 0.0155748\n",
      "Iter: 19000, Validation MSE: 0.0433241\n",
      "Iter: 19050, Training MSE: 0.0155707\n",
      "Iter: 19050, Validation MSE: 0.0433324\n",
      "Iter: 19100, Training MSE: 0.0155666\n",
      "Iter: 19100, Validation MSE: 0.0433409\n",
      "Iter: 19150, Training MSE: 0.0155625\n",
      "Iter: 19150, Validation MSE: 0.0433496\n",
      "Iter: 19200, Training MSE: 0.0155585\n",
      "Iter: 19200, Validation MSE: 0.0433586\n",
      "Iter: 19250, Training MSE: 0.0155545\n",
      "Iter: 19250, Validation MSE: 0.0433677\n",
      "Iter: 19300, Training MSE: 0.0155506\n",
      "Iter: 19300, Validation MSE: 0.0433770\n",
      "Iter: 19350, Training MSE: 0.0155467\n",
      "Iter: 19350, Validation MSE: 0.0433866\n",
      "Iter: 19400, Training MSE: 0.0155428\n",
      "Iter: 19400, Validation MSE: 0.0433964\n",
      "Iter: 19450, Training MSE: 0.0155390\n",
      "Iter: 19450, Validation MSE: 0.0434064\n",
      "Iter: 19500, Training MSE: 0.0155352\n",
      "Iter: 19500, Validation MSE: 0.0434167\n",
      "Iter: 19550, Training MSE: 0.0155314\n",
      "Iter: 19550, Validation MSE: 0.0434272\n",
      "Iter: 19600, Training MSE: 0.0155277\n",
      "Iter: 19600, Validation MSE: 0.0434380\n",
      "Iter: 19650, Training MSE: 0.0155240\n",
      "Iter: 19650, Validation MSE: 0.0434489\n",
      "Iter: 19700, Training MSE: 0.0155203\n",
      "Iter: 19700, Validation MSE: 0.0434602\n",
      "Iter: 19750, Training MSE: 0.0155167\n",
      "Iter: 19750, Validation MSE: 0.0434717\n",
      "Iter: 19800, Training MSE: 0.0155131\n",
      "Iter: 19800, Validation MSE: 0.0434835\n",
      "Iter: 19850, Training MSE: 0.0155095\n",
      "Iter: 19850, Validation MSE: 0.0434955\n",
      "Iter: 19900, Training MSE: 0.0155059\n",
      "Iter: 19900, Validation MSE: 0.0435078\n",
      "Iter: 19950, Training MSE: 0.0155024\n",
      "Iter: 19950, Validation MSE: 0.0435204\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, num_epochs):\n",
    "\n",
    "    # Compute the predicted output for the hidden layer, then adding bias\n",
    "    predicted_h = 1 / (1 + np.exp(np.dot(-weight_hidden, data_in)))\n",
    "    predicted_h = np.concatenate((np.ones([1, data_in.shape[1]]), predicted_h), axis=0)\n",
    "    \n",
    "    # Compute the output of network\n",
    "    predicted_out = 1 / (1 + np.exp(np.dot(-weight_out, predicted_h)))\n",
    "    # print(predicted_out)\n",
    "    # Compute the derivatives for the weight updates\n",
    "    deriv_out = (predicted_out * (1 - predicted_out)) * (target - predicted_out)\n",
    "    deriv_h = (predicted_h * (1 - predicted_h)) * (weight_out.T * deriv_out)\n",
    "\n",
    "    # Compute the update to the input to hidden node weights\n",
    "    deriv_weight_h = learning_rate * np.dot(\n",
    "        data_in, deriv_h[1:].T).T + momemtum * weight_hidden_prev\n",
    "    weight_hidden = weight_hidden + deriv_weight_h\n",
    "    weight_hidden_prev = deriv_weight_h\n",
    "    \n",
    "    # Compute the update to the hidden to output node weights\n",
    "    deriv_weight_out = learning_rate * np.dot(predicted_h, deriv_out.T).T \n",
    "    deriv_weight_out = deriv_weight_out + momemtum * weight_out_prev #add momentum\n",
    "    weight_out = weight_out + deriv_weight_out \n",
    "    weight_out_prev = deriv_weight_out\n",
    "\n",
    "    # Compute the error (loss) of the network for this epoch  #Change the data_in.shape[1] to X_val and X_test[1] for testing and validation set error \n",
    "    predicted_out_err = 1 / (1 + np.exp(np.dot(-weight_out, np.concatenate(\n",
    "        (np.ones([1, data_in.shape[1]]), (1 / (1 + np.exp(np.dot(-weight_hidden, data_in)))))))))\n",
    "    error_log[i] = 0.5 * ((predicted_out_err - target) ** 2).mean(axis=None)\n",
    "    \n",
    "    predicted_out_err = 1 / (1 + np.exp(np.dot(-weight_out, np.concatenate(\n",
    "        (np.ones([1, X_val.shape[1]]), (1 / (1 + np.exp(np.dot(-weight_hidden, X_val)))))))))\n",
    "    errorv_log[i] = 0.5 * ((predicted_out_err - Y_val) ** 2).mean(axis=None) \n",
    "    if errorv_log[i] < min_val_loss:\n",
    "        min_val_loss = errorv_log[i]\n",
    "        weight_hidden_val = weight_hidden\n",
    "        weight_out_val = weight_out\n",
    "    if (i % 50) == 0:\n",
    "        # print('XOR bias momentum MSE: {0}'.format(error_log[i]))\n",
    "        print(\"Iter: %d, Training MSE: %8.7f\" % (i, error_log[i]))\n",
    "        print(\"Iter: %d, Validation MSE: %8.7f\" % (i, errorv_log[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding out the minimum validation MSE and storing the corresponding weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.026457912487354303 Iter:211\n"
     ]
    }
   ],
   "source": [
    "minimum = min(errorv_log)\n",
    "i =np.where(errorv_log == minimum)\n",
    "print(minimum,\"Iter:%d\" % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_out = weight_out_val\n",
    "weight_hidden = weight_hidden_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.40373190e-02, -3.71316657e-01, -2.06344484e-01,\n",
       "        -6.31892832e-01, -4.16405182e-01,  1.94646210e-01,\n",
       "        -1.73555858e-01,  7.08601510e-01,  4.16167330e-01,\n",
       "         1.44545993e-01,  1.45752548e-02,  5.22391091e-01,\n",
       "        -1.02530372e+00, -3.15832856e-01, -6.00743705e-01,\n",
       "         7.43084010e-01,  6.49546156e-01, -5.32857084e-01,\n",
       "         4.02761691e-01,  4.61300967e-03, -3.22853300e-02,\n",
       "        -1.43000026e-01, -9.33601686e-02, -8.42021764e-01,\n",
       "        -5.03505986e-01, -2.16864572e-01, -8.40438144e-01,\n",
       "        -5.14856097e-01, -4.15620956e-04, -9.54460376e-01,\n",
       "        -4.37565247e-01, -1.87748942e-01,  3.56742348e-01,\n",
       "        -8.35281510e-02, -5.69317008e-01,  3.54687762e-01,\n",
       "         7.81864269e-01,  6.83999749e-01, -3.99256806e-01,\n",
       "        -1.64673470e-01,  7.88706693e-01, -2.16476648e-01,\n",
       "         1.63152830e-01, -2.28659045e-01, -4.28012954e-01,\n",
       "        -1.25277453e-01,  3.53523638e-01,  2.07585086e-01,\n",
       "         4.71083397e-01,  1.09016537e+00, -2.09229518e-01,\n",
       "        -9.92651110e-01, -1.19296863e-01,  6.25846453e-01,\n",
       "        -5.03386329e-01, -1.06062569e+00, -6.28619968e-02,\n",
       "        -2.16269556e-01,  8.51142001e-01,  3.44653278e-01,\n",
       "         3.01344302e-01, -2.09726230e-01,  6.05098112e-01,\n",
       "        -6.71125329e-01,  2.72034350e-01,  2.92910360e-01,\n",
       "        -7.63682972e-01, -7.98912676e-01,  8.89770378e-01,\n",
       "        -2.27913164e-01, -9.18656371e-03,  4.64257691e-02,\n",
       "        -8.17168609e-01, -6.73759934e-01, -7.42664338e-02,\n",
       "         2.95084358e-01,  8.85668590e-02, -3.24830244e-01,\n",
       "         4.94974500e-01, -3.13996529e-01, -1.66021384e-01,\n",
       "        -5.35259846e-01,  2.49418611e-01, -2.14851454e-01,\n",
       "        -4.45701161e-01,  4.52426620e-01, -5.29735451e-01,\n",
       "         7.26417800e-02,  6.45615428e-01, -5.10929390e-01,\n",
       "         7.94757655e-02, -6.09471356e-01,  3.05038271e-01,\n",
       "         8.10101398e-03, -5.43059457e-01, -5.81005674e-01,\n",
       "         1.26145531e-02,  5.99871780e-01, -1.06680904e-01,\n",
       "         2.55256246e-01,  3.04310443e-01]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.37441197,  0.15046796,  0.18892919, ...,  0.06365076,\n",
       "        -0.27877965,  0.01478299],\n",
       "       [-0.00577391,  0.03896966,  0.18488312, ..., -0.0343521 ,\n",
       "         0.06644235,  0.31994041],\n",
       "       [ 0.50392259, -0.57773425, -0.32850043, ..., -0.17809873,\n",
       "         0.15743577, -0.54068707],\n",
       "       ...,\n",
       "       [ 0.09997225,  0.04982138,  0.30411283, ..., -0.29794397,\n",
       "         0.04741357, -0.38150526],\n",
       "       [-0.15567135,  0.03484058, -0.05752257, ..., -0.06631286,\n",
       "        -0.00337356, -0.20600603],\n",
       "       [-0.25236842, -0.2669241 ,  0.08639636, ..., -0.35396957,\n",
       "         0.32418162,  0.0169252 ]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EQLBv6nerI_I"
   },
   "source": [
    "### Evaluate the model on the training set and print the results.\n",
    "And then plot the training error for each epoch through the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2O-8W7vRrL6p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target outputs: [1 0 1 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1\n",
      " 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 0 0 1 1 1 1 0\n",
      " 1 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 1 1 0]\n",
      "Predicted outputs: [[1.00000000e+00 2.77619375e-11 9.99995630e-01 3.34251675e-12\n",
      "  9.99998669e-01 9.99983224e-01 9.99977792e-01 8.45044117e-03\n",
      "  1.53363766e-06 1.63644698e-04 1.00000000e+00 9.94753863e-11\n",
      "  1.00086818e-07 2.77237369e-06 1.33572023e-05 1.14421309e-01\n",
      "  7.00085818e-06 9.21248111e-01 9.47793035e-03 4.99100109e-10\n",
      "  1.08844725e-01 2.22277389e-07 2.43642744e-03 9.99414174e-01\n",
      "  9.97435843e-01 9.43921847e-01 6.39310840e-02 1.94360969e-03\n",
      "  8.30452391e-05 9.85442035e-07 2.74998697e-12 9.93099100e-02\n",
      "  7.43136867e-10 2.44062428e-06 1.18787400e-13 9.99999996e-01\n",
      "  6.98785905e-07 9.41762162e-05 8.88220942e-01 8.84228679e-01\n",
      "  9.51632528e-01 2.29474863e-12 9.99999739e-01 2.28624260e-06\n",
      "  9.99932934e-01 9.99999991e-01 3.15875128e-12 9.99999998e-01\n",
      "  1.00000000e+00 9.99999994e-01 1.60525748e-08 1.40127334e-09\n",
      "  9.66338369e-07 9.99999994e-01 3.00528842e-04 9.99999984e-01\n",
      "  9.99999996e-01 3.71497048e-08 1.01380143e-09 9.99360463e-01\n",
      "  1.02986105e-06 8.12668070e-03 2.21511853e-09 1.39417630e-05\n",
      "  1.00000000e+00 9.99837798e-01 5.47129136e-02 2.68999833e-06\n",
      "  1.80824660e-09 7.89943138e-05 9.99999306e-01 1.00000000e+00\n",
      "  9.93783289e-01 9.99247667e-01 9.99999999e-01 3.16216953e-10\n",
      "  2.27991333e-03 8.69938139e-06 1.00000000e+00 9.99999987e-01\n",
      "  1.16753521e-08 1.01636825e-11 9.11825281e-01 2.27096882e-10\n",
      "  9.99981695e-01 9.99999920e-01 9.72054960e-01 1.13265754e-01\n",
      "  9.99957366e-01 2.31627650e-09 9.99969008e-01 1.00000000e+00\n",
      "  9.99999997e-01 9.88502858e-01 8.54640960e-10 1.00000000e+00\n",
      "  1.29284064e-06 9.44081006e-01 9.99999860e-01 9.99999391e-01\n",
      "  4.51074513e-09 1.05540417e-01 9.96915906e-01 9.99999998e-01\n",
      "  6.41453526e-03 4.68766660e-10 8.96408119e-01 9.99999821e-01\n",
      "  9.99999842e-01 9.99999959e-01 6.16357606e-10 1.00000000e+00\n",
      "  2.69896748e-03 8.34857337e-02 8.85399867e-08 1.34768499e-03\n",
      "  1.00000000e+00 6.52906869e-12 9.99367620e-01 2.36692159e-06\n",
      "  3.14956023e-02 9.92745986e-01 9.99977542e-01 9.99999669e-01\n",
      "  4.42495189e-04 2.10994644e-04 9.98396354e-01 3.54417188e-10\n",
      "  2.07961094e-14 9.25240719e-01 9.99999999e-01 9.99999997e-01\n",
      "  4.57545659e-02]]\n",
      "[0.09668118 0.20988665 0.13848331 ... 0.04353251 0.04353277 0.04353303] blue\n",
      "[0.09668118 0.20988665 0.13848331 ... 0.04353251 0.04353277 0.04353303] Red\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print the traget values and the networks predictions\n",
    "print(\"Target outputs:\", target)\n",
    "print(\"Predicted outputs:\", predicted_out)\n",
    "\n",
    "# Set up the plot of the training error by epoch\n",
    "plt.figure(4)\n",
    "plt.xlabel('Training Epochs')\n",
    "plt.ylabel('Mean Square Error')\n",
    "plt.title('MLP with two hidden nodes with bias and momentum for Real estate')\n",
    "plt.plot(errorv_log)\n",
    "plt.plot(error_log)\n",
    "print(errorv_log, 'blue')\n",
    "print(errorv_log, 'Red')\n",
    "plt.draw()\n",
    "\n",
    "plt.show()  # keeping the plots alive until you close them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on labelled test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 17)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29,)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_test= np.array([[1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = (np.column_stack((Z_test,X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 18)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 29)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 18)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_out_err = 1 / (1 + np.exp(np.dot(-weight_out, np.concatenate(\n",
    "        (np.ones([1, X_test.shape[1]]), (1 / (1 + np.exp(np.dot(-weight_hidden, X_test)))))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validating again on complete labelled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('RealEstateLabelled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "185    1\n",
      "186    0\n",
      "187    0\n",
      "188    0\n",
      "189    0\n",
      "Name: WBFPSTK, Length: 190, dtype: object\n",
      "     DWLUN  RDOS  YRBLT  TOTFIXT  HEATING WBFPSTK  BMNTGAR  ATTFRGAR  \\\n",
      "0        1    21   1900        5        2       0        0         0   \n",
      "1        2     0   1900       10        2       0        0         0   \n",
      "2        1     1   1959        7        2       0        0         0   \n",
      "3        2    19   1910       10        2       0        0         0   \n",
      "4        1    10   1900        8        2       0        0         0   \n",
      "..     ...   ...    ...      ...      ...     ...      ...       ...   \n",
      "185      1    15   1955        7        2       1        0         0   \n",
      "186      1    17   1900        5        2       0        0         0   \n",
      "187      2    19   1930       12        2       0        0         0   \n",
      "188      3    20   1885       15        2       0        0         0   \n",
      "189      1    17   1967        7        2       0        0         0   \n",
      "\n",
      "     TOTLIVAR  DECKOFP  ENCLPOR  NBHDGRP  RECROOM  FINBSMT  GRADE  CDU  \\\n",
      "0        1098       58        0        2        0        0   0.92    5   \n",
      "1        2112      232       72        1        0        0   1.08    4   \n",
      "2        1110       20       77        1      288        0   1.00    4   \n",
      "3        1634      120       98        1        0        0   1.08    4   \n",
      "4        1808        0        0        1        0        0   1.00    4   \n",
      "..        ...      ...      ...      ...      ...      ...    ...  ...   \n",
      "185      1290       72      174        1      216        0   1.08    4   \n",
      "186      1886       24       84        1        0        0   1.00    4   \n",
      "187      2300      346        0        1        0        0   1.00    4   \n",
      "188      2936      247      184        1        0        0   1.08    4   \n",
      "189      1430        0        0        1        0        0   1.08    4   \n",
      "\n",
      "     TOTOBY  SALEPRIC   \n",
      "0         0     103000  \n",
      "1         0     162000  \n",
      "2       420     160500  \n",
      "3      5530     170000  \n",
      "4         0     170000  \n",
      "..      ...        ...  \n",
      "185   13020     160500  \n",
      "186    3250     175000  \n",
      "187       0     200000  \n",
      "188       0     236000  \n",
      "189    2770     155000  \n",
      "\n",
      "[190 rows x 18 columns]\n"
     ]
    }
   ],
   "source": [
    "length_wood = len(df1.WBFPSTK)\n",
    "\n",
    "for y in range(length_wood):\n",
    "        if df1.WBFPSTK[y] == 'yes':\n",
    "             df1.WBFPSTK[y] = 1\n",
    "        else: \n",
    "             df1.WBFPSTK[y] = 0\n",
    "        \n",
    "print(df1.WBFPSTK) \n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 21, 1900, ..., 5, 0, 103000],\n",
       "       [2, 0, 1900, ..., 4, 0, 162000],\n",
       "       [1, 1, 1959, ..., 4, 420, 160500],\n",
       "       ...,\n",
       "       [2, 19, 1930, ..., 4, 0, 200000],\n",
       "       [3, 20, 1885, ..., 4, 0, 236000],\n",
       "       [1, 17, 1967, ..., 4, 2770, 155000]], dtype=object)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RealEstate = df1.values\n",
    "RealEstate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = RealEstate[:,0:17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 21, 1900, ..., 0.92, 5, 0],\n",
       "       [2, 0, 1900, ..., 1.08, 4, 0],\n",
       "       [1, 1, 1959, ..., 1.0, 4, 420],\n",
       "       ...,\n",
       "       [2, 19, 1930, ..., 1.0, 4, 0],\n",
       "       [3, 20, 1885, ..., 1.08, 4, 0],\n",
       "       [1, 17, 1967, ..., 1.08, 4, 2770]], dtype=object)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = RealEstate[:,17] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 0 0 1 1\n",
      " 1 1 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 0 0\n",
      " 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 1 1 0 0 0 1 0 1 0 1\n",
      " 0 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0\n",
      " 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1\n",
      " 0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "length = len(target)\n",
    "\n",
    "for x in range(length):\n",
    "    if target[x] >= 170000:\n",
    "        target[x] = 1\n",
    "    else: \n",
    "        target[x] = 0\n",
    "        \n",
    "target = np.array(target, dtype='int32')\n",
    "print(target)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "data_in = min_max_scaler.fit_transform(data_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z= np.array([[1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1],\n",
    " [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1],\n",
    " [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1],\n",
    " [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1],\n",
    " [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1],\n",
    " [1], [1], [1], [1], [1], [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(190, 1)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(190, 17)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = (np.column_stack((Z,data_in)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(190, 18)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(190,)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(data_in, target, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(161, 18)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.37441197,  0.15046796,  0.18892919, ...,  0.06365076,\n",
       "        -0.27877965,  0.01478299],\n",
       "       [-0.00577391,  0.03896966,  0.18488312, ..., -0.0343521 ,\n",
       "         0.06644235,  0.31994041],\n",
       "       [ 0.50392259, -0.57773425, -0.32850043, ..., -0.17809873,\n",
       "         0.15743577, -0.54068707],\n",
       "       ...,\n",
       "       [ 0.09997225,  0.04982138,  0.30411283, ..., -0.29794397,\n",
       "         0.04741357, -0.38150526],\n",
       "       [-0.15567135,  0.03484058, -0.05752257, ..., -0.06631286,\n",
       "        -0.00337356, -0.20600603],\n",
       "       [-0.25236842, -0.2669241 ,  0.08639636, ..., -0.35396957,\n",
       "         0.32418162,  0.0169252 ]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 18)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting original random weights to train the model on full data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outout node initial weights:\n",
      "[[-6.40050979e-02 -4.74073768e-01  4.96624779e-02 -6.46776074e-02\n",
      "  -7.96321979e-02 -1.69665179e-01 -2.95351366e-01  1.19270966e-01\n",
      "  -2.00345326e-01 -2.33172725e-01  1.21133833e-01  2.91420943e-02\n",
      "  -3.65420055e-01  1.35781213e-02 -3.15560134e-01  2.85335148e-01\n",
      "   3.53975293e-01 -5.76316262e-03  3.46561485e-01 -4.20354523e-01\n",
      "   5.24609012e-03 -4.34713496e-01 -7.18776724e-02 -4.03469084e-01\n",
      "  -3.72840028e-01  9.67453090e-02 -2.73987999e-01 -3.93054316e-01\n",
      "  -2.79693793e-01 -1.50173715e-01 -3.22125154e-02 -2.98256774e-01\n",
      "   1.40406725e-01 -1.69301644e-02  5.23672002e-03 -1.13107349e-01\n",
      "   2.93637454e-01  8.00041789e-02 -3.37701401e-01  2.00752347e-01\n",
      "   4.64551080e-01  8.36117022e-06  3.89520064e-01 -1.58386347e-01\n",
      "   6.71441276e-02 -7.24540367e-02 -6.32527370e-02  2.76559185e-01\n",
      "   3.56041735e-02  4.53742227e-01  4.42081601e-02 -4.17905078e-01\n",
      "  -1.33657598e-01  3.50850504e-01 -9.37249570e-02 -4.72797634e-01\n",
      "  -2.52822761e-01 -4.32855629e-01  4.93852011e-01  4.70580313e-01\n",
      "   3.00258351e-01  1.01817121e-01  2.64959860e-01 -3.30774553e-01\n",
      "  -2.06976768e-01  2.40668753e-02 -1.43375719e-01 -4.54321035e-01\n",
      "   4.83153445e-01 -5.86450806e-02  4.00043938e-03 -1.76458682e-01\n",
      "  -2.40255247e-01 -1.13110115e-01  3.32016900e-01  2.36747056e-01\n",
      "  -1.20789433e-01 -4.86982663e-01  2.97404939e-01 -2.30611202e-01\n",
      "   8.26848885e-02 -4.74449058e-01  1.62202019e-01 -1.12476574e-01\n",
      "  -2.92620127e-03 -8.50941626e-02 -1.49128099e-01  5.09779053e-02\n",
      "   4.72910690e-01 -3.87223785e-01 -1.86741472e-01 -4.58202290e-01\n",
      "   2.38399759e-01  1.57512388e-01 -2.85364254e-01 -8.32465598e-02\n",
      "   1.43841934e-01  1.61481327e-01 -3.29522867e-01  3.81652236e-01\n",
      "   2.78008160e-01]]\n",
      "Hidden node initial weights:\n",
      "[[-0.36604579  0.36891663  0.24877788 ...  0.12356318 -0.32328784\n",
      "   0.09125735]\n",
      " [-0.01073383  0.04790778  0.19952062 ... -0.02859014  0.04949682\n",
      "   0.34511312]\n",
      " [ 0.4885097  -0.45113191 -0.26788175 ... -0.14286507  0.08978199\n",
      "  -0.44777727]\n",
      " ...\n",
      " [ 0.10317594  0.13978223  0.32183481 ... -0.2783459   0.03616054\n",
      "  -0.36133889]\n",
      " [-0.14693846 -0.14242811 -0.09744914 ... -0.10083784  0.03872644\n",
      "  -0.26230271]\n",
      " [-0.24670952 -0.42437322  0.04092962 ... -0.3882126   0.36502514\n",
      "  -0.04152997]]\n"
     ]
    }
   ],
   "source": [
    "# Set the learing parameters\n",
    "learning_rate = 0.01\n",
    "momemtum = 0.5\n",
    "num_epochs = 20000\n",
    "min_val_loss = np.inf\n",
    "lamda = 1.8 #lamda variable acts as weight_cost coefficient\n",
    "\n",
    "\n",
    "# Set the dimensions of the input, hidden, and output layers\n",
    "size_in = 17\n",
    "size_hidden = 100  # Can be anything - larger will take longer\n",
    "size_out = target.ndim\n",
    "\n",
    "# Set the seed value of the random number generator\n",
    "random_seed = 2\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Initialize the network weights, and place to store previous epoch weights\n",
    "weight_out = np.random.rand(size_out, size_hidden + 1) - 0.5\n",
    "weight_hidden = np.random.rand(size_hidden, size_in + 1) - 0.5\n",
    "\n",
    "weight_hidden_prev = np.zeros(weight_hidden.shape)\n",
    "weight_out_prev = np.zeros(weight_out.shape)\n",
    "\n",
    "weight_hidden_val = np.zeros(weight_hidden.shape)\n",
    "weight_out_val = np.zeros(weight_out.shape)\n",
    "\n",
    "# Initialize a vector to store the train errors for each epoch\n",
    "error_log = np.zeros([num_epochs])\n",
    "errorv_log = np.zeros([num_epochs])\n",
    "\n",
    "print(\"Outout node initial weights:\")\n",
    "print(weight_out)\n",
    "print(\"Hidden node initial weights:\")\n",
    "print(weight_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(161, 18)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = data_in.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 161)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 18)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 18)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_val.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 29)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29,)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Training MSE: 0.1396935\n",
      "Iter: 0, Validation MSE: 0.1375511\n",
      "Iter: 50, Training MSE: 0.2322843\n",
      "Iter: 50, Validation MSE: 0.2064204\n",
      "Iter: 100, Training MSE: 0.0453930\n",
      "Iter: 100, Validation MSE: 0.0469987\n",
      "Iter: 150, Training MSE: 0.0399090\n",
      "Iter: 150, Validation MSE: 0.0490368\n",
      "Iter: 200, Training MSE: 0.0370193\n",
      "Iter: 200, Validation MSE: 0.0531705\n",
      "Iter: 250, Training MSE: 0.0350542\n",
      "Iter: 250, Validation MSE: 0.0577698\n",
      "Iter: 300, Training MSE: 0.0335612\n",
      "Iter: 300, Validation MSE: 0.0617826\n",
      "Iter: 350, Training MSE: 0.0323724\n",
      "Iter: 350, Validation MSE: 0.0647929\n",
      "Iter: 400, Training MSE: 0.0314155\n",
      "Iter: 400, Validation MSE: 0.0668313\n",
      "Iter: 450, Training MSE: 0.0306476\n",
      "Iter: 450, Validation MSE: 0.0681421\n",
      "Iter: 500, Training MSE: 0.0300339\n",
      "Iter: 500, Validation MSE: 0.0689763\n",
      "Iter: 550, Training MSE: 0.0295437\n",
      "Iter: 550, Validation MSE: 0.0695157\n",
      "Iter: 600, Training MSE: 0.0291505\n",
      "Iter: 600, Validation MSE: 0.0698757\n",
      "Iter: 650, Training MSE: 0.0288324\n",
      "Iter: 650, Validation MSE: 0.0701255\n",
      "Iter: 700, Training MSE: 0.0285721\n",
      "Iter: 700, Validation MSE: 0.0703068\n",
      "Iter: 750, Training MSE: 0.0283564\n",
      "Iter: 750, Validation MSE: 0.0704445\n",
      "Iter: 800, Training MSE: 0.0281750\n",
      "Iter: 800, Validation MSE: 0.0705540\n",
      "Iter: 850, Training MSE: 0.0280203\n",
      "Iter: 850, Validation MSE: 0.0706452\n",
      "Iter: 900, Training MSE: 0.0278865\n",
      "Iter: 900, Validation MSE: 0.0707242\n",
      "Iter: 950, Training MSE: 0.0277691\n",
      "Iter: 950, Validation MSE: 0.0707953\n",
      "Iter: 1000, Training MSE: 0.0276647\n",
      "Iter: 1000, Validation MSE: 0.0708614\n",
      "Iter: 1050, Training MSE: 0.0275708\n",
      "Iter: 1050, Validation MSE: 0.0709244\n",
      "Iter: 1100, Training MSE: 0.0274853\n",
      "Iter: 1100, Validation MSE: 0.0709858\n",
      "Iter: 1150, Training MSE: 0.0274066\n",
      "Iter: 1150, Validation MSE: 0.0710467\n",
      "Iter: 1200, Training MSE: 0.0273333\n",
      "Iter: 1200, Validation MSE: 0.0711077\n",
      "Iter: 1250, Training MSE: 0.0272645\n",
      "Iter: 1250, Validation MSE: 0.0711694\n",
      "Iter: 1300, Training MSE: 0.0271991\n",
      "Iter: 1300, Validation MSE: 0.0712322\n",
      "Iter: 1350, Training MSE: 0.0271366\n",
      "Iter: 1350, Validation MSE: 0.0712963\n",
      "Iter: 1400, Training MSE: 0.0270761\n",
      "Iter: 1400, Validation MSE: 0.0713619\n",
      "Iter: 1450, Training MSE: 0.0270173\n",
      "Iter: 1450, Validation MSE: 0.0714292\n",
      "Iter: 1500, Training MSE: 0.0269596\n",
      "Iter: 1500, Validation MSE: 0.0714981\n",
      "Iter: 1550, Training MSE: 0.0269026\n",
      "Iter: 1550, Validation MSE: 0.0715688\n",
      "Iter: 1600, Training MSE: 0.0268458\n",
      "Iter: 1600, Validation MSE: 0.0716411\n",
      "Iter: 1650, Training MSE: 0.0267890\n",
      "Iter: 1650, Validation MSE: 0.0717152\n",
      "Iter: 1700, Training MSE: 0.0267317\n",
      "Iter: 1700, Validation MSE: 0.0717910\n",
      "Iter: 1750, Training MSE: 0.0266736\n",
      "Iter: 1750, Validation MSE: 0.0718685\n",
      "Iter: 1800, Training MSE: 0.0266144\n",
      "Iter: 1800, Validation MSE: 0.0719479\n",
      "Iter: 1850, Training MSE: 0.0265537\n",
      "Iter: 1850, Validation MSE: 0.0720292\n",
      "Iter: 1900, Training MSE: 0.0264912\n",
      "Iter: 1900, Validation MSE: 0.0721125\n",
      "Iter: 1950, Training MSE: 0.0264266\n",
      "Iter: 1950, Validation MSE: 0.0721981\n",
      "Iter: 2000, Training MSE: 0.0263595\n",
      "Iter: 2000, Validation MSE: 0.0722861\n",
      "Iter: 2050, Training MSE: 0.0262895\n",
      "Iter: 2050, Validation MSE: 0.0723771\n",
      "Iter: 2100, Training MSE: 0.0262162\n",
      "Iter: 2100, Validation MSE: 0.0724713\n",
      "Iter: 2150, Training MSE: 0.0261394\n",
      "Iter: 2150, Validation MSE: 0.0725695\n",
      "Iter: 2200, Training MSE: 0.0260583\n",
      "Iter: 2200, Validation MSE: 0.0726723\n",
      "Iter: 2250, Training MSE: 0.0259727\n",
      "Iter: 2250, Validation MSE: 0.0727805\n",
      "Iter: 2300, Training MSE: 0.0258819\n",
      "Iter: 2300, Validation MSE: 0.0728954\n",
      "Iter: 2350, Training MSE: 0.0257853\n",
      "Iter: 2350, Validation MSE: 0.0730182\n",
      "Iter: 2400, Training MSE: 0.0256820\n",
      "Iter: 2400, Validation MSE: 0.0731506\n",
      "Iter: 2450, Training MSE: 0.0255711\n",
      "Iter: 2450, Validation MSE: 0.0732948\n",
      "Iter: 2500, Training MSE: 0.0254512\n",
      "Iter: 2500, Validation MSE: 0.0734534\n",
      "Iter: 2550, Training MSE: 0.0253207\n",
      "Iter: 2550, Validation MSE: 0.0736293\n",
      "Iter: 2600, Training MSE: 0.0251773\n",
      "Iter: 2600, Validation MSE: 0.0738263\n",
      "Iter: 2650, Training MSE: 0.0250180\n",
      "Iter: 2650, Validation MSE: 0.0740480\n",
      "Iter: 2700, Training MSE: 0.0248393\n",
      "Iter: 2700, Validation MSE: 0.0742972\n",
      "Iter: 2750, Training MSE: 0.0246374\n",
      "Iter: 2750, Validation MSE: 0.0745733\n",
      "Iter: 2800, Training MSE: 0.0244111\n",
      "Iter: 2800, Validation MSE: 0.0748703\n",
      "Iter: 2850, Training MSE: 0.0241629\n",
      "Iter: 2850, Validation MSE: 0.0751790\n",
      "Iter: 2900, Training MSE: 0.0238992\n",
      "Iter: 2900, Validation MSE: 0.0754963\n",
      "Iter: 2950, Training MSE: 0.0236269\n",
      "Iter: 2950, Validation MSE: 0.0758325\n",
      "Iter: 3000, Training MSE: 0.0233502\n",
      "Iter: 3000, Validation MSE: 0.0762059\n",
      "Iter: 3050, Training MSE: 0.0230711\n",
      "Iter: 3050, Validation MSE: 0.0766334\n",
      "Iter: 3100, Training MSE: 0.0227908\n",
      "Iter: 3100, Validation MSE: 0.0771269\n",
      "Iter: 3150, Training MSE: 0.0225100\n",
      "Iter: 3150, Validation MSE: 0.0776938\n",
      "Iter: 3200, Training MSE: 0.0222295\n",
      "Iter: 3200, Validation MSE: 0.0783377\n",
      "Iter: 3250, Training MSE: 0.0219497\n",
      "Iter: 3250, Validation MSE: 0.0790597\n",
      "Iter: 3300, Training MSE: 0.0216712\n",
      "Iter: 3300, Validation MSE: 0.0798581\n",
      "Iter: 3350, Training MSE: 0.0213943\n",
      "Iter: 3350, Validation MSE: 0.0807292\n",
      "Iter: 3400, Training MSE: 0.0211196\n",
      "Iter: 3400, Validation MSE: 0.0816673\n",
      "Iter: 3450, Training MSE: 0.0208473\n",
      "Iter: 3450, Validation MSE: 0.0826653\n",
      "Iter: 3500, Training MSE: 0.0205778\n",
      "Iter: 3500, Validation MSE: 0.0837153\n",
      "Iter: 3550, Training MSE: 0.0203115\n",
      "Iter: 3550, Validation MSE: 0.0848089\n",
      "Iter: 3600, Training MSE: 0.0200486\n",
      "Iter: 3600, Validation MSE: 0.0859373\n",
      "Iter: 3650, Training MSE: 0.0197896\n",
      "Iter: 3650, Validation MSE: 0.0870921\n",
      "Iter: 3700, Training MSE: 0.0195348\n",
      "Iter: 3700, Validation MSE: 0.0882650\n",
      "Iter: 3750, Training MSE: 0.0192846\n",
      "Iter: 3750, Validation MSE: 0.0894479\n",
      "Iter: 3800, Training MSE: 0.0190393\n",
      "Iter: 3800, Validation MSE: 0.0906334\n",
      "Iter: 3850, Training MSE: 0.0187993\n",
      "Iter: 3850, Validation MSE: 0.0918143\n",
      "Iter: 3900, Training MSE: 0.0185650\n",
      "Iter: 3900, Validation MSE: 0.0929842\n",
      "Iter: 3950, Training MSE: 0.0183366\n",
      "Iter: 3950, Validation MSE: 0.0941368\n",
      "Iter: 4000, Training MSE: 0.0181146\n",
      "Iter: 4000, Validation MSE: 0.0952669\n",
      "Iter: 4050, Training MSE: 0.0178990\n",
      "Iter: 4050, Validation MSE: 0.0963696\n",
      "Iter: 4100, Training MSE: 0.0176903\n",
      "Iter: 4100, Validation MSE: 0.0974406\n",
      "Iter: 4150, Training MSE: 0.0174884\n",
      "Iter: 4150, Validation MSE: 0.0984763\n",
      "Iter: 4200, Training MSE: 0.0172937\n",
      "Iter: 4200, Validation MSE: 0.0994738\n",
      "Iter: 4250, Training MSE: 0.0171060\n",
      "Iter: 4250, Validation MSE: 0.1004308\n",
      "Iter: 4300, Training MSE: 0.0169254\n",
      "Iter: 4300, Validation MSE: 0.1013455\n",
      "Iter: 4350, Training MSE: 0.0167517\n",
      "Iter: 4350, Validation MSE: 0.1022171\n",
      "Iter: 4400, Training MSE: 0.0165848\n",
      "Iter: 4400, Validation MSE: 0.1030449\n",
      "Iter: 4450, Training MSE: 0.0164245\n",
      "Iter: 4450, Validation MSE: 0.1038289\n",
      "Iter: 4500, Training MSE: 0.0162706\n",
      "Iter: 4500, Validation MSE: 0.1045698\n",
      "Iter: 4550, Training MSE: 0.0161228\n",
      "Iter: 4550, Validation MSE: 0.1052684\n",
      "Iter: 4600, Training MSE: 0.0159807\n",
      "Iter: 4600, Validation MSE: 0.1059258\n",
      "Iter: 4650, Training MSE: 0.0158442\n",
      "Iter: 4650, Validation MSE: 0.1065436\n",
      "Iter: 4700, Training MSE: 0.0157130\n",
      "Iter: 4700, Validation MSE: 0.1071233\n",
      "Iter: 4750, Training MSE: 0.0155867\n",
      "Iter: 4750, Validation MSE: 0.1076666\n",
      "Iter: 4800, Training MSE: 0.0154650\n",
      "Iter: 4800, Validation MSE: 0.1081755\n",
      "Iter: 4850, Training MSE: 0.0153479\n",
      "Iter: 4850, Validation MSE: 0.1086517\n",
      "Iter: 4900, Training MSE: 0.0152349\n",
      "Iter: 4900, Validation MSE: 0.1090969\n",
      "Iter: 4950, Training MSE: 0.0151258\n",
      "Iter: 4950, Validation MSE: 0.1095130\n",
      "Iter: 5000, Training MSE: 0.0150203\n",
      "Iter: 5000, Validation MSE: 0.1099016\n",
      "Iter: 5050, Training MSE: 0.0149183\n",
      "Iter: 5050, Validation MSE: 0.1102643\n",
      "Iter: 5100, Training MSE: 0.0148194\n",
      "Iter: 5100, Validation MSE: 0.1106024\n",
      "Iter: 5150, Training MSE: 0.0147234\n",
      "Iter: 5150, Validation MSE: 0.1109175\n",
      "Iter: 5200, Training MSE: 0.0146300\n",
      "Iter: 5200, Validation MSE: 0.1112105\n",
      "Iter: 5250, Training MSE: 0.0145389\n",
      "Iter: 5250, Validation MSE: 0.1114825\n",
      "Iter: 5300, Training MSE: 0.0144499\n",
      "Iter: 5300, Validation MSE: 0.1117344\n",
      "Iter: 5350, Training MSE: 0.0143626\n",
      "Iter: 5350, Validation MSE: 0.1119668\n",
      "Iter: 5400, Training MSE: 0.0142768\n",
      "Iter: 5400, Validation MSE: 0.1121801\n",
      "Iter: 5450, Training MSE: 0.0141921\n",
      "Iter: 5450, Validation MSE: 0.1123746\n",
      "Iter: 5500, Training MSE: 0.0141083\n",
      "Iter: 5500, Validation MSE: 0.1125505\n",
      "Iter: 5550, Training MSE: 0.0140251\n",
      "Iter: 5550, Validation MSE: 0.1127076\n",
      "Iter: 5600, Training MSE: 0.0139422\n",
      "Iter: 5600, Validation MSE: 0.1128458\n",
      "Iter: 5650, Training MSE: 0.0138595\n",
      "Iter: 5650, Validation MSE: 0.1129650\n",
      "Iter: 5700, Training MSE: 0.0137767\n",
      "Iter: 5700, Validation MSE: 0.1130649\n",
      "Iter: 5750, Training MSE: 0.0136938\n",
      "Iter: 5750, Validation MSE: 0.1131455\n",
      "Iter: 5800, Training MSE: 0.0136108\n",
      "Iter: 5800, Validation MSE: 0.1132071\n",
      "Iter: 5850, Training MSE: 0.0135275\n",
      "Iter: 5850, Validation MSE: 0.1132499\n",
      "Iter: 5900, Training MSE: 0.0134443\n",
      "Iter: 5900, Validation MSE: 0.1132748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 5950, Training MSE: 0.0133611\n",
      "Iter: 5950, Validation MSE: 0.1132828\n",
      "Iter: 6000, Training MSE: 0.0132781\n",
      "Iter: 6000, Validation MSE: 0.1132752\n",
      "Iter: 6050, Training MSE: 0.0131954\n",
      "Iter: 6050, Validation MSE: 0.1132533\n",
      "Iter: 6100, Training MSE: 0.0131133\n",
      "Iter: 6100, Validation MSE: 0.1132187\n",
      "Iter: 6150, Training MSE: 0.0130317\n",
      "Iter: 6150, Validation MSE: 0.1131730\n",
      "Iter: 6200, Training MSE: 0.0129510\n",
      "Iter: 6200, Validation MSE: 0.1131176\n",
      "Iter: 6250, Training MSE: 0.0128711\n",
      "Iter: 6250, Validation MSE: 0.1130540\n",
      "Iter: 6300, Training MSE: 0.0127922\n",
      "Iter: 6300, Validation MSE: 0.1129834\n",
      "Iter: 6350, Training MSE: 0.0127144\n",
      "Iter: 6350, Validation MSE: 0.1129071\n",
      "Iter: 6400, Training MSE: 0.0126377\n",
      "Iter: 6400, Validation MSE: 0.1128263\n",
      "Iter: 6450, Training MSE: 0.0125622\n",
      "Iter: 6450, Validation MSE: 0.1127417\n",
      "Iter: 6500, Training MSE: 0.0124879\n",
      "Iter: 6500, Validation MSE: 0.1126544\n",
      "Iter: 6550, Training MSE: 0.0124149\n",
      "Iter: 6550, Validation MSE: 0.1125650\n",
      "Iter: 6600, Training MSE: 0.0123433\n",
      "Iter: 6600, Validation MSE: 0.1124742\n",
      "Iter: 6650, Training MSE: 0.0122730\n",
      "Iter: 6650, Validation MSE: 0.1123826\n",
      "Iter: 6700, Training MSE: 0.0122041\n",
      "Iter: 6700, Validation MSE: 0.1122907\n",
      "Iter: 6750, Training MSE: 0.0121366\n",
      "Iter: 6750, Validation MSE: 0.1121988\n",
      "Iter: 6800, Training MSE: 0.0120705\n",
      "Iter: 6800, Validation MSE: 0.1121074\n",
      "Iter: 6850, Training MSE: 0.0120058\n",
      "Iter: 6850, Validation MSE: 0.1120168\n",
      "Iter: 6900, Training MSE: 0.0119425\n",
      "Iter: 6900, Validation MSE: 0.1119272\n",
      "Iter: 6950, Training MSE: 0.0118806\n",
      "Iter: 6950, Validation MSE: 0.1118387\n",
      "Iter: 7000, Training MSE: 0.0118202\n",
      "Iter: 7000, Validation MSE: 0.1117516\n",
      "Iter: 7050, Training MSE: 0.0117611\n",
      "Iter: 7050, Validation MSE: 0.1116660\n",
      "Iter: 7100, Training MSE: 0.0117034\n",
      "Iter: 7100, Validation MSE: 0.1115820\n",
      "Iter: 7150, Training MSE: 0.0116472\n",
      "Iter: 7150, Validation MSE: 0.1114996\n",
      "Iter: 7200, Training MSE: 0.0115922\n",
      "Iter: 7200, Validation MSE: 0.1114189\n",
      "Iter: 7250, Training MSE: 0.0115387\n",
      "Iter: 7250, Validation MSE: 0.1113399\n",
      "Iter: 7300, Training MSE: 0.0114864\n",
      "Iter: 7300, Validation MSE: 0.1112626\n",
      "Iter: 7350, Training MSE: 0.0114355\n",
      "Iter: 7350, Validation MSE: 0.1111870\n",
      "Iter: 7400, Training MSE: 0.0113859\n",
      "Iter: 7400, Validation MSE: 0.1111130\n",
      "Iter: 7450, Training MSE: 0.0113375\n",
      "Iter: 7450, Validation MSE: 0.1110406\n",
      "Iter: 7500, Training MSE: 0.0112904\n",
      "Iter: 7500, Validation MSE: 0.1109698\n",
      "Iter: 7550, Training MSE: 0.0112445\n",
      "Iter: 7550, Validation MSE: 0.1109006\n",
      "Iter: 7600, Training MSE: 0.0111999\n",
      "Iter: 7600, Validation MSE: 0.1108328\n",
      "Iter: 7650, Training MSE: 0.0111564\n",
      "Iter: 7650, Validation MSE: 0.1107665\n",
      "Iter: 7700, Training MSE: 0.0111140\n",
      "Iter: 7700, Validation MSE: 0.1107015\n",
      "Iter: 7750, Training MSE: 0.0110729\n",
      "Iter: 7750, Validation MSE: 0.1106378\n",
      "Iter: 7800, Training MSE: 0.0110328\n",
      "Iter: 7800, Validation MSE: 0.1105753\n",
      "Iter: 7850, Training MSE: 0.0109938\n",
      "Iter: 7850, Validation MSE: 0.1105141\n",
      "Iter: 7900, Training MSE: 0.0109559\n",
      "Iter: 7900, Validation MSE: 0.1104539\n",
      "Iter: 7950, Training MSE: 0.0109190\n",
      "Iter: 7950, Validation MSE: 0.1103949\n",
      "Iter: 8000, Training MSE: 0.0108831\n",
      "Iter: 8000, Validation MSE: 0.1103368\n",
      "Iter: 8050, Training MSE: 0.0108482\n",
      "Iter: 8050, Validation MSE: 0.1102798\n",
      "Iter: 8100, Training MSE: 0.0108142\n",
      "Iter: 8100, Validation MSE: 0.1102236\n",
      "Iter: 8150, Training MSE: 0.0107812\n",
      "Iter: 8150, Validation MSE: 0.1101684\n",
      "Iter: 8200, Training MSE: 0.0107492\n",
      "Iter: 8200, Validation MSE: 0.1101140\n",
      "Iter: 8250, Training MSE: 0.0107180\n",
      "Iter: 8250, Validation MSE: 0.1100604\n",
      "Iter: 8300, Training MSE: 0.0106876\n",
      "Iter: 8300, Validation MSE: 0.1100076\n",
      "Iter: 8350, Training MSE: 0.0106581\n",
      "Iter: 8350, Validation MSE: 0.1099555\n",
      "Iter: 8400, Training MSE: 0.0106295\n",
      "Iter: 8400, Validation MSE: 0.1099042\n",
      "Iter: 8450, Training MSE: 0.0106016\n",
      "Iter: 8450, Validation MSE: 0.1098535\n",
      "Iter: 8500, Training MSE: 0.0105745\n",
      "Iter: 8500, Validation MSE: 0.1098035\n",
      "Iter: 8550, Training MSE: 0.0105481\n",
      "Iter: 8550, Validation MSE: 0.1097541\n",
      "Iter: 8600, Training MSE: 0.0105225\n",
      "Iter: 8600, Validation MSE: 0.1097054\n",
      "Iter: 8650, Training MSE: 0.0104976\n",
      "Iter: 8650, Validation MSE: 0.1096572\n",
      "Iter: 8700, Training MSE: 0.0104734\n",
      "Iter: 8700, Validation MSE: 0.1096097\n",
      "Iter: 8750, Training MSE: 0.0104499\n",
      "Iter: 8750, Validation MSE: 0.1095627\n",
      "Iter: 8800, Training MSE: 0.0104270\n",
      "Iter: 8800, Validation MSE: 0.1095162\n",
      "Iter: 8850, Training MSE: 0.0104047\n",
      "Iter: 8850, Validation MSE: 0.1094703\n",
      "Iter: 8900, Training MSE: 0.0103830\n",
      "Iter: 8900, Validation MSE: 0.1094250\n",
      "Iter: 8950, Training MSE: 0.0103620\n",
      "Iter: 8950, Validation MSE: 0.1093801\n",
      "Iter: 9000, Training MSE: 0.0103415\n",
      "Iter: 9000, Validation MSE: 0.1093358\n",
      "Iter: 9050, Training MSE: 0.0103215\n",
      "Iter: 9050, Validation MSE: 0.1092920\n",
      "Iter: 9100, Training MSE: 0.0103022\n",
      "Iter: 9100, Validation MSE: 0.1092487\n",
      "Iter: 9150, Training MSE: 0.0102833\n",
      "Iter: 9150, Validation MSE: 0.1092058\n",
      "Iter: 9200, Training MSE: 0.0102649\n",
      "Iter: 9200, Validation MSE: 0.1091635\n",
      "Iter: 9250, Training MSE: 0.0102471\n",
      "Iter: 9250, Validation MSE: 0.1091216\n",
      "Iter: 9300, Training MSE: 0.0102297\n",
      "Iter: 9300, Validation MSE: 0.1090802\n",
      "Iter: 9350, Training MSE: 0.0102128\n",
      "Iter: 9350, Validation MSE: 0.1090393\n",
      "Iter: 9400, Training MSE: 0.0101963\n",
      "Iter: 9400, Validation MSE: 0.1089988\n",
      "Iter: 9450, Training MSE: 0.0101803\n",
      "Iter: 9450, Validation MSE: 0.1089588\n",
      "Iter: 9500, Training MSE: 0.0101647\n",
      "Iter: 9500, Validation MSE: 0.1089192\n",
      "Iter: 9550, Training MSE: 0.0101495\n",
      "Iter: 9550, Validation MSE: 0.1088801\n",
      "Iter: 9600, Training MSE: 0.0101346\n",
      "Iter: 9600, Validation MSE: 0.1088414\n",
      "Iter: 9650, Training MSE: 0.0101202\n",
      "Iter: 9650, Validation MSE: 0.1088031\n",
      "Iter: 9700, Training MSE: 0.0101062\n",
      "Iter: 9700, Validation MSE: 0.1087653\n",
      "Iter: 9750, Training MSE: 0.0100925\n",
      "Iter: 9750, Validation MSE: 0.1087279\n",
      "Iter: 9800, Training MSE: 0.0100792\n",
      "Iter: 9800, Validation MSE: 0.1086910\n",
      "Iter: 9850, Training MSE: 0.0100662\n",
      "Iter: 9850, Validation MSE: 0.1086544\n",
      "Iter: 9900, Training MSE: 0.0100535\n",
      "Iter: 9900, Validation MSE: 0.1086183\n",
      "Iter: 9950, Training MSE: 0.0100412\n",
      "Iter: 9950, Validation MSE: 0.1085826\n",
      "Iter: 10000, Training MSE: 0.0100291\n",
      "Iter: 10000, Validation MSE: 0.1085473\n",
      "Iter: 10050, Training MSE: 0.0100174\n",
      "Iter: 10050, Validation MSE: 0.1085124\n",
      "Iter: 10100, Training MSE: 0.0100060\n",
      "Iter: 10100, Validation MSE: 0.1084779\n",
      "Iter: 10150, Training MSE: 0.0099948\n",
      "Iter: 10150, Validation MSE: 0.1084439\n",
      "Iter: 10200, Training MSE: 0.0099839\n",
      "Iter: 10200, Validation MSE: 0.1084101\n",
      "Iter: 10250, Training MSE: 0.0099733\n",
      "Iter: 10250, Validation MSE: 0.1083768\n",
      "Iter: 10300, Training MSE: 0.0099630\n",
      "Iter: 10300, Validation MSE: 0.1083439\n",
      "Iter: 10350, Training MSE: 0.0099529\n",
      "Iter: 10350, Validation MSE: 0.1083114\n",
      "Iter: 10400, Training MSE: 0.0099430\n",
      "Iter: 10400, Validation MSE: 0.1082792\n",
      "Iter: 10450, Training MSE: 0.0099334\n",
      "Iter: 10450, Validation MSE: 0.1082474\n",
      "Iter: 10500, Training MSE: 0.0099240\n",
      "Iter: 10500, Validation MSE: 0.1082159\n",
      "Iter: 10550, Training MSE: 0.0099148\n",
      "Iter: 10550, Validation MSE: 0.1081849\n",
      "Iter: 10600, Training MSE: 0.0099058\n",
      "Iter: 10600, Validation MSE: 0.1081542\n",
      "Iter: 10650, Training MSE: 0.0098971\n",
      "Iter: 10650, Validation MSE: 0.1081238\n",
      "Iter: 10700, Training MSE: 0.0098885\n",
      "Iter: 10700, Validation MSE: 0.1080938\n",
      "Iter: 10750, Training MSE: 0.0098802\n",
      "Iter: 10750, Validation MSE: 0.1080641\n",
      "Iter: 10800, Training MSE: 0.0098720\n",
      "Iter: 10800, Validation MSE: 0.1080348\n",
      "Iter: 10850, Training MSE: 0.0098640\n",
      "Iter: 10850, Validation MSE: 0.1080058\n",
      "Iter: 10900, Training MSE: 0.0098562\n",
      "Iter: 10900, Validation MSE: 0.1079772\n",
      "Iter: 10950, Training MSE: 0.0098486\n",
      "Iter: 10950, Validation MSE: 0.1079489\n",
      "Iter: 11000, Training MSE: 0.0098412\n",
      "Iter: 11000, Validation MSE: 0.1079209\n",
      "Iter: 11050, Training MSE: 0.0098339\n",
      "Iter: 11050, Validation MSE: 0.1078932\n",
      "Iter: 11100, Training MSE: 0.0098268\n",
      "Iter: 11100, Validation MSE: 0.1078658\n",
      "Iter: 11150, Training MSE: 0.0098198\n",
      "Iter: 11150, Validation MSE: 0.1078388\n",
      "Iter: 11200, Training MSE: 0.0098130\n",
      "Iter: 11200, Validation MSE: 0.1078120\n",
      "Iter: 11250, Training MSE: 0.0098063\n",
      "Iter: 11250, Validation MSE: 0.1077856\n",
      "Iter: 11300, Training MSE: 0.0097998\n",
      "Iter: 11300, Validation MSE: 0.1077595\n",
      "Iter: 11350, Training MSE: 0.0097934\n",
      "Iter: 11350, Validation MSE: 0.1077336\n",
      "Iter: 11400, Training MSE: 0.0097872\n",
      "Iter: 11400, Validation MSE: 0.1077081\n",
      "Iter: 11450, Training MSE: 0.0097810\n",
      "Iter: 11450, Validation MSE: 0.1076828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 11500, Training MSE: 0.0097751\n",
      "Iter: 11500, Validation MSE: 0.1076579\n",
      "Iter: 11550, Training MSE: 0.0097692\n",
      "Iter: 11550, Validation MSE: 0.1076332\n",
      "Iter: 11600, Training MSE: 0.0097635\n",
      "Iter: 11600, Validation MSE: 0.1076087\n",
      "Iter: 11650, Training MSE: 0.0097578\n",
      "Iter: 11650, Validation MSE: 0.1075846\n",
      "Iter: 11700, Training MSE: 0.0097523\n",
      "Iter: 11700, Validation MSE: 0.1075607\n",
      "Iter: 11750, Training MSE: 0.0097469\n",
      "Iter: 11750, Validation MSE: 0.1075371\n",
      "Iter: 11800, Training MSE: 0.0097416\n",
      "Iter: 11800, Validation MSE: 0.1075138\n",
      "Iter: 11850, Training MSE: 0.0097365\n",
      "Iter: 11850, Validation MSE: 0.1074907\n",
      "Iter: 11900, Training MSE: 0.0097314\n",
      "Iter: 11900, Validation MSE: 0.1074679\n",
      "Iter: 11950, Training MSE: 0.0097264\n",
      "Iter: 11950, Validation MSE: 0.1074453\n",
      "Iter: 12000, Training MSE: 0.0097215\n",
      "Iter: 12000, Validation MSE: 0.1074229\n",
      "Iter: 12050, Training MSE: 0.0097168\n",
      "Iter: 12050, Validation MSE: 0.1074009\n",
      "Iter: 12100, Training MSE: 0.0097121\n",
      "Iter: 12100, Validation MSE: 0.1073790\n",
      "Iter: 12150, Training MSE: 0.0097075\n",
      "Iter: 12150, Validation MSE: 0.1073574\n",
      "Iter: 12200, Training MSE: 0.0097030\n",
      "Iter: 12200, Validation MSE: 0.1073360\n",
      "Iter: 12250, Training MSE: 0.0096985\n",
      "Iter: 12250, Validation MSE: 0.1073149\n",
      "Iter: 12300, Training MSE: 0.0096942\n",
      "Iter: 12300, Validation MSE: 0.1072939\n",
      "Iter: 12350, Training MSE: 0.0096899\n",
      "Iter: 12350, Validation MSE: 0.1072732\n",
      "Iter: 12400, Training MSE: 0.0096858\n",
      "Iter: 12400, Validation MSE: 0.1072528\n",
      "Iter: 12450, Training MSE: 0.0096817\n",
      "Iter: 12450, Validation MSE: 0.1072325\n",
      "Iter: 12500, Training MSE: 0.0096776\n",
      "Iter: 12500, Validation MSE: 0.1072125\n",
      "Iter: 12550, Training MSE: 0.0096737\n",
      "Iter: 12550, Validation MSE: 0.1071926\n",
      "Iter: 12600, Training MSE: 0.0096698\n",
      "Iter: 12600, Validation MSE: 0.1071730\n",
      "Iter: 12650, Training MSE: 0.0096660\n",
      "Iter: 12650, Validation MSE: 0.1071536\n",
      "Iter: 12700, Training MSE: 0.0096623\n",
      "Iter: 12700, Validation MSE: 0.1071344\n",
      "Iter: 12750, Training MSE: 0.0096586\n",
      "Iter: 12750, Validation MSE: 0.1071154\n",
      "Iter: 12800, Training MSE: 0.0096550\n",
      "Iter: 12800, Validation MSE: 0.1070965\n",
      "Iter: 12850, Training MSE: 0.0096515\n",
      "Iter: 12850, Validation MSE: 0.1070779\n",
      "Iter: 12900, Training MSE: 0.0096480\n",
      "Iter: 12900, Validation MSE: 0.1070595\n",
      "Iter: 12950, Training MSE: 0.0096446\n",
      "Iter: 12950, Validation MSE: 0.1070412\n",
      "Iter: 13000, Training MSE: 0.0096412\n",
      "Iter: 13000, Validation MSE: 0.1070232\n",
      "Iter: 13050, Training MSE: 0.0096379\n",
      "Iter: 13050, Validation MSE: 0.1070053\n",
      "Iter: 13100, Training MSE: 0.0096347\n",
      "Iter: 13100, Validation MSE: 0.1069876\n",
      "Iter: 13150, Training MSE: 0.0096315\n",
      "Iter: 13150, Validation MSE: 0.1069701\n",
      "Iter: 13200, Training MSE: 0.0096283\n",
      "Iter: 13200, Validation MSE: 0.1069528\n",
      "Iter: 13250, Training MSE: 0.0096252\n",
      "Iter: 13250, Validation MSE: 0.1069356\n",
      "Iter: 13300, Training MSE: 0.0096222\n",
      "Iter: 13300, Validation MSE: 0.1069186\n",
      "Iter: 13350, Training MSE: 0.0096192\n",
      "Iter: 13350, Validation MSE: 0.1069018\n",
      "Iter: 13400, Training MSE: 0.0096163\n",
      "Iter: 13400, Validation MSE: 0.1068851\n",
      "Iter: 13450, Training MSE: 0.0096134\n",
      "Iter: 13450, Validation MSE: 0.1068687\n",
      "Iter: 13500, Training MSE: 0.0096106\n",
      "Iter: 13500, Validation MSE: 0.1068523\n",
      "Iter: 13550, Training MSE: 0.0096078\n",
      "Iter: 13550, Validation MSE: 0.1068362\n",
      "Iter: 13600, Training MSE: 0.0096051\n",
      "Iter: 13600, Validation MSE: 0.1068201\n",
      "Iter: 13650, Training MSE: 0.0096024\n",
      "Iter: 13650, Validation MSE: 0.1068043\n",
      "Iter: 13700, Training MSE: 0.0095997\n",
      "Iter: 13700, Validation MSE: 0.1067886\n",
      "Iter: 13750, Training MSE: 0.0095971\n",
      "Iter: 13750, Validation MSE: 0.1067730\n",
      "Iter: 13800, Training MSE: 0.0095945\n",
      "Iter: 13800, Validation MSE: 0.1067576\n",
      "Iter: 13850, Training MSE: 0.0095920\n",
      "Iter: 13850, Validation MSE: 0.1067424\n",
      "Iter: 13900, Training MSE: 0.0095895\n",
      "Iter: 13900, Validation MSE: 0.1067272\n",
      "Iter: 13950, Training MSE: 0.0095870\n",
      "Iter: 13950, Validation MSE: 0.1067123\n",
      "Iter: 14000, Training MSE: 0.0095846\n",
      "Iter: 14000, Validation MSE: 0.1066974\n",
      "Iter: 14050, Training MSE: 0.0095823\n",
      "Iter: 14050, Validation MSE: 0.1066828\n",
      "Iter: 14100, Training MSE: 0.0095799\n",
      "Iter: 14100, Validation MSE: 0.1066682\n",
      "Iter: 14150, Training MSE: 0.0095776\n",
      "Iter: 14150, Validation MSE: 0.1066538\n",
      "Iter: 14200, Training MSE: 0.0095753\n",
      "Iter: 14200, Validation MSE: 0.1066395\n",
      "Iter: 14250, Training MSE: 0.0095731\n",
      "Iter: 14250, Validation MSE: 0.1066254\n",
      "Iter: 14300, Training MSE: 0.0095709\n",
      "Iter: 14300, Validation MSE: 0.1066113\n",
      "Iter: 14350, Training MSE: 0.0095687\n",
      "Iter: 14350, Validation MSE: 0.1065974\n",
      "Iter: 14400, Training MSE: 0.0095666\n",
      "Iter: 14400, Validation MSE: 0.1065837\n",
      "Iter: 14450, Training MSE: 0.0095645\n",
      "Iter: 14450, Validation MSE: 0.1065700\n",
      "Iter: 14500, Training MSE: 0.0095624\n",
      "Iter: 14500, Validation MSE: 0.1065565\n",
      "Iter: 14550, Training MSE: 0.0095604\n",
      "Iter: 14550, Validation MSE: 0.1065431\n",
      "Iter: 14600, Training MSE: 0.0095583\n",
      "Iter: 14600, Validation MSE: 0.1065298\n",
      "Iter: 14650, Training MSE: 0.0095563\n",
      "Iter: 14650, Validation MSE: 0.1065167\n",
      "Iter: 14700, Training MSE: 0.0095544\n",
      "Iter: 14700, Validation MSE: 0.1065036\n",
      "Iter: 14750, Training MSE: 0.0095525\n",
      "Iter: 14750, Validation MSE: 0.1064907\n",
      "Iter: 14800, Training MSE: 0.0095506\n",
      "Iter: 14800, Validation MSE: 0.1064779\n",
      "Iter: 14850, Training MSE: 0.0095487\n",
      "Iter: 14850, Validation MSE: 0.1064652\n",
      "Iter: 14900, Training MSE: 0.0095468\n",
      "Iter: 14900, Validation MSE: 0.1064526\n",
      "Iter: 14950, Training MSE: 0.0095450\n",
      "Iter: 14950, Validation MSE: 0.1064402\n",
      "Iter: 15000, Training MSE: 0.0095432\n",
      "Iter: 15000, Validation MSE: 0.1064278\n",
      "Iter: 15050, Training MSE: 0.0095414\n",
      "Iter: 15050, Validation MSE: 0.1064155\n",
      "Iter: 15100, Training MSE: 0.0095397\n",
      "Iter: 15100, Validation MSE: 0.1064034\n",
      "Iter: 15150, Training MSE: 0.0095379\n",
      "Iter: 15150, Validation MSE: 0.1063913\n",
      "Iter: 15200, Training MSE: 0.0095362\n",
      "Iter: 15200, Validation MSE: 0.1063794\n",
      "Iter: 15250, Training MSE: 0.0095346\n",
      "Iter: 15250, Validation MSE: 0.1063675\n",
      "Iter: 15300, Training MSE: 0.0095329\n",
      "Iter: 15300, Validation MSE: 0.1063558\n",
      "Iter: 15350, Training MSE: 0.0095313\n",
      "Iter: 15350, Validation MSE: 0.1063441\n",
      "Iter: 15400, Training MSE: 0.0095296\n",
      "Iter: 15400, Validation MSE: 0.1063326\n",
      "Iter: 15450, Training MSE: 0.0095281\n",
      "Iter: 15450, Validation MSE: 0.1063211\n",
      "Iter: 15500, Training MSE: 0.0095265\n",
      "Iter: 15500, Validation MSE: 0.1063097\n",
      "Iter: 15550, Training MSE: 0.0095249\n",
      "Iter: 15550, Validation MSE: 0.1062985\n",
      "Iter: 15600, Training MSE: 0.0095234\n",
      "Iter: 15600, Validation MSE: 0.1062873\n",
      "Iter: 15650, Training MSE: 0.0095219\n",
      "Iter: 15650, Validation MSE: 0.1062762\n",
      "Iter: 15700, Training MSE: 0.0095204\n",
      "Iter: 15700, Validation MSE: 0.1062652\n",
      "Iter: 15750, Training MSE: 0.0095189\n",
      "Iter: 15750, Validation MSE: 0.1062543\n",
      "Iter: 15800, Training MSE: 0.0095175\n",
      "Iter: 15800, Validation MSE: 0.1062435\n",
      "Iter: 15850, Training MSE: 0.0095160\n",
      "Iter: 15850, Validation MSE: 0.1062328\n",
      "Iter: 15900, Training MSE: 0.0095146\n",
      "Iter: 15900, Validation MSE: 0.1062221\n",
      "Iter: 15950, Training MSE: 0.0095132\n",
      "Iter: 15950, Validation MSE: 0.1062116\n",
      "Iter: 16000, Training MSE: 0.0095118\n",
      "Iter: 16000, Validation MSE: 0.1062011\n",
      "Iter: 16050, Training MSE: 0.0095105\n",
      "Iter: 16050, Validation MSE: 0.1061907\n",
      "Iter: 16100, Training MSE: 0.0095091\n",
      "Iter: 16100, Validation MSE: 0.1061804\n",
      "Iter: 16150, Training MSE: 0.0095078\n",
      "Iter: 16150, Validation MSE: 0.1061702\n",
      "Iter: 16200, Training MSE: 0.0095065\n",
      "Iter: 16200, Validation MSE: 0.1061600\n",
      "Iter: 16250, Training MSE: 0.0095052\n",
      "Iter: 16250, Validation MSE: 0.1061500\n",
      "Iter: 16300, Training MSE: 0.0095039\n",
      "Iter: 16300, Validation MSE: 0.1061400\n",
      "Iter: 16350, Training MSE: 0.0095026\n",
      "Iter: 16350, Validation MSE: 0.1061301\n",
      "Iter: 16400, Training MSE: 0.0095013\n",
      "Iter: 16400, Validation MSE: 0.1061203\n",
      "Iter: 16450, Training MSE: 0.0095001\n",
      "Iter: 16450, Validation MSE: 0.1061105\n",
      "Iter: 16500, Training MSE: 0.0094989\n",
      "Iter: 16500, Validation MSE: 0.1061008\n",
      "Iter: 16550, Training MSE: 0.0094977\n",
      "Iter: 16550, Validation MSE: 0.1060912\n",
      "Iter: 16600, Training MSE: 0.0094965\n",
      "Iter: 16600, Validation MSE: 0.1060817\n",
      "Iter: 16650, Training MSE: 0.0094953\n",
      "Iter: 16650, Validation MSE: 0.1060722\n",
      "Iter: 16700, Training MSE: 0.0094941\n",
      "Iter: 16700, Validation MSE: 0.1060628\n",
      "Iter: 16750, Training MSE: 0.0094930\n",
      "Iter: 16750, Validation MSE: 0.1060535\n",
      "Iter: 16800, Training MSE: 0.0094918\n",
      "Iter: 16800, Validation MSE: 0.1060442\n",
      "Iter: 16850, Training MSE: 0.0094907\n",
      "Iter: 16850, Validation MSE: 0.1060350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 16900, Training MSE: 0.0094896\n",
      "Iter: 16900, Validation MSE: 0.1060259\n",
      "Iter: 16950, Training MSE: 0.0094885\n",
      "Iter: 16950, Validation MSE: 0.1060169\n",
      "Iter: 17000, Training MSE: 0.0094874\n",
      "Iter: 17000, Validation MSE: 0.1060079\n",
      "Iter: 17050, Training MSE: 0.0094863\n",
      "Iter: 17050, Validation MSE: 0.1059990\n",
      "Iter: 17100, Training MSE: 0.0094853\n",
      "Iter: 17100, Validation MSE: 0.1059901\n",
      "Iter: 17150, Training MSE: 0.0094842\n",
      "Iter: 17150, Validation MSE: 0.1059813\n",
      "Iter: 17200, Training MSE: 0.0094832\n",
      "Iter: 17200, Validation MSE: 0.1059726\n",
      "Iter: 17250, Training MSE: 0.0094821\n",
      "Iter: 17250, Validation MSE: 0.1059639\n",
      "Iter: 17300, Training MSE: 0.0094811\n",
      "Iter: 17300, Validation MSE: 0.1059553\n",
      "Iter: 17350, Training MSE: 0.0094801\n",
      "Iter: 17350, Validation MSE: 0.1059468\n",
      "Iter: 17400, Training MSE: 0.0094791\n",
      "Iter: 17400, Validation MSE: 0.1059383\n",
      "Iter: 17450, Training MSE: 0.0094781\n",
      "Iter: 17450, Validation MSE: 0.1059299\n",
      "Iter: 17500, Training MSE: 0.0094771\n",
      "Iter: 17500, Validation MSE: 0.1059216\n",
      "Iter: 17550, Training MSE: 0.0094762\n",
      "Iter: 17550, Validation MSE: 0.1059133\n",
      "Iter: 17600, Training MSE: 0.0094752\n",
      "Iter: 17600, Validation MSE: 0.1059050\n",
      "Iter: 17650, Training MSE: 0.0094743\n",
      "Iter: 17650, Validation MSE: 0.1058968\n",
      "Iter: 17700, Training MSE: 0.0094733\n",
      "Iter: 17700, Validation MSE: 0.1058887\n",
      "Iter: 17750, Training MSE: 0.0094724\n",
      "Iter: 17750, Validation MSE: 0.1058806\n",
      "Iter: 17800, Training MSE: 0.0094715\n",
      "Iter: 17800, Validation MSE: 0.1058726\n",
      "Iter: 17850, Training MSE: 0.0094706\n",
      "Iter: 17850, Validation MSE: 0.1058646\n",
      "Iter: 17900, Training MSE: 0.0094697\n",
      "Iter: 17900, Validation MSE: 0.1058567\n",
      "Iter: 17950, Training MSE: 0.0094688\n",
      "Iter: 17950, Validation MSE: 0.1058489\n",
      "Iter: 18000, Training MSE: 0.0094679\n",
      "Iter: 18000, Validation MSE: 0.1058411\n",
      "Iter: 18050, Training MSE: 0.0094671\n",
      "Iter: 18050, Validation MSE: 0.1058333\n",
      "Iter: 18100, Training MSE: 0.0094662\n",
      "Iter: 18100, Validation MSE: 0.1058256\n",
      "Iter: 18150, Training MSE: 0.0094654\n",
      "Iter: 18150, Validation MSE: 0.1058180\n",
      "Iter: 18200, Training MSE: 0.0094645\n",
      "Iter: 18200, Validation MSE: 0.1058104\n",
      "Iter: 18250, Training MSE: 0.0094637\n",
      "Iter: 18250, Validation MSE: 0.1058029\n",
      "Iter: 18300, Training MSE: 0.0094629\n",
      "Iter: 18300, Validation MSE: 0.1057954\n",
      "Iter: 18350, Training MSE: 0.0094620\n",
      "Iter: 18350, Validation MSE: 0.1057879\n",
      "Iter: 18400, Training MSE: 0.0094612\n",
      "Iter: 18400, Validation MSE: 0.1057805\n",
      "Iter: 18450, Training MSE: 0.0094604\n",
      "Iter: 18450, Validation MSE: 0.1057732\n",
      "Iter: 18500, Training MSE: 0.0094596\n",
      "Iter: 18500, Validation MSE: 0.1057659\n",
      "Iter: 18550, Training MSE: 0.0094589\n",
      "Iter: 18550, Validation MSE: 0.1057586\n",
      "Iter: 18600, Training MSE: 0.0094581\n",
      "Iter: 18600, Validation MSE: 0.1057514\n",
      "Iter: 18650, Training MSE: 0.0094573\n",
      "Iter: 18650, Validation MSE: 0.1057443\n",
      "Iter: 18700, Training MSE: 0.0094566\n",
      "Iter: 18700, Validation MSE: 0.1057372\n",
      "Iter: 18750, Training MSE: 0.0094558\n",
      "Iter: 18750, Validation MSE: 0.1057301\n",
      "Iter: 18800, Training MSE: 0.0094551\n",
      "Iter: 18800, Validation MSE: 0.1057231\n",
      "Iter: 18850, Training MSE: 0.0094543\n",
      "Iter: 18850, Validation MSE: 0.1057161\n",
      "Iter: 18900, Training MSE: 0.0094536\n",
      "Iter: 18900, Validation MSE: 0.1057092\n",
      "Iter: 18950, Training MSE: 0.0094529\n",
      "Iter: 18950, Validation MSE: 0.1057023\n",
      "Iter: 19000, Training MSE: 0.0094521\n",
      "Iter: 19000, Validation MSE: 0.1056954\n",
      "Iter: 19050, Training MSE: 0.0094514\n",
      "Iter: 19050, Validation MSE: 0.1056886\n",
      "Iter: 19100, Training MSE: 0.0094507\n",
      "Iter: 19100, Validation MSE: 0.1056819\n",
      "Iter: 19150, Training MSE: 0.0094500\n",
      "Iter: 19150, Validation MSE: 0.1056751\n",
      "Iter: 19200, Training MSE: 0.0094493\n",
      "Iter: 19200, Validation MSE: 0.1056685\n",
      "Iter: 19250, Training MSE: 0.0094486\n",
      "Iter: 19250, Validation MSE: 0.1056618\n",
      "Iter: 19300, Training MSE: 0.0094480\n",
      "Iter: 19300, Validation MSE: 0.1056552\n",
      "Iter: 19350, Training MSE: 0.0094473\n",
      "Iter: 19350, Validation MSE: 0.1056487\n",
      "Iter: 19400, Training MSE: 0.0094466\n",
      "Iter: 19400, Validation MSE: 0.1056422\n",
      "Iter: 19450, Training MSE: 0.0094460\n",
      "Iter: 19450, Validation MSE: 0.1056357\n",
      "Iter: 19500, Training MSE: 0.0094453\n",
      "Iter: 19500, Validation MSE: 0.1056292\n",
      "Iter: 19550, Training MSE: 0.0094447\n",
      "Iter: 19550, Validation MSE: 0.1056228\n",
      "Iter: 19600, Training MSE: 0.0094440\n",
      "Iter: 19600, Validation MSE: 0.1056165\n",
      "Iter: 19650, Training MSE: 0.0094434\n",
      "Iter: 19650, Validation MSE: 0.1056102\n",
      "Iter: 19700, Training MSE: 0.0094427\n",
      "Iter: 19700, Validation MSE: 0.1056039\n",
      "Iter: 19750, Training MSE: 0.0094421\n",
      "Iter: 19750, Validation MSE: 0.1055976\n",
      "Iter: 19800, Training MSE: 0.0094415\n",
      "Iter: 19800, Validation MSE: 0.1055914\n",
      "Iter: 19850, Training MSE: 0.0094409\n",
      "Iter: 19850, Validation MSE: 0.1055852\n",
      "Iter: 19900, Training MSE: 0.0094403\n",
      "Iter: 19900, Validation MSE: 0.1055791\n",
      "Iter: 19950, Training MSE: 0.0094397\n",
      "Iter: 19950, Validation MSE: 0.1055730\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, num_epochs):\n",
    "\n",
    "    # Compute the predicted output for the hidden layer, then adding bias\n",
    "    predicted_h = 1 / (1 + np.exp(np.dot(-weight_hidden, data_in)))\n",
    "    predicted_h = np.concatenate((np.ones([1, data_in.shape[1]]), predicted_h), axis=0)\n",
    "    \n",
    "    # Compute the output of network\n",
    "    predicted_out = 1 / (1 + np.exp(np.dot(-weight_out, predicted_h)))\n",
    "    # print(predicted_out)\n",
    "    # Compute the derivatives for the weight updates\n",
    "    deriv_out = (predicted_out * (1 - predicted_out)) * (target - predicted_out)\n",
    "    deriv_h = (predicted_h * (1 - predicted_h)) * (weight_out.T * deriv_out)\n",
    "\n",
    "    # Compute the update to the input to hidden node weights\n",
    "    deriv_weight_h = learning_rate * np.dot(\n",
    "        data_in, deriv_h[1:].T).T + momemtum * weight_hidden_prev\n",
    "    weight_hidden = weight_hidden + lamda * deriv_weight_h #using the weight_cost coefficient to minimize the values of weight matrix\n",
    "    weight_hidden_prev = deriv_weight_h\n",
    "    \n",
    "    # Compute the update to the hidden to output node weights\n",
    "    deriv_weight_out = learning_rate * np.dot(predicted_h, deriv_out.T).T \n",
    "    deriv_weight_out = deriv_weight_out + momemtum * weight_out_prev #add momentum\n",
    "    weight_out = weight_out + lamda * deriv_weight_out \n",
    "    weight_out_prev = deriv_weight_out\n",
    "\n",
    "    # Compute the error (loss) of the network for this epoch  #Change the data_in.shape[1] to X_val and X_test[1] for testing and validation set error \n",
    "    predicted_out_err = 1 / (1 + np.exp(np.dot(-weight_out, np.concatenate(\n",
    "        (np.ones([1, data_in.shape[1]]), (1 / (1 + np.exp(np.dot(-weight_hidden, data_in)))))))))\n",
    "    error_log[i] = 0.5 * ((predicted_out_err - target) ** 2).mean(axis=None)\n",
    "   \n",
    "    # Computing the mse for validation set\n",
    "    predicted_out_err = 1 / (1 + np.exp(np.dot(-weight_out, np.concatenate(\n",
    "        (np.ones([1, X_val.shape[1]]), (1 / (1 + np.exp(np.dot(-weight_hidden, X_val)))))))))\n",
    "    errorv_log[i] = 0.5 * ((predicted_out_err - Y_val) ** 2).mean(axis=None) \n",
    "    if errorv_log[i] < min_val_loss:\n",
    "        min_val_loss = errorv_log[i]\n",
    "        weight_hidden_val = weight_hidden\n",
    "        weight_out_val = weight_out\n",
    "    if (i % 50) == 0:\n",
    "        # print('XOR bias momentum MSE: {0}'.format(error_log[i]))\n",
    "        print(\"Iter: %d, Training MSE: %8.7f\" % (i, error_log[i]))\n",
    "        print(\"Iter: %d, Validation MSE: %8.7f\" % (i, errorv_log[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04685164643312453 Iter:90\n"
     ]
    }
   ],
   "source": [
    "minimum = min(errorv_log)\n",
    "i =np.where(errorv_log == minimum)\n",
    "print(minimum,\"Iter:%d\" % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0123669  -0.32064187 -0.13906919 -0.52557872 -0.34271534  0.03476609\n",
      "  -0.20899122  0.54388938  0.34869546  0.15234627  0.05403824  0.51046554\n",
      "  -0.86603424 -0.12693552 -0.57091328  0.65127949  0.51339663 -0.41474552\n",
      "   0.37639205 -0.02454349  0.08111906 -0.16480222 -0.11851338 -0.66600608\n",
      "  -0.41985758 -0.18033173 -0.68438933 -0.5314442  -0.05020943 -0.74698722\n",
      "  -0.30338675 -0.08034286  0.32708206 -0.06468542 -0.4377226   0.23153667\n",
      "   0.79287465  0.66061879 -0.44729586 -0.07726061  0.79606506 -0.17844757\n",
      "   0.15896003 -0.17993263 -0.24823986  0.01558226  0.29528606  0.22479118\n",
      "   0.42030308  0.87895096 -0.19100512 -0.73159629 -0.1123741   0.55853913\n",
      "  -0.42309682 -0.7548037  -0.12911203 -0.28994108  0.55575655  0.31802334\n",
      "   0.27074044 -0.08189665  0.43087016 -0.55076584  0.15440585  0.28921652\n",
      "  -0.58790718 -0.7378412   0.7941953  -0.27709574  0.02340871  0.11461865\n",
      "  -0.60568729 -0.56076943 -0.12240745  0.29991826  0.08082178 -0.27170735\n",
      "   0.37910987 -0.34432724 -0.24756747 -0.517179    0.31134475 -0.26432972\n",
      "  -0.26269656  0.39798236 -0.49405345 -0.02185689  0.56386016 -0.43244173\n",
      "   0.10260616 -0.56507398  0.23867084 -0.00795889 -0.53981584 -0.42527465\n",
      "   0.043368    0.28096269 -0.01802847  0.2472658   0.30296503]]\n",
      "[[-4.34103846e-01  1.17135093e-01  1.90084833e-01 ...  4.44378342e-02\n",
      "  -3.53685765e-01  7.50057691e-02]\n",
      " [-6.16177692e-02  1.71520644e-02  1.69062136e-01 ... -7.05814996e-02\n",
      "   2.40557363e-02  3.40068791e-01]\n",
      " [ 4.31991738e-01 -6.14385619e-01 -3.19008036e-01 ... -2.04353754e-01\n",
      "   6.87398635e-02 -4.61418018e-01]\n",
      " ...\n",
      " [ 4.08327660e-02  2.03344252e-02  2.82144965e-01 ... -3.38085275e-01\n",
      "   3.58915077e-03 -3.68815155e-01]\n",
      " [-2.09239905e-01  2.12390781e-02 -1.10596652e-01 ... -1.28970300e-01\n",
      "  -1.27279037e-04 -2.56352565e-01]\n",
      " [-2.95418756e-01 -2.74155534e-01  3.55031457e-02 ... -4.07746762e-01\n",
      "   3.33362938e-01 -3.51809610e-02]]\n"
     ]
    }
   ],
   "source": [
    "weight_out = weight_out_val\n",
    "weight_hidden = weight_hidden_val\n",
    "print(weight_out)\n",
    "print(weight_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation over the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target outputs: [0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 0\n",
      " 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1\n",
      " 0 1 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1\n",
      " 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0\n",
      " 0 1 1 1 0 0 1 0 0 1 1 1 0]\n",
      "Predicted outputs: [[3.23089940e-03 3.51250755e-02 7.56545354e-05 4.23449519e-09\n",
      "  1.82401921e-03 2.00489218e-08 3.12774784e-09 9.99997054e-01\n",
      "  1.00000000e+00 1.16086935e-09 1.80466253e-07 1.69159973e-04\n",
      "  1.25910279e-05 3.83359819e-02 3.56966295e-08 9.99948791e-01\n",
      "  9.97996912e-01 9.99660439e-01 9.99987617e-01 5.91903865e-05\n",
      "  9.99799541e-01 1.00000000e+00 2.55904820e-05 1.39119515e-08\n",
      "  9.95383838e-01 9.99998115e-01 6.37994736e-09 9.99999995e-01\n",
      "  9.99999998e-01 2.00518344e-10 9.99878211e-01 3.05773780e-09\n",
      "  9.96257790e-01 9.93258226e-01 9.99999599e-01 7.70580479e-04\n",
      "  3.13488474e-03 1.14764559e-06 1.00000000e+00 4.59586796e-09\n",
      "  2.08086688e-08 7.12458401e-07 1.53410105e-04 4.20242600e-02\n",
      "  2.83708632e-05 9.66635499e-01 9.10847839e-01 2.25137257e-11\n",
      "  1.77973772e-02 2.68261833e-08 7.53571057e-03 9.99986731e-01\n",
      "  9.94841010e-01 9.74813692e-01 4.13818313e-02 4.85833090e-02\n",
      "  2.64244723e-04 6.35422676e-04 4.16826012e-08 4.62105767e-02\n",
      "  3.56125515e-11 6.01439708e-05 6.60089287e-11 9.93731215e-01\n",
      "  8.86763274e-06 1.23544413e-06 9.42084649e-01 9.96211590e-01\n",
      "  9.99842709e-01 7.58282025e-11 9.99950260e-01 3.19862999e-03\n",
      "  9.94542761e-01 9.99999996e-01 1.00352773e-09 9.99266521e-01\n",
      "  9.99999985e-01 9.97525895e-01 2.56288691e-09 5.18120698e-08\n",
      "  2.86423744e-08 9.99998095e-01 8.15070652e-04 9.99649740e-01\n",
      "  9.99999992e-01 4.46509734e-08 1.38499359e-06 9.99728387e-01\n",
      "  6.39226492e-08 5.93107045e-03 2.36987536e-09 1.17698314e-03\n",
      "  9.99999998e-01 9.99992547e-01 3.53582937e-02 9.61804518e-06\n",
      "  1.35237300e-05 6.58611594e-06 9.99999999e-01 9.99885040e-01\n",
      "  9.99090105e-01 9.99999839e-01 9.99996878e-01 6.56440034e-09\n",
      "  2.19146607e-05 4.69733224e-08 9.99989352e-01 9.98537805e-01\n",
      "  3.01854929e-09 1.52606436e-09 9.88145765e-01 1.29000372e-08\n",
      "  9.99906394e-01 9.89189573e-01 9.88149172e-01 4.53508171e-02\n",
      "  9.97899755e-01 4.52354518e-08 9.99997597e-01 9.99999821e-01\n",
      "  9.99999896e-01 9.98886775e-01 9.39927268e-07 1.00000000e+00\n",
      "  4.77046276e-05 9.65403882e-01 9.99999843e-01 9.69655839e-01\n",
      "  2.64927423e-09 3.99313296e-02 9.92104008e-01 1.00000000e+00\n",
      "  1.48552185e-03 7.39247467e-08 9.63512259e-01 9.99999960e-01\n",
      "  9.99999851e-01 9.99999716e-01 1.95431085e-11 9.99999995e-01\n",
      "  1.02798794e-03 5.44428238e-02 9.49632172e-09 9.37819507e-05\n",
      "  1.00000000e+00 4.08161589e-09 9.93574395e-01 4.52429620e-05\n",
      "  4.06284177e-03 9.97263041e-01 9.99997716e-01 9.70823655e-01\n",
      "  2.99455412e-04 3.89533027e-05 9.98517813e-01 2.31332013e-07\n",
      "  1.82829590e-11 9.56919153e-01 9.99913030e-01 9.99999576e-01\n",
      "  2.21036989e-02]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print the traget values and the networks predictions\n",
    "print(\"Target outputs:\", target)\n",
    "print(\"Predicted outputs:\", predicted_out)\n",
    "\n",
    "# Set up the plot of the training error by epoch\n",
    "plt.figure(4)\n",
    "plt.xlabel('Training Epochs')\n",
    "plt.ylabel('Mean Square Error')\n",
    "plt.title('MLP with two hidden nodes with bias and momentum for Real estate')\n",
    "plt.plot(errorv_log)\n",
    "plt.plot(error_log)\n",
    "plt.draw()\n",
    "\n",
    "plt.show()  # keeping the plots alive until you close them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making final predictions of unlabelled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('RealEstateUnlabelled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     1\n",
      "1     0\n",
      "2     0\n",
      "3     1\n",
      "4     0\n",
      "5     0\n",
      "6     0\n",
      "7     0\n",
      "8     0\n",
      "9     0\n",
      "10    0\n",
      "11    0\n",
      "12    0\n",
      "13    0\n",
      "14    0\n",
      "15    0\n",
      "16    0\n",
      "17    0\n",
      "18    0\n",
      "19    0\n",
      "20    0\n",
      "21    0\n",
      "22    0\n",
      "23    0\n",
      "24    0\n",
      "25    0\n",
      "26    0\n",
      "Name: WBFPSTK, dtype: object\n",
      "    DWLUN  RDOS  YRBLT  TOTFIXT  HEATING WBFPSTK  BMNTGAR  ATTFRGAR  TOTLIVAR  \\\n",
      "0       1     9   1900        5        2       1        0         0      1714   \n",
      "1       2    20   1890       10        2       0        0         0      1374   \n",
      "2       1     3   1920        5        2       0        0         0      1396   \n",
      "3       2    19   1940        8        2       1        0         0      1494   \n",
      "4       2    15   1900       10        2       0        0         0      3138   \n",
      "5       2    21   1900       10        2       0        0         0      1750   \n",
      "6       2     8   1900       10        2       0        0         0      2678   \n",
      "7       1    15   1893        8        2       0        0         0      1467   \n",
      "8       2     3   1910       10        2       0        0         0      2844   \n",
      "9       2    12   1935       10        2       0        2         0      2830   \n",
      "10      2    14   1926       10        2       0        0         0      2254   \n",
      "11      3     5   1900       15        2       0        0         0      3318   \n",
      "12      2    11   1920       10        2       0        0         0      2448   \n",
      "13      3    17   1910       15        2       0        0         0      3236   \n",
      "14      2     6   1930       10        2       0        0         0      2444   \n",
      "15      1     3   1930        7        2       0        0         0      1476   \n",
      "16      1     7   1925        5        2       0        1         0      1529   \n",
      "17      2    19   1925       10        2       0        0         0      2250   \n",
      "18      1    16   1910        5        2       0        0         0      1495   \n",
      "19      1    21   1885        5        2       0        0         0      1106   \n",
      "20      2    20   1930       10        2       0        0         0      2511   \n",
      "21      3     9   1900       15        2       0        0         0      2916   \n",
      "22      2    21   1900       10        2       0        0         0      1900   \n",
      "23      2    22   1900       10        2       0        0         0      1973   \n",
      "24      1    18   1900        5        2       0        0         0      1354   \n",
      "25      2    23   1900       15        2       0        0         0      3186   \n",
      "26      3    16   1910       15        2       0        0         0      3732   \n",
      "\n",
      "    DECKOFP  ENCLPOR  NBHDGRP  RECROOM  FINBSMT  GRADE  CDU  TOTOBY SALEPRIC   \n",
      "0        98        0        1        0        0   1.00    4       0         ?  \n",
      "1       115        0        2        0        0   1.00    4       0         ?  \n",
      "2         0      150        1        0        0   1.00    4       0         ?  \n",
      "3        60      230        1        0        0   1.00    4    4750         ?  \n",
      "4       272        0        1        0        0   1.08    4       0         ?  \n",
      "5       200       66        1        0      300   1.08    4       0         ?  \n",
      "6       150       98        1        0        0   1.08    4       0         ?  \n",
      "7       109        0        1        0        0   1.00    4       0         ?  \n",
      "8       293        0        1        0        0   1.00    4       0         ?  \n",
      "9       282        0        1        0        0   1.08    4       0         ?  \n",
      "10       80        0        1        0        0   1.00    4    2740         ?  \n",
      "11      234        0        1        0        0   1.00    4       0         ?  \n",
      "12      105       60        1        0        0   1.00    4    6170         ?  \n",
      "13      144        0        1        0        0   1.08    4       0         ?  \n",
      "14      228       84        1        0        0   1.08    4    4340         ?  \n",
      "15      184        0        1      144        0   1.08    4       0         ?  \n",
      "16        0      192        1        0        0   1.00    4       0         ?  \n",
      "17      246       78        1        0        0   1.08    4    4900         ?  \n",
      "18        0        0        2        0        0   1.00    5       0         ?  \n",
      "19        0       33        1        0        0   1.00    4       0         ?  \n",
      "20      469        0        1      156        0   1.00    4    5370         ?  \n",
      "21      216      216        1        0        0   1.08    4       0         ?  \n",
      "22      418        0        2        0        0   1.00    5    1950         ?  \n",
      "23       60      224        1        0        0   1.08    4    5780         ?  \n",
      "24      540        0        1        0        0   1.00    4       0         ?  \n",
      "25      418        0        1        0        0   1.00    4    4900         ?  \n",
      "26      218        0        1        0        0   1.00    4       0         ?  \n"
     ]
    }
   ],
   "source": [
    "length_wood = len(df2.WBFPSTK)\n",
    "\n",
    "for y in range(length_wood):\n",
    "        if df2.WBFPSTK[y] == 'yes':\n",
    "             df2.WBFPSTK[y] = 1\n",
    "        else: \n",
    "             df2.WBFPSTK[y] = 0\n",
    "        \n",
    "print(df2.WBFPSTK) \n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "RealEstate = df2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 18)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RealEstate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = RealEstate[:,0:17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in= np.array(data_in, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_in.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 17)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z= np.array([[1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 1)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = (np.column_stack((Z,data_in)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 18)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,    1,    9, 1900,    5,    2,    1,    0,    0, 1714,   98,\n",
       "           0,    1,    0,    0,    1,    4,    0],\n",
       "       [   1,    2,   20, 1890,   10,    2,    0,    0,    0, 1374,  115,\n",
       "           0,    2,    0,    0,    1,    4,    0],\n",
       "       [   1,    1,    3, 1920,    5,    2,    0,    0,    0, 1396,    0,\n",
       "         150,    1,    0,    0,    1,    4,    0],\n",
       "       [   1,    2,   19, 1940,    8,    2,    1,    0,    0, 1494,   60,\n",
       "         230,    1,    0,    0,    1,    4, 4750],\n",
       "       [   1,    2,   15, 1900,   10,    2,    0,    0,    0, 3138,  272,\n",
       "           0,    1,    0,    0,    1,    4,    0],\n",
       "       [   1,    2,   21, 1900,   10,    2,    0,    0,    0, 1750,  200,\n",
       "          66,    1,    0,  300,    1,    4,    0],\n",
       "       [   1,    2,    8, 1900,   10,    2,    0,    0,    0, 2678,  150,\n",
       "          98,    1,    0,    0,    1,    4,    0],\n",
       "       [   1,    1,   15, 1893,    8,    2,    0,    0,    0, 1467,  109,\n",
       "           0,    1,    0,    0,    1,    4,    0],\n",
       "       [   1,    2,    3, 1910,   10,    2,    0,    0,    0, 2844,  293,\n",
       "           0,    1,    0,    0,    1,    4,    0],\n",
       "       [   1,    2,   12, 1935,   10,    2,    0,    2,    0, 2830,  282,\n",
       "           0,    1,    0,    0,    1,    4,    0],\n",
       "       [   1,    2,   14, 1926,   10,    2,    0,    0,    0, 2254,   80,\n",
       "           0,    1,    0,    0,    1,    4, 2740],\n",
       "       [   1,    3,    5, 1900,   15,    2,    0,    0,    0, 3318,  234,\n",
       "           0,    1,    0,    0,    1,    4,    0],\n",
       "       [   1,    2,   11, 1920,   10,    2,    0,    0,    0, 2448,  105,\n",
       "          60,    1,    0,    0,    1,    4, 6170],\n",
       "       [   1,    3,   17, 1910,   15,    2,    0,    0,    0, 3236,  144,\n",
       "           0,    1,    0,    0,    1,    4,    0],\n",
       "       [   1,    2,    6, 1930,   10,    2,    0,    0,    0, 2444,  228,\n",
       "          84,    1,    0,    0,    1,    4, 4340],\n",
       "       [   1,    1,    3, 1930,    7,    2,    0,    0,    0, 1476,  184,\n",
       "           0,    1,  144,    0,    1,    4,    0],\n",
       "       [   1,    1,    7, 1925,    5,    2,    0,    1,    0, 1529,    0,\n",
       "         192,    1,    0,    0,    1,    4,    0],\n",
       "       [   1,    2,   19, 1925,   10,    2,    0,    0,    0, 2250,  246,\n",
       "          78,    1,    0,    0,    1,    4, 4900],\n",
       "       [   1,    1,   16, 1910,    5,    2,    0,    0,    0, 1495,    0,\n",
       "           0,    2,    0,    0,    1,    5,    0],\n",
       "       [   1,    1,   21, 1885,    5,    2,    0,    0,    0, 1106,    0,\n",
       "          33,    1,    0,    0,    1,    4,    0],\n",
       "       [   1,    2,   20, 1930,   10,    2,    0,    0,    0, 2511,  469,\n",
       "           0,    1,  156,    0,    1,    4, 5370],\n",
       "       [   1,    3,    9, 1900,   15,    2,    0,    0,    0, 2916,  216,\n",
       "         216,    1,    0,    0,    1,    4,    0],\n",
       "       [   1,    2,   21, 1900,   10,    2,    0,    0,    0, 1900,  418,\n",
       "           0,    2,    0,    0,    1,    5, 1950],\n",
       "       [   1,    2,   22, 1900,   10,    2,    0,    0,    0, 1973,   60,\n",
       "         224,    1,    0,    0,    1,    4, 5780],\n",
       "       [   1,    1,   18, 1900,    5,    2,    0,    0,    0, 1354,  540,\n",
       "           0,    1,    0,    0,    1,    4,    0],\n",
       "       [   1,    2,   23, 1900,   15,    2,    0,    0,    0, 3186,  418,\n",
       "           0,    1,    0,    0,    1,    4, 4900],\n",
       "       [   1,    3,   16, 1910,   15,    2,    0,    0,    0, 3732,  218,\n",
       "           0,    1,    0,    0,    1,    4,    0]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = data_in.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 27)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_in.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_out_err = 1 / (1 + np.exp(np.dot(-weight_out, np.concatenate(\n",
    "        (np.ones([1, data_in.shape[1]]), (1 / (1 + np.exp(np.dot(-weight_hidden, data_in)))))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(predicted_out_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = result_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.989511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.956423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.967994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.098934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.989897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.989505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.992095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.967994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.992942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.992945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.963904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.994363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.098934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.989897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.633555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.956423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.990523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.191525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.964658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.956423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.191525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.992095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.927085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.101040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.947751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.838985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.998385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "0   0.989511\n",
       "1   0.956423\n",
       "2   0.967994\n",
       "3   0.098934\n",
       "4   0.989897\n",
       "5   0.989505\n",
       "6   0.992095\n",
       "7   0.967994\n",
       "8   0.992942\n",
       "9   0.992945\n",
       "10  0.963904\n",
       "11  0.994363\n",
       "12  0.098934\n",
       "13  0.989897\n",
       "14  0.633555\n",
       "15  0.956423\n",
       "16  0.990523\n",
       "17  0.191525\n",
       "18  0.964658\n",
       "19  0.956423\n",
       "20  0.191525\n",
       "21  0.992095\n",
       "22  0.927085\n",
       "23  0.101040\n",
       "24  0.947751\n",
       "25  0.838985\n",
       "26  0.998385"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.columns = ['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_csv = result_df.to_csv ('RealEstateUnlabelled.csv', index = None, header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.98951063 0.95642282 0.96799418 0.09893357 0.98989735 0.98950475\n",
      "  0.99209541 0.96799417 0.99294189 0.99294456 0.9639041  0.99436263\n",
      "  0.09893357 0.98989735 0.63355468 0.95642282 0.99052278 0.19152494\n",
      "  0.96465824 0.95642282 0.19152494 0.99209538 0.9270853  0.10104004\n",
      "  0.94775136 0.83898482 0.99838467]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, num_epochs):\n",
    "\n",
    "    # Compute the predicted output for the hidden layer, then adding bias\n",
    "    predicted_h = 1 / (1 + np.exp(np.dot(-weight_hidden, data_in)))\n",
    "    predicted_h = np.concatenate((np.ones([1, data_in.shape[1]]), predicted_h), axis=0)\n",
    "    \n",
    "    # Compute the output of network\n",
    "    predicted_out = 1 / (1 + np.exp(np.dot(-weight_out, predicted_h)))\n",
    "    \n",
    "print(predicted_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding one more hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('RealEstateLabelled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "185    1\n",
      "186    0\n",
      "187    0\n",
      "188    0\n",
      "189    0\n",
      "Name: WBFPSTK, Length: 190, dtype: object\n",
      "     DWLUN  RDOS  YRBLT  TOTFIXT  HEATING WBFPSTK  BMNTGAR  ATTFRGAR  \\\n",
      "0        1    21   1900        5        2       0        0         0   \n",
      "1        2     0   1900       10        2       0        0         0   \n",
      "2        1     1   1959        7        2       0        0         0   \n",
      "3        2    19   1910       10        2       0        0         0   \n",
      "4        1    10   1900        8        2       0        0         0   \n",
      "..     ...   ...    ...      ...      ...     ...      ...       ...   \n",
      "185      1    15   1955        7        2       1        0         0   \n",
      "186      1    17   1900        5        2       0        0         0   \n",
      "187      2    19   1930       12        2       0        0         0   \n",
      "188      3    20   1885       15        2       0        0         0   \n",
      "189      1    17   1967        7        2       0        0         0   \n",
      "\n",
      "     TOTLIVAR  DECKOFP  ENCLPOR  NBHDGRP  RECROOM  FINBSMT  GRADE  CDU  \\\n",
      "0        1098       58        0        2        0        0   0.92    5   \n",
      "1        2112      232       72        1        0        0   1.08    4   \n",
      "2        1110       20       77        1      288        0   1.00    4   \n",
      "3        1634      120       98        1        0        0   1.08    4   \n",
      "4        1808        0        0        1        0        0   1.00    4   \n",
      "..        ...      ...      ...      ...      ...      ...    ...  ...   \n",
      "185      1290       72      174        1      216        0   1.08    4   \n",
      "186      1886       24       84        1        0        0   1.00    4   \n",
      "187      2300      346        0        1        0        0   1.00    4   \n",
      "188      2936      247      184        1        0        0   1.08    4   \n",
      "189      1430        0        0        1        0        0   1.08    4   \n",
      "\n",
      "     TOTOBY  SALEPRIC   \n",
      "0         0     103000  \n",
      "1         0     162000  \n",
      "2       420     160500  \n",
      "3      5530     170000  \n",
      "4         0     170000  \n",
      "..      ...        ...  \n",
      "185   13020     160500  \n",
      "186    3250     175000  \n",
      "187       0     200000  \n",
      "188       0     236000  \n",
      "189    2770     155000  \n",
      "\n",
      "[190 rows x 18 columns]\n"
     ]
    }
   ],
   "source": [
    "length_wood = len(df2.WBFPSTK)\n",
    "\n",
    "for y in range(length_wood):\n",
    "        if df2.WBFPSTK[y] == 'yes':\n",
    "             df2.WBFPSTK[y] = 1\n",
    "        else: \n",
    "             df2.WBFPSTK[y] = 0\n",
    "        \n",
    "print(df2.WBFPSTK) \n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "RealEstate = df2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 21, 1900, ..., 5, 0, 103000],\n",
       "       [2, 0, 1900, ..., 4, 0, 162000],\n",
       "       [1, 1, 1959, ..., 4, 420, 160500],\n",
       "       ...,\n",
       "       [2, 19, 1930, ..., 4, 0, 200000],\n",
       "       [3, 20, 1885, ..., 4, 0, 236000],\n",
       "       [1, 17, 1967, ..., 4, 2770, 155000]], dtype=object)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RealEstate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = RealEstate[:,0:17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([103000, 162000, 160500, 170000, 170000, 145000, 182000, 165000,\n",
       "       240000, 160000, 154000, 151000, 161000, 220000, 202500, 137000,\n",
       "       135000, 198000, 175000, 160000, 200000, 158000, 125000, 170000,\n",
       "       205000, 155000, 145000, 250000, 155000, 128000, 128800, 175000,\n",
       "       170000, 155000, 135000, 180000, 190000, 170000, 205000, 160000,\n",
       "       178000, 140000, 172000, 175000, 132000, 160000, 136000, 155000,\n",
       "       155000, 153000, 180000, 139900, 205000, 146000, 146000, 147000,\n",
       "       155000, 160000, 162000, 175000, 180000, 178000, 130000, 195000,\n",
       "       185000, 130000, 185000, 162500, 117500, 202500, 150000, 160000,\n",
       "       145000, 145575, 179900, 142000, 165000, 180000, 212500, 145000,\n",
       "       140000, 165000, 165000, 165000, 142000, 125000, 157000, 155000,\n",
       "       166000, 162500, 215000, 170000, 160000, 120000, 206750, 177000,\n",
       "       172000, 155000, 160000, 155000, 190000, 180000, 173000, 150800,\n",
       "       125000, 134000, 220000, 140000, 172000, 166666, 206000, 140000,\n",
       "       150000, 187000, 187000, 175000, 225000, 157000, 150000, 162000,\n",
       "       174500, 220000, 168000, 155000, 223000, 179900, 232000, 225000,\n",
       "       136000, 178000, 145000, 170000, 200000, 155000, 230000, 128000,\n",
       "       115000, 193000, 150000, 187900, 192000, 118000, 155000, 190000,\n",
       "       155000, 136500, 153000, 147500, 185000, 157500, 208000, 183500,\n",
       "       228000, 140000, 175000, 150000, 199900, 239900, 210000, 230000,\n",
       "       230000, 121000, 156000, 195000, 235000, 220000, 171000, 187000,\n",
       "       141000, 195000, 155000, 222000, 149500, 165900, 155000, 140000,\n",
       "       138500, 155000, 169000, 200000, 186500, 200000, 173000, 120000,\n",
       "       200000, 160500, 175000, 200000, 236000, 155000], dtype=object)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = RealEstate[:,17]\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 0 0 1 1\n",
      " 1 1 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 0 0\n",
      " 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 1 1 0 0 0 1 0 1 0 1\n",
      " 0 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0\n",
      " 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1\n",
      " 0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "length = len(target)\n",
    "\n",
    "for x in range(length):\n",
    "    if target[x] >= 170000:\n",
    "        target[x] = 1\n",
    "    else: \n",
    "        target[x] = 0\n",
    "        \n",
    "target = np.array(target, dtype='int32')\n",
    "print(target)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "data_in = min_max_scaler.fit_transform(data_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z= np.array([[1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1],\n",
    " [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1],\n",
    " [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1],\n",
    " [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1],\n",
    " [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1],\n",
    " [1], [1], [1], [1], [1], [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = (np.column_stack((Z,data_in)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(190, 18)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(data_in, target, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 18)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = data_in.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 161)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outout node initial weights:\n",
      "[[-6.40050979e-02 -4.74073768e-01  4.96624779e-02 -6.46776074e-02\n",
      "  -7.96321979e-02 -1.69665179e-01 -2.95351366e-01  1.19270966e-01\n",
      "  -2.00345326e-01 -2.33172725e-01  1.21133833e-01  2.91420943e-02\n",
      "  -3.65420055e-01  1.35781213e-02 -3.15560134e-01  2.85335148e-01\n",
      "   3.53975293e-01 -5.76316262e-03  3.46561485e-01 -4.20354523e-01\n",
      "   5.24609012e-03 -4.34713496e-01 -7.18776724e-02 -4.03469084e-01\n",
      "  -3.72840028e-01  9.67453090e-02 -2.73987999e-01 -3.93054316e-01\n",
      "  -2.79693793e-01 -1.50173715e-01 -3.22125154e-02 -2.98256774e-01\n",
      "   1.40406725e-01 -1.69301644e-02  5.23672002e-03 -1.13107349e-01\n",
      "   2.93637454e-01  8.00041789e-02 -3.37701401e-01  2.00752347e-01\n",
      "   4.64551080e-01  8.36117022e-06  3.89520064e-01 -1.58386347e-01\n",
      "   6.71441276e-02 -7.24540367e-02 -6.32527370e-02  2.76559185e-01\n",
      "   3.56041735e-02  4.53742227e-01  4.42081601e-02 -4.17905078e-01\n",
      "  -1.33657598e-01  3.50850504e-01 -9.37249570e-02 -4.72797634e-01\n",
      "  -2.52822761e-01 -4.32855629e-01  4.93852011e-01  4.70580313e-01\n",
      "   3.00258351e-01  1.01817121e-01  2.64959860e-01 -3.30774553e-01\n",
      "  -2.06976768e-01  2.40668753e-02 -1.43375719e-01 -4.54321035e-01\n",
      "   4.83153445e-01 -5.86450806e-02  4.00043938e-03 -1.76458682e-01\n",
      "  -2.40255247e-01 -1.13110115e-01  3.32016900e-01  2.36747056e-01\n",
      "  -1.20789433e-01 -4.86982663e-01  2.97404939e-01 -2.30611202e-01\n",
      "   8.26848885e-02 -4.74449058e-01  1.62202019e-01 -1.12476574e-01\n",
      "  -2.92620127e-03 -8.50941626e-02 -1.49128099e-01  5.09779053e-02\n",
      "   4.72910690e-01 -3.87223785e-01 -1.86741472e-01 -4.58202290e-01\n",
      "   2.38399759e-01  1.57512388e-01 -2.85364254e-01 -8.32465598e-02\n",
      "   1.43841934e-01  1.61481327e-01 -3.29522867e-01  3.81652236e-01\n",
      "   2.78008160e-01]]\n",
      "Hidden node initial weights:\n",
      "[[-0.13857523 -0.38865987  0.18883013 ... -0.26401456  0.25827949\n",
      "  -0.1311586 ]\n",
      " [-0.12311635  0.09886987  0.35098554 ...  0.0697716  -0.1391389\n",
      "  -0.23109254]\n",
      " [ 0.30552639  0.48313313  0.22679527 ... -0.39935067  0.16840681\n",
      "   0.14440612]\n",
      " ...\n",
      " [ 0.45287775 -0.44576319 -0.05398686 ...  0.22432374 -0.40834554\n",
      "  -0.34514771]\n",
      " [-0.33179307  0.0509731  -0.15093553 ...  0.24279731  0.0914161\n",
      "  -0.43104229]\n",
      " [-0.41582952  0.24409613 -0.03529069 ... -0.30230551  0.42435406\n",
      "   0.28204381]]\n",
      "Hidden node 2 initial weights:\n",
      "[[-0.36604579  0.36891663  0.24877788 ...  0.20226424  0.00121645\n",
      "   0.00608683]\n",
      " [-0.28117921 -0.49533648 -0.08216376 ... -0.47085068 -0.12710712\n",
      "  -0.40412669]\n",
      " [-0.30303568  0.44242061 -0.12872883 ...  0.0628574   0.03946109\n",
      "  -0.004638  ]\n",
      " ...\n",
      " [-0.28125846  0.39218312  0.22069113 ... -0.28028392  0.23417613\n",
      "   0.27316319]\n",
      " [-0.32672667 -0.45479064 -0.23916525 ... -0.26288522  0.49272178\n",
      "   0.31077683]\n",
      " [-0.30563241  0.33472529 -0.3488582  ... -0.22577078 -0.12852666\n",
      "  -0.16969142]]\n"
     ]
    }
   ],
   "source": [
    "# Set the learing parameters\n",
    "learning_rate = 0.01\n",
    "momemtum = 0.5\n",
    "num_epochs = 20000\n",
    "min_val_loss = np.inf\n",
    "\n",
    "\n",
    "# Set the dimensions of the input, hidden, and output layers\n",
    "size_in = 17\n",
    "size_hidden = 100  # Can be anything - larger will take longer\n",
    "size_hidden2 = 100\n",
    "size_out = target.ndim\n",
    "\n",
    "# Set the seed value of the random number generator\n",
    "random_seed = 2\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Initialize the network weights, and place to store previous epoch weights\n",
    "weight_out = np.random.rand(size_out, size_hidden2 + 1) - 0.5\n",
    "weight_hidden2 = np.random.rand(size_hidden2, size_hidden + 1) - 0.5\n",
    "weight_hidden = np.random.rand(size_hidden, size_in + 1) - 0.5\n",
    "\n",
    "weight_hidden_prev = np.zeros(weight_hidden.shape)\n",
    "weight_out_prev = np.zeros(weight_out.shape)\n",
    "weight_hidden2_prev = np.zeros(weight_hidden2.shape)\n",
    "\n",
    "weight_hidden_val = np.zeros(weight_hidden.shape)\n",
    "weight_out_val = np.zeros(weight_out.shape)\n",
    "weight_hidden2_val = np.zeros(weight_hidden2.shape)\n",
    "# Initialize a vector to store the train errors for each epoch\n",
    "error_log = np.zeros([num_epochs])\n",
    "errorv_log = np.zeros([num_epochs])\n",
    "\n",
    "print(\"Outout node initial weights:\")\n",
    "print(weight_out)\n",
    "print(\"Hidden node initial weights:\")\n",
    "print(weight_hidden)\n",
    "print(\"Hidden node 2 initial weights:\")\n",
    "print(weight_hidden2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 18)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 101)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_hidden2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_val.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 29)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 18)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29,)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Training MSE: 0.1731504\n",
      "Iter: 0, Validation MSE: 0.1862108\n",
      "Iter: 50, Training MSE: 0.0510935\n",
      "Iter: 50, Validation MSE: 0.0494727\n",
      "Iter: 100, Training MSE: 0.0433430\n",
      "Iter: 100, Validation MSE: 0.0463951\n",
      "Iter: 150, Training MSE: 0.0397291\n",
      "Iter: 150, Validation MSE: 0.0478951\n",
      "Iter: 200, Training MSE: 0.0372867\n",
      "Iter: 200, Validation MSE: 0.0511792\n",
      "Iter: 250, Training MSE: 0.0354981\n",
      "Iter: 250, Validation MSE: 0.0552746\n",
      "Iter: 300, Training MSE: 0.0341004\n",
      "Iter: 300, Validation MSE: 0.0592716\n",
      "Iter: 350, Training MSE: 0.0329387\n",
      "Iter: 350, Validation MSE: 0.0627055\n",
      "Iter: 400, Training MSE: 0.0319451\n",
      "Iter: 400, Validation MSE: 0.0653370\n",
      "Iter: 450, Training MSE: 0.0310977\n",
      "Iter: 450, Validation MSE: 0.0671654\n",
      "Iter: 500, Training MSE: 0.0303866\n",
      "Iter: 500, Validation MSE: 0.0683635\n",
      "Iter: 550, Training MSE: 0.0297999\n",
      "Iter: 550, Validation MSE: 0.0691383\n",
      "Iter: 600, Training MSE: 0.0293215\n",
      "Iter: 600, Validation MSE: 0.0696517\n",
      "Iter: 650, Training MSE: 0.0289331\n",
      "Iter: 650, Validation MSE: 0.0700092\n",
      "Iter: 700, Training MSE: 0.0286170\n",
      "Iter: 700, Validation MSE: 0.0702736\n",
      "Iter: 750, Training MSE: 0.0283580\n",
      "Iter: 750, Validation MSE: 0.0704812\n",
      "Iter: 800, Training MSE: 0.0281432\n",
      "Iter: 800, Validation MSE: 0.0706528\n",
      "Iter: 850, Training MSE: 0.0279629\n",
      "Iter: 850, Validation MSE: 0.0708008\n",
      "Iter: 900, Training MSE: 0.0278093\n",
      "Iter: 900, Validation MSE: 0.0709325\n",
      "Iter: 950, Training MSE: 0.0276766\n",
      "Iter: 950, Validation MSE: 0.0710530\n",
      "Iter: 1000, Training MSE: 0.0275603\n",
      "Iter: 1000, Validation MSE: 0.0711656\n",
      "Iter: 1050, Training MSE: 0.0274569\n",
      "Iter: 1050, Validation MSE: 0.0712726\n",
      "Iter: 1100, Training MSE: 0.0273639\n",
      "Iter: 1100, Validation MSE: 0.0713760\n",
      "Iter: 1150, Training MSE: 0.0272792\n",
      "Iter: 1150, Validation MSE: 0.0714770\n",
      "Iter: 1200, Training MSE: 0.0272013\n",
      "Iter: 1200, Validation MSE: 0.0715767\n",
      "Iter: 1250, Training MSE: 0.0271287\n",
      "Iter: 1250, Validation MSE: 0.0716757\n",
      "Iter: 1300, Training MSE: 0.0270605\n",
      "Iter: 1300, Validation MSE: 0.0717746\n",
      "Iter: 1350, Training MSE: 0.0269957\n",
      "Iter: 1350, Validation MSE: 0.0718736\n",
      "Iter: 1400, Training MSE: 0.0269337\n",
      "Iter: 1400, Validation MSE: 0.0719731\n",
      "Iter: 1450, Training MSE: 0.0268737\n",
      "Iter: 1450, Validation MSE: 0.0720730\n",
      "Iter: 1500, Training MSE: 0.0268152\n",
      "Iter: 1500, Validation MSE: 0.0721734\n",
      "Iter: 1550, Training MSE: 0.0267576\n",
      "Iter: 1550, Validation MSE: 0.0722744\n",
      "Iter: 1600, Training MSE: 0.0267004\n",
      "Iter: 1600, Validation MSE: 0.0723762\n",
      "Iter: 1650, Training MSE: 0.0266430\n",
      "Iter: 1650, Validation MSE: 0.0724789\n",
      "Iter: 1700, Training MSE: 0.0265848\n",
      "Iter: 1700, Validation MSE: 0.0725829\n",
      "Iter: 1750, Training MSE: 0.0265254\n",
      "Iter: 1750, Validation MSE: 0.0726885\n",
      "Iter: 1800, Training MSE: 0.0264639\n",
      "Iter: 1800, Validation MSE: 0.0727962\n",
      "Iter: 1850, Training MSE: 0.0263997\n",
      "Iter: 1850, Validation MSE: 0.0729069\n",
      "Iter: 1900, Training MSE: 0.0263318\n",
      "Iter: 1900, Validation MSE: 0.0730213\n",
      "Iter: 1950, Training MSE: 0.0262593\n",
      "Iter: 1950, Validation MSE: 0.0731404\n",
      "Iter: 2000, Training MSE: 0.0261808\n",
      "Iter: 2000, Validation MSE: 0.0732652\n",
      "Iter: 2050, Training MSE: 0.0260948\n",
      "Iter: 2050, Validation MSE: 0.0733967\n",
      "Iter: 2100, Training MSE: 0.0259996\n",
      "Iter: 2100, Validation MSE: 0.0735350\n",
      "Iter: 2150, Training MSE: 0.0258932\n",
      "Iter: 2150, Validation MSE: 0.0736789\n",
      "Iter: 2200, Training MSE: 0.0257740\n",
      "Iter: 2200, Validation MSE: 0.0738253\n",
      "Iter: 2250, Training MSE: 0.0256409\n",
      "Iter: 2250, Validation MSE: 0.0739682\n",
      "Iter: 2300, Training MSE: 0.0254943\n",
      "Iter: 2300, Validation MSE: 0.0741005\n",
      "Iter: 2350, Training MSE: 0.0253356\n",
      "Iter: 2350, Validation MSE: 0.0742168\n",
      "Iter: 2400, Training MSE: 0.0251666\n",
      "Iter: 2400, Validation MSE: 0.0743169\n",
      "Iter: 2450, Training MSE: 0.0249890\n",
      "Iter: 2450, Validation MSE: 0.0744055\n",
      "Iter: 2500, Training MSE: 0.0248040\n",
      "Iter: 2500, Validation MSE: 0.0744904\n",
      "Iter: 2550, Training MSE: 0.0246123\n",
      "Iter: 2550, Validation MSE: 0.0745800\n",
      "Iter: 2600, Training MSE: 0.0244142\n",
      "Iter: 2600, Validation MSE: 0.0746821\n",
      "Iter: 2650, Training MSE: 0.0242102\n",
      "Iter: 2650, Validation MSE: 0.0748036\n",
      "Iter: 2700, Training MSE: 0.0240007\n",
      "Iter: 2700, Validation MSE: 0.0749505\n",
      "Iter: 2750, Training MSE: 0.0237861\n",
      "Iter: 2750, Validation MSE: 0.0751286\n",
      "Iter: 2800, Training MSE: 0.0235669\n",
      "Iter: 2800, Validation MSE: 0.0753435\n",
      "Iter: 2850, Training MSE: 0.0233436\n",
      "Iter: 2850, Validation MSE: 0.0756005\n",
      "Iter: 2900, Training MSE: 0.0231166\n",
      "Iter: 2900, Validation MSE: 0.0759051\n",
      "Iter: 2950, Training MSE: 0.0228863\n",
      "Iter: 2950, Validation MSE: 0.0762626\n",
      "Iter: 3000, Training MSE: 0.0226530\n",
      "Iter: 3000, Validation MSE: 0.0766778\n",
      "Iter: 3050, Training MSE: 0.0224167\n",
      "Iter: 3050, Validation MSE: 0.0771549\n",
      "Iter: 3100, Training MSE: 0.0221775\n",
      "Iter: 3100, Validation MSE: 0.0776971\n",
      "Iter: 3150, Training MSE: 0.0219352\n",
      "Iter: 3150, Validation MSE: 0.0783067\n",
      "Iter: 3200, Training MSE: 0.0216895\n",
      "Iter: 3200, Validation MSE: 0.0789841\n",
      "Iter: 3250, Training MSE: 0.0214403\n",
      "Iter: 3250, Validation MSE: 0.0797289\n",
      "Iter: 3300, Training MSE: 0.0211869\n",
      "Iter: 3300, Validation MSE: 0.0805391\n",
      "Iter: 3350, Training MSE: 0.0209293\n",
      "Iter: 3350, Validation MSE: 0.0814118\n",
      "Iter: 3400, Training MSE: 0.0206670\n",
      "Iter: 3400, Validation MSE: 0.0823434\n",
      "Iter: 3450, Training MSE: 0.0203999\n",
      "Iter: 3450, Validation MSE: 0.0833299\n",
      "Iter: 3500, Training MSE: 0.0201281\n",
      "Iter: 3500, Validation MSE: 0.0843670\n",
      "Iter: 3550, Training MSE: 0.0198518\n",
      "Iter: 3550, Validation MSE: 0.0854506\n",
      "Iter: 3600, Training MSE: 0.0195713\n",
      "Iter: 3600, Validation MSE: 0.0865764\n",
      "Iter: 3650, Training MSE: 0.0192875\n",
      "Iter: 3650, Validation MSE: 0.0877401\n",
      "Iter: 3700, Training MSE: 0.0190012\n",
      "Iter: 3700, Validation MSE: 0.0889374\n",
      "Iter: 3750, Training MSE: 0.0187136\n",
      "Iter: 3750, Validation MSE: 0.0901638\n",
      "Iter: 3800, Training MSE: 0.0184262\n",
      "Iter: 3800, Validation MSE: 0.0914144\n",
      "Iter: 3850, Training MSE: 0.0181405\n",
      "Iter: 3850, Validation MSE: 0.0926838\n",
      "Iter: 3900, Training MSE: 0.0178581\n",
      "Iter: 3900, Validation MSE: 0.0939659\n",
      "Iter: 3950, Training MSE: 0.0175808\n",
      "Iter: 3950, Validation MSE: 0.0952537\n",
      "Iter: 4000, Training MSE: 0.0173102\n",
      "Iter: 4000, Validation MSE: 0.0965393\n",
      "Iter: 4050, Training MSE: 0.0170480\n",
      "Iter: 4050, Validation MSE: 0.0978143\n",
      "Iter: 4100, Training MSE: 0.0167955\n",
      "Iter: 4100, Validation MSE: 0.0990697\n",
      "Iter: 4150, Training MSE: 0.0165538\n",
      "Iter: 4150, Validation MSE: 0.1002966\n",
      "Iter: 4200, Training MSE: 0.0163235\n",
      "Iter: 4200, Validation MSE: 0.1014866\n",
      "Iter: 4250, Training MSE: 0.0161052\n",
      "Iter: 4250, Validation MSE: 0.1026323\n",
      "Iter: 4300, Training MSE: 0.0158988\n",
      "Iter: 4300, Validation MSE: 0.1037282\n",
      "Iter: 4350, Training MSE: 0.0157042\n",
      "Iter: 4350, Validation MSE: 0.1047702\n",
      "Iter: 4400, Training MSE: 0.0155210\n",
      "Iter: 4400, Validation MSE: 0.1057562\n",
      "Iter: 4450, Training MSE: 0.0153489\n",
      "Iter: 4450, Validation MSE: 0.1066856\n",
      "Iter: 4500, Training MSE: 0.0151872\n",
      "Iter: 4500, Validation MSE: 0.1075592\n",
      "Iter: 4550, Training MSE: 0.0150355\n",
      "Iter: 4550, Validation MSE: 0.1083791\n",
      "Iter: 4600, Training MSE: 0.0148932\n",
      "Iter: 4600, Validation MSE: 0.1091479\n",
      "Iter: 4650, Training MSE: 0.0147599\n",
      "Iter: 4650, Validation MSE: 0.1098689\n",
      "Iter: 4700, Training MSE: 0.0146350\n",
      "Iter: 4700, Validation MSE: 0.1105456\n",
      "Iter: 4750, Training MSE: 0.0145180\n",
      "Iter: 4750, Validation MSE: 0.1111817\n",
      "Iter: 4800, Training MSE: 0.0144085\n",
      "Iter: 4800, Validation MSE: 0.1117808\n",
      "Iter: 4850, Training MSE: 0.0143060\n",
      "Iter: 4850, Validation MSE: 0.1123462\n",
      "Iter: 4900, Training MSE: 0.0142101\n",
      "Iter: 4900, Validation MSE: 0.1128812\n",
      "Iter: 4950, Training MSE: 0.0141203\n",
      "Iter: 4950, Validation MSE: 0.1133888\n",
      "Iter: 5000, Training MSE: 0.0140362\n",
      "Iter: 5000, Validation MSE: 0.1138718\n",
      "Iter: 5050, Training MSE: 0.0139574\n",
      "Iter: 5050, Validation MSE: 0.1143326\n",
      "Iter: 5100, Training MSE: 0.0138835\n",
      "Iter: 5100, Validation MSE: 0.1147735\n",
      "Iter: 5150, Training MSE: 0.0138142\n",
      "Iter: 5150, Validation MSE: 0.1151965\n",
      "Iter: 5200, Training MSE: 0.0137491\n",
      "Iter: 5200, Validation MSE: 0.1156036\n",
      "Iter: 5250, Training MSE: 0.0136879\n",
      "Iter: 5250, Validation MSE: 0.1159964\n",
      "Iter: 5300, Training MSE: 0.0136303\n",
      "Iter: 5300, Validation MSE: 0.1163764\n",
      "Iter: 5350, Training MSE: 0.0135761\n",
      "Iter: 5350, Validation MSE: 0.1167451\n",
      "Iter: 5400, Training MSE: 0.0135249\n",
      "Iter: 5400, Validation MSE: 0.1171038\n",
      "Iter: 5450, Training MSE: 0.0134765\n",
      "Iter: 5450, Validation MSE: 0.1174536\n",
      "Iter: 5500, Training MSE: 0.0134307\n",
      "Iter: 5500, Validation MSE: 0.1177958\n",
      "Iter: 5550, Training MSE: 0.0133872\n",
      "Iter: 5550, Validation MSE: 0.1181315\n",
      "Iter: 5600, Training MSE: 0.0133459\n",
      "Iter: 5600, Validation MSE: 0.1184617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 5650, Training MSE: 0.0133064\n",
      "Iter: 5650, Validation MSE: 0.1187875\n",
      "Iter: 5700, Training MSE: 0.0132686\n",
      "Iter: 5700, Validation MSE: 0.1191100\n",
      "Iter: 5750, Training MSE: 0.0132322\n",
      "Iter: 5750, Validation MSE: 0.1194303\n",
      "Iter: 5800, Training MSE: 0.0131971\n",
      "Iter: 5800, Validation MSE: 0.1197496\n",
      "Iter: 5850, Training MSE: 0.0131629\n",
      "Iter: 5850, Validation MSE: 0.1200690\n",
      "Iter: 5900, Training MSE: 0.0131294\n",
      "Iter: 5900, Validation MSE: 0.1203900\n",
      "Iter: 5950, Training MSE: 0.0130961\n",
      "Iter: 5950, Validation MSE: 0.1207138\n",
      "Iter: 6000, Training MSE: 0.0130627\n",
      "Iter: 6000, Validation MSE: 0.1210422\n",
      "Iter: 6050, Training MSE: 0.0130285\n",
      "Iter: 6050, Validation MSE: 0.1213766\n",
      "Iter: 6100, Training MSE: 0.0129927\n",
      "Iter: 6100, Validation MSE: 0.1217187\n",
      "Iter: 6150, Training MSE: 0.0129540\n",
      "Iter: 6150, Validation MSE: 0.1220700\n",
      "Iter: 6200, Training MSE: 0.0129106\n",
      "Iter: 6200, Validation MSE: 0.1224310\n",
      "Iter: 6250, Training MSE: 0.0128599\n",
      "Iter: 6250, Validation MSE: 0.1228005\n",
      "Iter: 6300, Training MSE: 0.0127983\n",
      "Iter: 6300, Validation MSE: 0.1231735\n",
      "Iter: 6350, Training MSE: 0.0127217\n",
      "Iter: 6350, Validation MSE: 0.1235390\n",
      "Iter: 6400, Training MSE: 0.0126280\n",
      "Iter: 6400, Validation MSE: 0.1238784\n",
      "Iter: 6450, Training MSE: 0.0125200\n",
      "Iter: 6450, Validation MSE: 0.1241684\n",
      "Iter: 6500, Training MSE: 0.0124049\n",
      "Iter: 6500, Validation MSE: 0.1243896\n",
      "Iter: 6550, Training MSE: 0.0122896\n",
      "Iter: 6550, Validation MSE: 0.1245375\n",
      "Iter: 6600, Training MSE: 0.0121774\n",
      "Iter: 6600, Validation MSE: 0.1246249\n",
      "Iter: 6650, Training MSE: 0.0120695\n",
      "Iter: 6650, Validation MSE: 0.1246729\n",
      "Iter: 6700, Training MSE: 0.0119659\n",
      "Iter: 6700, Validation MSE: 0.1247009\n",
      "Iter: 6750, Training MSE: 0.0118663\n",
      "Iter: 6750, Validation MSE: 0.1247232\n",
      "Iter: 6800, Training MSE: 0.0117705\n",
      "Iter: 6800, Validation MSE: 0.1247478\n",
      "Iter: 6850, Training MSE: 0.0116783\n",
      "Iter: 6850, Validation MSE: 0.1247789\n",
      "Iter: 6900, Training MSE: 0.0115897\n",
      "Iter: 6900, Validation MSE: 0.1248179\n",
      "Iter: 6950, Training MSE: 0.0115045\n",
      "Iter: 6950, Validation MSE: 0.1248645\n",
      "Iter: 7000, Training MSE: 0.0114226\n",
      "Iter: 7000, Validation MSE: 0.1249179\n",
      "Iter: 7050, Training MSE: 0.0113439\n",
      "Iter: 7050, Validation MSE: 0.1249769\n",
      "Iter: 7100, Training MSE: 0.0112685\n",
      "Iter: 7100, Validation MSE: 0.1250399\n",
      "Iter: 7150, Training MSE: 0.0111962\n",
      "Iter: 7150, Validation MSE: 0.1251058\n",
      "Iter: 7200, Training MSE: 0.0111270\n",
      "Iter: 7200, Validation MSE: 0.1251733\n",
      "Iter: 7250, Training MSE: 0.0110607\n",
      "Iter: 7250, Validation MSE: 0.1252414\n",
      "Iter: 7300, Training MSE: 0.0109972\n",
      "Iter: 7300, Validation MSE: 0.1253092\n",
      "Iter: 7350, Training MSE: 0.0109366\n",
      "Iter: 7350, Validation MSE: 0.1253759\n",
      "Iter: 7400, Training MSE: 0.0108786\n",
      "Iter: 7400, Validation MSE: 0.1254410\n",
      "Iter: 7450, Training MSE: 0.0108232\n",
      "Iter: 7450, Validation MSE: 0.1255039\n",
      "Iter: 7500, Training MSE: 0.0107703\n",
      "Iter: 7500, Validation MSE: 0.1255643\n",
      "Iter: 7550, Training MSE: 0.0107197\n",
      "Iter: 7550, Validation MSE: 0.1256218\n",
      "Iter: 7600, Training MSE: 0.0106714\n",
      "Iter: 7600, Validation MSE: 0.1256763\n",
      "Iter: 7650, Training MSE: 0.0106253\n",
      "Iter: 7650, Validation MSE: 0.1257275\n",
      "Iter: 7700, Training MSE: 0.0105812\n",
      "Iter: 7700, Validation MSE: 0.1257754\n",
      "Iter: 7750, Training MSE: 0.0105391\n",
      "Iter: 7750, Validation MSE: 0.1258200\n",
      "Iter: 7800, Training MSE: 0.0104989\n",
      "Iter: 7800, Validation MSE: 0.1258612\n",
      "Iter: 7850, Training MSE: 0.0104605\n",
      "Iter: 7850, Validation MSE: 0.1258990\n",
      "Iter: 7900, Training MSE: 0.0104237\n",
      "Iter: 7900, Validation MSE: 0.1259335\n",
      "Iter: 7950, Training MSE: 0.0103886\n",
      "Iter: 7950, Validation MSE: 0.1259647\n",
      "Iter: 8000, Training MSE: 0.0103551\n",
      "Iter: 8000, Validation MSE: 0.1259928\n",
      "Iter: 8050, Training MSE: 0.0103229\n",
      "Iter: 8050, Validation MSE: 0.1260178\n",
      "Iter: 8100, Training MSE: 0.0102922\n",
      "Iter: 8100, Validation MSE: 0.1260399\n",
      "Iter: 8150, Training MSE: 0.0102628\n",
      "Iter: 8150, Validation MSE: 0.1260591\n",
      "Iter: 8200, Training MSE: 0.0102347\n",
      "Iter: 8200, Validation MSE: 0.1260755\n",
      "Iter: 8250, Training MSE: 0.0102077\n",
      "Iter: 8250, Validation MSE: 0.1260893\n",
      "Iter: 8300, Training MSE: 0.0101819\n",
      "Iter: 8300, Validation MSE: 0.1261006\n",
      "Iter: 8350, Training MSE: 0.0101571\n",
      "Iter: 8350, Validation MSE: 0.1261094\n",
      "Iter: 8400, Training MSE: 0.0101334\n",
      "Iter: 8400, Validation MSE: 0.1261160\n",
      "Iter: 8450, Training MSE: 0.0101106\n",
      "Iter: 8450, Validation MSE: 0.1261205\n",
      "Iter: 8500, Training MSE: 0.0100888\n",
      "Iter: 8500, Validation MSE: 0.1261228\n",
      "Iter: 8550, Training MSE: 0.0100678\n",
      "Iter: 8550, Validation MSE: 0.1261232\n",
      "Iter: 8600, Training MSE: 0.0100476\n",
      "Iter: 8600, Validation MSE: 0.1261218\n",
      "Iter: 8650, Training MSE: 0.0100283\n",
      "Iter: 8650, Validation MSE: 0.1261187\n",
      "Iter: 8700, Training MSE: 0.0100097\n",
      "Iter: 8700, Validation MSE: 0.1261139\n",
      "Iter: 8750, Training MSE: 0.0099918\n",
      "Iter: 8750, Validation MSE: 0.1261076\n",
      "Iter: 8800, Training MSE: 0.0099746\n",
      "Iter: 8800, Validation MSE: 0.1260998\n",
      "Iter: 8850, Training MSE: 0.0099581\n",
      "Iter: 8850, Validation MSE: 0.1260906\n",
      "Iter: 8900, Training MSE: 0.0099422\n",
      "Iter: 8900, Validation MSE: 0.1260802\n",
      "Iter: 8950, Training MSE: 0.0099269\n",
      "Iter: 8950, Validation MSE: 0.1260686\n",
      "Iter: 9000, Training MSE: 0.0099121\n",
      "Iter: 9000, Validation MSE: 0.1260558\n",
      "Iter: 9050, Training MSE: 0.0098979\n",
      "Iter: 9050, Validation MSE: 0.1260420\n",
      "Iter: 9100, Training MSE: 0.0098841\n",
      "Iter: 9100, Validation MSE: 0.1260273\n",
      "Iter: 9150, Training MSE: 0.0098709\n",
      "Iter: 9150, Validation MSE: 0.1260116\n",
      "Iter: 9200, Training MSE: 0.0098582\n",
      "Iter: 9200, Validation MSE: 0.1259950\n",
      "Iter: 9250, Training MSE: 0.0098458\n",
      "Iter: 9250, Validation MSE: 0.1259777\n",
      "Iter: 9300, Training MSE: 0.0098340\n",
      "Iter: 9300, Validation MSE: 0.1259596\n",
      "Iter: 9350, Training MSE: 0.0098225\n",
      "Iter: 9350, Validation MSE: 0.1259408\n",
      "Iter: 9400, Training MSE: 0.0098114\n",
      "Iter: 9400, Validation MSE: 0.1259214\n",
      "Iter: 9450, Training MSE: 0.0098007\n",
      "Iter: 9450, Validation MSE: 0.1259014\n",
      "Iter: 9500, Training MSE: 0.0097903\n",
      "Iter: 9500, Validation MSE: 0.1258808\n",
      "Iter: 9550, Training MSE: 0.0097803\n",
      "Iter: 9550, Validation MSE: 0.1258598\n",
      "Iter: 9600, Training MSE: 0.0097706\n",
      "Iter: 9600, Validation MSE: 0.1258382\n",
      "Iter: 9650, Training MSE: 0.0097612\n",
      "Iter: 9650, Validation MSE: 0.1258163\n",
      "Iter: 9700, Training MSE: 0.0097521\n",
      "Iter: 9700, Validation MSE: 0.1257940\n",
      "Iter: 9750, Training MSE: 0.0097433\n",
      "Iter: 9750, Validation MSE: 0.1257713\n",
      "Iter: 9800, Training MSE: 0.0097348\n",
      "Iter: 9800, Validation MSE: 0.1257482\n",
      "Iter: 9850, Training MSE: 0.0097265\n",
      "Iter: 9850, Validation MSE: 0.1257249\n",
      "Iter: 9900, Training MSE: 0.0097185\n",
      "Iter: 9900, Validation MSE: 0.1257013\n",
      "Iter: 9950, Training MSE: 0.0097108\n",
      "Iter: 9950, Validation MSE: 0.1256775\n",
      "Iter: 10000, Training MSE: 0.0097033\n",
      "Iter: 10000, Validation MSE: 0.1256534\n",
      "Iter: 10050, Training MSE: 0.0096959\n",
      "Iter: 10050, Validation MSE: 0.1256291\n",
      "Iter: 10100, Training MSE: 0.0096889\n",
      "Iter: 10100, Validation MSE: 0.1256047\n",
      "Iter: 10150, Training MSE: 0.0096820\n",
      "Iter: 10150, Validation MSE: 0.1255801\n",
      "Iter: 10200, Training MSE: 0.0096753\n",
      "Iter: 10200, Validation MSE: 0.1255554\n",
      "Iter: 10250, Training MSE: 0.0096688\n",
      "Iter: 10250, Validation MSE: 0.1255305\n",
      "Iter: 10300, Training MSE: 0.0096625\n",
      "Iter: 10300, Validation MSE: 0.1255056\n",
      "Iter: 10350, Training MSE: 0.0096564\n",
      "Iter: 10350, Validation MSE: 0.1254806\n",
      "Iter: 10400, Training MSE: 0.0096504\n",
      "Iter: 10400, Validation MSE: 0.1254555\n",
      "Iter: 10450, Training MSE: 0.0096447\n",
      "Iter: 10450, Validation MSE: 0.1254303\n",
      "Iter: 10500, Training MSE: 0.0096390\n",
      "Iter: 10500, Validation MSE: 0.1254051\n",
      "Iter: 10550, Training MSE: 0.0096335\n",
      "Iter: 10550, Validation MSE: 0.1253799\n",
      "Iter: 10600, Training MSE: 0.0096282\n",
      "Iter: 10600, Validation MSE: 0.1253547\n",
      "Iter: 10650, Training MSE: 0.0096230\n",
      "Iter: 10650, Validation MSE: 0.1253294\n",
      "Iter: 10700, Training MSE: 0.0096180\n",
      "Iter: 10700, Validation MSE: 0.1253041\n",
      "Iter: 10750, Training MSE: 0.0096131\n",
      "Iter: 10750, Validation MSE: 0.1252789\n",
      "Iter: 10800, Training MSE: 0.0096083\n",
      "Iter: 10800, Validation MSE: 0.1252537\n",
      "Iter: 10850, Training MSE: 0.0096036\n",
      "Iter: 10850, Validation MSE: 0.1252285\n",
      "Iter: 10900, Training MSE: 0.0095991\n",
      "Iter: 10900, Validation MSE: 0.1252033\n",
      "Iter: 10950, Training MSE: 0.0095946\n",
      "Iter: 10950, Validation MSE: 0.1251782\n",
      "Iter: 11000, Training MSE: 0.0095903\n",
      "Iter: 11000, Validation MSE: 0.1251531\n",
      "Iter: 11050, Training MSE: 0.0095861\n",
      "Iter: 11050, Validation MSE: 0.1251281\n",
      "Iter: 11100, Training MSE: 0.0095820\n",
      "Iter: 11100, Validation MSE: 0.1251032\n",
      "Iter: 11150, Training MSE: 0.0095780\n",
      "Iter: 11150, Validation MSE: 0.1250783\n",
      "Iter: 11200, Training MSE: 0.0095741\n",
      "Iter: 11200, Validation MSE: 0.1250535\n",
      "Iter: 11250, Training MSE: 0.0095703\n",
      "Iter: 11250, Validation MSE: 0.1250287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 11300, Training MSE: 0.0095665\n",
      "Iter: 11300, Validation MSE: 0.1250041\n",
      "Iter: 11350, Training MSE: 0.0095629\n",
      "Iter: 11350, Validation MSE: 0.1249795\n",
      "Iter: 11400, Training MSE: 0.0095593\n",
      "Iter: 11400, Validation MSE: 0.1249550\n",
      "Iter: 11450, Training MSE: 0.0095559\n",
      "Iter: 11450, Validation MSE: 0.1249307\n",
      "Iter: 11500, Training MSE: 0.0095525\n",
      "Iter: 11500, Validation MSE: 0.1249064\n",
      "Iter: 11550, Training MSE: 0.0095492\n",
      "Iter: 11550, Validation MSE: 0.1248822\n",
      "Iter: 11600, Training MSE: 0.0095459\n",
      "Iter: 11600, Validation MSE: 0.1248581\n",
      "Iter: 11650, Training MSE: 0.0095428\n",
      "Iter: 11650, Validation MSE: 0.1248341\n",
      "Iter: 11700, Training MSE: 0.0095397\n",
      "Iter: 11700, Validation MSE: 0.1248102\n",
      "Iter: 11750, Training MSE: 0.0095367\n",
      "Iter: 11750, Validation MSE: 0.1247864\n",
      "Iter: 11800, Training MSE: 0.0095337\n",
      "Iter: 11800, Validation MSE: 0.1247627\n",
      "Iter: 11850, Training MSE: 0.0095308\n",
      "Iter: 11850, Validation MSE: 0.1247392\n",
      "Iter: 11900, Training MSE: 0.0095280\n",
      "Iter: 11900, Validation MSE: 0.1247157\n",
      "Iter: 11950, Training MSE: 0.0095252\n",
      "Iter: 11950, Validation MSE: 0.1246924\n",
      "Iter: 12000, Training MSE: 0.0095225\n",
      "Iter: 12000, Validation MSE: 0.1246692\n",
      "Iter: 12050, Training MSE: 0.0095198\n",
      "Iter: 12050, Validation MSE: 0.1246461\n",
      "Iter: 12100, Training MSE: 0.0095172\n",
      "Iter: 12100, Validation MSE: 0.1246231\n",
      "Iter: 12150, Training MSE: 0.0095147\n",
      "Iter: 12150, Validation MSE: 0.1246002\n",
      "Iter: 12200, Training MSE: 0.0095122\n",
      "Iter: 12200, Validation MSE: 0.1245775\n",
      "Iter: 12250, Training MSE: 0.0095098\n",
      "Iter: 12250, Validation MSE: 0.1245548\n",
      "Iter: 12300, Training MSE: 0.0095074\n",
      "Iter: 12300, Validation MSE: 0.1245323\n",
      "Iter: 12350, Training MSE: 0.0095050\n",
      "Iter: 12350, Validation MSE: 0.1245099\n",
      "Iter: 12400, Training MSE: 0.0095027\n",
      "Iter: 12400, Validation MSE: 0.1244877\n",
      "Iter: 12450, Training MSE: 0.0095005\n",
      "Iter: 12450, Validation MSE: 0.1244655\n",
      "Iter: 12500, Training MSE: 0.0094983\n",
      "Iter: 12500, Validation MSE: 0.1244435\n",
      "Iter: 12550, Training MSE: 0.0094961\n",
      "Iter: 12550, Validation MSE: 0.1244216\n",
      "Iter: 12600, Training MSE: 0.0094940\n",
      "Iter: 12600, Validation MSE: 0.1243998\n",
      "Iter: 12650, Training MSE: 0.0094919\n",
      "Iter: 12650, Validation MSE: 0.1243781\n",
      "Iter: 12700, Training MSE: 0.0094899\n",
      "Iter: 12700, Validation MSE: 0.1243566\n",
      "Iter: 12750, Training MSE: 0.0094879\n",
      "Iter: 12750, Validation MSE: 0.1243351\n",
      "Iter: 12800, Training MSE: 0.0094860\n",
      "Iter: 12800, Validation MSE: 0.1243138\n",
      "Iter: 12850, Training MSE: 0.0094840\n",
      "Iter: 12850, Validation MSE: 0.1242926\n",
      "Iter: 12900, Training MSE: 0.0094821\n",
      "Iter: 12900, Validation MSE: 0.1242716\n",
      "Iter: 12950, Training MSE: 0.0094803\n",
      "Iter: 12950, Validation MSE: 0.1242506\n",
      "Iter: 13000, Training MSE: 0.0094785\n",
      "Iter: 13000, Validation MSE: 0.1242298\n",
      "Iter: 13050, Training MSE: 0.0094767\n",
      "Iter: 13050, Validation MSE: 0.1242091\n",
      "Iter: 13100, Training MSE: 0.0094749\n",
      "Iter: 13100, Validation MSE: 0.1241885\n",
      "Iter: 13150, Training MSE: 0.0094732\n",
      "Iter: 13150, Validation MSE: 0.1241680\n",
      "Iter: 13200, Training MSE: 0.0094715\n",
      "Iter: 13200, Validation MSE: 0.1241477\n",
      "Iter: 13250, Training MSE: 0.0094699\n",
      "Iter: 13250, Validation MSE: 0.1241275\n",
      "Iter: 13300, Training MSE: 0.0094682\n",
      "Iter: 13300, Validation MSE: 0.1241073\n",
      "Iter: 13350, Training MSE: 0.0094666\n",
      "Iter: 13350, Validation MSE: 0.1240873\n",
      "Iter: 13400, Training MSE: 0.0094651\n",
      "Iter: 13400, Validation MSE: 0.1240674\n",
      "Iter: 13450, Training MSE: 0.0094635\n",
      "Iter: 13450, Validation MSE: 0.1240477\n",
      "Iter: 13500, Training MSE: 0.0094620\n",
      "Iter: 13500, Validation MSE: 0.1240280\n",
      "Iter: 13550, Training MSE: 0.0094605\n",
      "Iter: 13550, Validation MSE: 0.1240085\n",
      "Iter: 13600, Training MSE: 0.0094590\n",
      "Iter: 13600, Validation MSE: 0.1239890\n",
      "Iter: 13650, Training MSE: 0.0094576\n",
      "Iter: 13650, Validation MSE: 0.1239697\n",
      "Iter: 13700, Training MSE: 0.0094562\n",
      "Iter: 13700, Validation MSE: 0.1239505\n",
      "Iter: 13750, Training MSE: 0.0094548\n",
      "Iter: 13750, Validation MSE: 0.1239314\n",
      "Iter: 13800, Training MSE: 0.0094534\n",
      "Iter: 13800, Validation MSE: 0.1239124\n",
      "Iter: 13850, Training MSE: 0.0094521\n",
      "Iter: 13850, Validation MSE: 0.1238936\n",
      "Iter: 13900, Training MSE: 0.0094508\n",
      "Iter: 13900, Validation MSE: 0.1238748\n",
      "Iter: 13950, Training MSE: 0.0094494\n",
      "Iter: 13950, Validation MSE: 0.1238561\n",
      "Iter: 14000, Training MSE: 0.0094482\n",
      "Iter: 14000, Validation MSE: 0.1238376\n",
      "Iter: 14050, Training MSE: 0.0094469\n",
      "Iter: 14050, Validation MSE: 0.1238191\n",
      "Iter: 14100, Training MSE: 0.0094457\n",
      "Iter: 14100, Validation MSE: 0.1238008\n",
      "Iter: 14150, Training MSE: 0.0094444\n",
      "Iter: 14150, Validation MSE: 0.1237826\n",
      "Iter: 14200, Training MSE: 0.0094432\n",
      "Iter: 14200, Validation MSE: 0.1237645\n",
      "Iter: 14250, Training MSE: 0.0094421\n",
      "Iter: 14250, Validation MSE: 0.1237464\n",
      "Iter: 14300, Training MSE: 0.0094409\n",
      "Iter: 14300, Validation MSE: 0.1237285\n",
      "Iter: 14350, Training MSE: 0.0094397\n",
      "Iter: 14350, Validation MSE: 0.1237107\n",
      "Iter: 14400, Training MSE: 0.0094386\n",
      "Iter: 14400, Validation MSE: 0.1236930\n",
      "Iter: 14450, Training MSE: 0.0094375\n",
      "Iter: 14450, Validation MSE: 0.1236754\n",
      "Iter: 14500, Training MSE: 0.0094364\n",
      "Iter: 14500, Validation MSE: 0.1236578\n",
      "Iter: 14550, Training MSE: 0.0094353\n",
      "Iter: 14550, Validation MSE: 0.1236404\n",
      "Iter: 14600, Training MSE: 0.0094343\n",
      "Iter: 14600, Validation MSE: 0.1236231\n",
      "Iter: 14650, Training MSE: 0.0094332\n",
      "Iter: 14650, Validation MSE: 0.1236059\n",
      "Iter: 14700, Training MSE: 0.0094322\n",
      "Iter: 14700, Validation MSE: 0.1235888\n",
      "Iter: 14750, Training MSE: 0.0094312\n",
      "Iter: 14750, Validation MSE: 0.1235718\n",
      "Iter: 14800, Training MSE: 0.0094302\n",
      "Iter: 14800, Validation MSE: 0.1235548\n",
      "Iter: 14850, Training MSE: 0.0094292\n",
      "Iter: 14850, Validation MSE: 0.1235380\n",
      "Iter: 14900, Training MSE: 0.0094282\n",
      "Iter: 14900, Validation MSE: 0.1235213\n",
      "Iter: 14950, Training MSE: 0.0094273\n",
      "Iter: 14950, Validation MSE: 0.1235046\n",
      "Iter: 15000, Training MSE: 0.0094263\n",
      "Iter: 15000, Validation MSE: 0.1234881\n",
      "Iter: 15050, Training MSE: 0.0094254\n",
      "Iter: 15050, Validation MSE: 0.1234716\n",
      "Iter: 15100, Training MSE: 0.0094245\n",
      "Iter: 15100, Validation MSE: 0.1234552\n",
      "Iter: 15150, Training MSE: 0.0094236\n",
      "Iter: 15150, Validation MSE: 0.1234389\n",
      "Iter: 15200, Training MSE: 0.0094227\n",
      "Iter: 15200, Validation MSE: 0.1234228\n",
      "Iter: 15250, Training MSE: 0.0094218\n",
      "Iter: 15250, Validation MSE: 0.1234067\n",
      "Iter: 15300, Training MSE: 0.0094209\n",
      "Iter: 15300, Validation MSE: 0.1233907\n",
      "Iter: 15350, Training MSE: 0.0094201\n",
      "Iter: 15350, Validation MSE: 0.1233747\n",
      "Iter: 15400, Training MSE: 0.0094192\n",
      "Iter: 15400, Validation MSE: 0.1233589\n",
      "Iter: 15450, Training MSE: 0.0094184\n",
      "Iter: 15450, Validation MSE: 0.1233431\n",
      "Iter: 15500, Training MSE: 0.0094176\n",
      "Iter: 15500, Validation MSE: 0.1233275\n",
      "Iter: 15550, Training MSE: 0.0094168\n",
      "Iter: 15550, Validation MSE: 0.1233119\n",
      "Iter: 15600, Training MSE: 0.0094160\n",
      "Iter: 15600, Validation MSE: 0.1232964\n",
      "Iter: 15650, Training MSE: 0.0094152\n",
      "Iter: 15650, Validation MSE: 0.1232810\n",
      "Iter: 15700, Training MSE: 0.0094144\n",
      "Iter: 15700, Validation MSE: 0.1232657\n",
      "Iter: 15750, Training MSE: 0.0094137\n",
      "Iter: 15750, Validation MSE: 0.1232504\n",
      "Iter: 15800, Training MSE: 0.0094129\n",
      "Iter: 15800, Validation MSE: 0.1232353\n",
      "Iter: 15850, Training MSE: 0.0094122\n",
      "Iter: 15850, Validation MSE: 0.1232202\n",
      "Iter: 15900, Training MSE: 0.0094114\n",
      "Iter: 15900, Validation MSE: 0.1232052\n",
      "Iter: 15950, Training MSE: 0.0094107\n",
      "Iter: 15950, Validation MSE: 0.1231903\n",
      "Iter: 16000, Training MSE: 0.0094100\n",
      "Iter: 16000, Validation MSE: 0.1231754\n",
      "Iter: 16050, Training MSE: 0.0094093\n",
      "Iter: 16050, Validation MSE: 0.1231607\n",
      "Iter: 16100, Training MSE: 0.0094086\n",
      "Iter: 16100, Validation MSE: 0.1231460\n",
      "Iter: 16150, Training MSE: 0.0094079\n",
      "Iter: 16150, Validation MSE: 0.1231314\n",
      "Iter: 16200, Training MSE: 0.0094072\n",
      "Iter: 16200, Validation MSE: 0.1231168\n",
      "Iter: 16250, Training MSE: 0.0094065\n",
      "Iter: 16250, Validation MSE: 0.1231024\n",
      "Iter: 16300, Training MSE: 0.0094059\n",
      "Iter: 16300, Validation MSE: 0.1230880\n",
      "Iter: 16350, Training MSE: 0.0094052\n",
      "Iter: 16350, Validation MSE: 0.1230737\n",
      "Iter: 16400, Training MSE: 0.0094046\n",
      "Iter: 16400, Validation MSE: 0.1230595\n",
      "Iter: 16450, Training MSE: 0.0094039\n",
      "Iter: 16450, Validation MSE: 0.1230453\n",
      "Iter: 16500, Training MSE: 0.0094033\n",
      "Iter: 16500, Validation MSE: 0.1230312\n",
      "Iter: 16550, Training MSE: 0.0094027\n",
      "Iter: 16550, Validation MSE: 0.1230172\n",
      "Iter: 16600, Training MSE: 0.0094021\n",
      "Iter: 16600, Validation MSE: 0.1230033\n",
      "Iter: 16650, Training MSE: 0.0094015\n",
      "Iter: 16650, Validation MSE: 0.1229894\n",
      "Iter: 16700, Training MSE: 0.0094009\n",
      "Iter: 16700, Validation MSE: 0.1229756\n",
      "Iter: 16750, Training MSE: 0.0094003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 16750, Validation MSE: 0.1229619\n",
      "Iter: 16800, Training MSE: 0.0093997\n",
      "Iter: 16800, Validation MSE: 0.1229482\n",
      "Iter: 16850, Training MSE: 0.0093991\n",
      "Iter: 16850, Validation MSE: 0.1229346\n",
      "Iter: 16900, Training MSE: 0.0093985\n",
      "Iter: 16900, Validation MSE: 0.1229211\n",
      "Iter: 16950, Training MSE: 0.0093980\n",
      "Iter: 16950, Validation MSE: 0.1229077\n",
      "Iter: 17000, Training MSE: 0.0093974\n",
      "Iter: 17000, Validation MSE: 0.1228943\n",
      "Iter: 17050, Training MSE: 0.0093969\n",
      "Iter: 17050, Validation MSE: 0.1228810\n",
      "Iter: 17100, Training MSE: 0.0093963\n",
      "Iter: 17100, Validation MSE: 0.1228677\n",
      "Iter: 17150, Training MSE: 0.0093958\n",
      "Iter: 17150, Validation MSE: 0.1228545\n",
      "Iter: 17200, Training MSE: 0.0093952\n",
      "Iter: 17200, Validation MSE: 0.1228414\n",
      "Iter: 17250, Training MSE: 0.0093947\n",
      "Iter: 17250, Validation MSE: 0.1228283\n",
      "Iter: 17300, Training MSE: 0.0093942\n",
      "Iter: 17300, Validation MSE: 0.1228154\n",
      "Iter: 17350, Training MSE: 0.0093937\n",
      "Iter: 17350, Validation MSE: 0.1228024\n",
      "Iter: 17400, Training MSE: 0.0093932\n",
      "Iter: 17400, Validation MSE: 0.1227896\n",
      "Iter: 17450, Training MSE: 0.0093927\n",
      "Iter: 17450, Validation MSE: 0.1227768\n",
      "Iter: 17500, Training MSE: 0.0093922\n",
      "Iter: 17500, Validation MSE: 0.1227640\n",
      "Iter: 17550, Training MSE: 0.0093917\n",
      "Iter: 17550, Validation MSE: 0.1227513\n",
      "Iter: 17600, Training MSE: 0.0093912\n",
      "Iter: 17600, Validation MSE: 0.1227387\n",
      "Iter: 17650, Training MSE: 0.0093907\n",
      "Iter: 17650, Validation MSE: 0.1227262\n",
      "Iter: 17700, Training MSE: 0.0093902\n",
      "Iter: 17700, Validation MSE: 0.1227137\n",
      "Iter: 17750, Training MSE: 0.0093898\n",
      "Iter: 17750, Validation MSE: 0.1227012\n",
      "Iter: 17800, Training MSE: 0.0093893\n",
      "Iter: 17800, Validation MSE: 0.1226888\n",
      "Iter: 17850, Training MSE: 0.0093888\n",
      "Iter: 17850, Validation MSE: 0.1226765\n",
      "Iter: 17900, Training MSE: 0.0093884\n",
      "Iter: 17900, Validation MSE: 0.1226643\n",
      "Iter: 17950, Training MSE: 0.0093879\n",
      "Iter: 17950, Validation MSE: 0.1226520\n",
      "Iter: 18000, Training MSE: 0.0093875\n",
      "Iter: 18000, Validation MSE: 0.1226399\n",
      "Iter: 18050, Training MSE: 0.0093871\n",
      "Iter: 18050, Validation MSE: 0.1226278\n",
      "Iter: 18100, Training MSE: 0.0093866\n",
      "Iter: 18100, Validation MSE: 0.1226158\n",
      "Iter: 18150, Training MSE: 0.0093862\n",
      "Iter: 18150, Validation MSE: 0.1226038\n",
      "Iter: 18200, Training MSE: 0.0093858\n",
      "Iter: 18200, Validation MSE: 0.1225919\n",
      "Iter: 18250, Training MSE: 0.0093853\n",
      "Iter: 18250, Validation MSE: 0.1225800\n",
      "Iter: 18300, Training MSE: 0.0093849\n",
      "Iter: 18300, Validation MSE: 0.1225682\n",
      "Iter: 18350, Training MSE: 0.0093845\n",
      "Iter: 18350, Validation MSE: 0.1225564\n",
      "Iter: 18400, Training MSE: 0.0093841\n",
      "Iter: 18400, Validation MSE: 0.1225447\n",
      "Iter: 18450, Training MSE: 0.0093837\n",
      "Iter: 18450, Validation MSE: 0.1225331\n",
      "Iter: 18500, Training MSE: 0.0093833\n",
      "Iter: 18500, Validation MSE: 0.1225215\n",
      "Iter: 18550, Training MSE: 0.0093829\n",
      "Iter: 18550, Validation MSE: 0.1225099\n",
      "Iter: 18600, Training MSE: 0.0093825\n",
      "Iter: 18600, Validation MSE: 0.1224984\n",
      "Iter: 18650, Training MSE: 0.0093821\n",
      "Iter: 18650, Validation MSE: 0.1224870\n",
      "Iter: 18700, Training MSE: 0.0093817\n",
      "Iter: 18700, Validation MSE: 0.1224756\n",
      "Iter: 18750, Training MSE: 0.0093813\n",
      "Iter: 18750, Validation MSE: 0.1224643\n",
      "Iter: 18800, Training MSE: 0.0093810\n",
      "Iter: 18800, Validation MSE: 0.1224530\n",
      "Iter: 18850, Training MSE: 0.0093806\n",
      "Iter: 18850, Validation MSE: 0.1224417\n",
      "Iter: 18900, Training MSE: 0.0093802\n",
      "Iter: 18900, Validation MSE: 0.1224305\n",
      "Iter: 18950, Training MSE: 0.0093799\n",
      "Iter: 18950, Validation MSE: 0.1224194\n",
      "Iter: 19000, Training MSE: 0.0093795\n",
      "Iter: 19000, Validation MSE: 0.1224083\n",
      "Iter: 19050, Training MSE: 0.0093791\n",
      "Iter: 19050, Validation MSE: 0.1223972\n",
      "Iter: 19100, Training MSE: 0.0093788\n",
      "Iter: 19100, Validation MSE: 0.1223863\n",
      "Iter: 19150, Training MSE: 0.0093784\n",
      "Iter: 19150, Validation MSE: 0.1223753\n",
      "Iter: 19200, Training MSE: 0.0093781\n",
      "Iter: 19200, Validation MSE: 0.1223644\n",
      "Iter: 19250, Training MSE: 0.0093777\n",
      "Iter: 19250, Validation MSE: 0.1223535\n",
      "Iter: 19300, Training MSE: 0.0093774\n",
      "Iter: 19300, Validation MSE: 0.1223427\n",
      "Iter: 19350, Training MSE: 0.0093771\n",
      "Iter: 19350, Validation MSE: 0.1223320\n",
      "Iter: 19400, Training MSE: 0.0093767\n",
      "Iter: 19400, Validation MSE: 0.1223213\n",
      "Iter: 19450, Training MSE: 0.0093764\n",
      "Iter: 19450, Validation MSE: 0.1223106\n",
      "Iter: 19500, Training MSE: 0.0093761\n",
      "Iter: 19500, Validation MSE: 0.1223000\n",
      "Iter: 19550, Training MSE: 0.0093757\n",
      "Iter: 19550, Validation MSE: 0.1222894\n",
      "Iter: 19600, Training MSE: 0.0093754\n",
      "Iter: 19600, Validation MSE: 0.1222789\n",
      "Iter: 19650, Training MSE: 0.0093751\n",
      "Iter: 19650, Validation MSE: 0.1222684\n",
      "Iter: 19700, Training MSE: 0.0093748\n",
      "Iter: 19700, Validation MSE: 0.1222579\n",
      "Iter: 19750, Training MSE: 0.0093745\n",
      "Iter: 19750, Validation MSE: 0.1222475\n",
      "Iter: 19800, Training MSE: 0.0093742\n",
      "Iter: 19800, Validation MSE: 0.1222372\n",
      "Iter: 19850, Training MSE: 0.0093739\n",
      "Iter: 19850, Validation MSE: 0.1222268\n",
      "Iter: 19900, Training MSE: 0.0093736\n",
      "Iter: 19900, Validation MSE: 0.1222166\n",
      "Iter: 19950, Training MSE: 0.0093733\n",
      "Iter: 19950, Validation MSE: 0.1222063\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, num_epochs):\n",
    "    # Compute the predicted output for the hidden layer, then adding bias\n",
    "    predicted_h = 1 / (1 + np.exp(np.dot(-weight_hidden, data_in)))\n",
    "    predicted_h = np.concatenate((np.ones([1, data_in.shape[1]]), predicted_h), axis=0)\n",
    "    \n",
    "    predicted_h2 = 1 / (1 + np.exp(np.dot(-weight_hidden2, predicted_h)))\n",
    "    predicted_h2 = np.concatenate((np.ones([1, predicted_h.shape[1]]), predicted_h2), axis=0)\n",
    "    \n",
    "    # Compute the predicted output of hidden layer 1 for the hidden layer 2, then adding bias\n",
    "    predicted_out = 1 / (1 + np.exp(np.dot(-weight_out, predicted_h2)))\n",
    "\n",
    "    \n",
    "    # print(predicted_out)\n",
    "    # Compute the derivatives for the weight updates\n",
    "    deriv_out = (predicted_out * (1 - predicted_out)) * (target - predicted_out)\n",
    "    \n",
    "    deriv_h2 = (predicted_h2 * (1 - predicted_h2)) * (weight_out.T * deriv_out)\n",
    "    \n",
    "   \n",
    "    deriv_h = (predicted_h * (1 - predicted_h)) * np.dot(weight_hidden2.T,deriv_h2[1:])\n",
    "    \n",
    "    \n",
    "        # Compute the update to the input to hidden node weights\n",
    "    deriv_weight_h = learning_rate * np.dot(\n",
    "        data_in, deriv_h[1:].T).T + momemtum * weight_hidden_prev\n",
    "    weight_hidden = weight_hidden + deriv_weight_h\n",
    "    weight_hidden_prev = deriv_weight_h\n",
    "    \n",
    "     # Compute the update to the hidden1 to hidden2 node weights\n",
    "    deriv_weight_h2 = learning_rate * np.dot(predicted_h, deriv_h2.T).T \n",
    "    deriv_weight_h2 = deriv_weight_h2[1:] + momemtum * weight_hidden2_prev #add momentum\n",
    "    weight_hidden2 = weight_hidden2 + deriv_weight_h2\n",
    "    weight_hidden2_prev = deriv_weight_h2\n",
    "    \n",
    "    \n",
    "    # Compute the update to the hidden2 to output node weights\n",
    "    deriv_weight_out = learning_rate * np.dot(predicted_h2, deriv_out.T).T \n",
    "    deriv_weight_out = deriv_weight_out + momemtum * weight_out_prev #add momentum\n",
    "    weight_out = weight_out + deriv_weight_out \n",
    "    weight_out_prev = deriv_weight_out\n",
    "\n",
    "    predicted_h = 1 / (1 + np.exp(np.dot(-weight_hidden, data_in)))\n",
    "    predicted_h = np.concatenate((np.ones([1, data_in.shape[1]]), predicted_h), axis=0)\n",
    "    \n",
    "    # h1 for h2\n",
    "    predicted_h2 = 1 / (1 + np.exp(np.dot(-weight_hidden2, predicted_h)))\n",
    "    predicted_h2 = np.concatenate((np.ones([1, predicted_h.shape[1]]), predicted_h2), axis=0)\n",
    "\n",
    "    # Compute the predicted output of hidden layer 2 for the output, then adding bias\n",
    "    predicted_out_err = 1 / (1 + np.exp(np.dot(-weight_out, predicted_h2)))\n",
    "    \n",
    "    \n",
    "    error_log[i] = 0.5 * ((predicted_out_err - target) ** 2).mean(axis=None)\n",
    "    \n",
    "    \n",
    "    ##Validation remaining\n",
    "    predicted_h = 1 / (1 + np.exp(np.dot(-weight_hidden, X_val)))\n",
    "    predicted_h = np.concatenate((np.ones([1, X_val.shape[1]]), predicted_h), axis=0)\n",
    "    \n",
    "    # h1 for h2\n",
    "    predicted_h2 = 1 / (1 + np.exp(np.dot(-weight_hidden2, predicted_h)))\n",
    "    predicted_h2 = np.concatenate((np.ones([1, predicted_h.shape[1]]), predicted_h2), axis=0)\n",
    "\n",
    "    # Compute the predicted output of hidden layer 2 for the output, then adding bias\n",
    "    predicted_out_err = 1 / (1 + np.exp(np.dot(-weight_out, predicted_h2)))\n",
    "    \n",
    "    \n",
    "    errorv_log[i] = 0.5 * ((predicted_out_err - Y_val) ** 2).mean(axis=None) \n",
    "    if errorv_log[i] < min_val_loss:\n",
    "        min_val_loss = errorv_log[i]\n",
    "        weight_hidden_val = weight_hidden\n",
    "        weight_out_val = weight_out\n",
    "    if (i % 50) == 0:\n",
    "        # print('XOR bias momentum MSE: {0}'.format(error_log[i]))\n",
    "        print(\"Iter: %d, Training MSE: %8.7f\" % (i, error_log[i]))\n",
    "        print(\"Iter: %d, Validation MSE: %8.7f\" % (i, errorv_log[i]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04637459112018852 Iter:94\n"
     ]
    }
   ],
   "source": [
    "minimum = min(errorv_log)\n",
    "i =np.where(errorv_log == minimum)\n",
    "print(minimum,\"Iter:%d\" % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.05587262 -0.58877786  0.27398753 -0.07850269  0.0082167   0.06193959\n",
      "  -0.33069759  0.18360971 -0.18543966 -0.20968077  0.01830144  0.14910212\n",
      "  -0.70708454 -0.09582646 -0.53923021  0.43867634  0.37645344  0.35590553\n",
      "   0.4280251  -0.7239686   0.12538324 -0.38861972  0.06108634 -0.43981712\n",
      "  -0.35594291  0.10603695 -0.39599706 -0.3721225  -0.4649082  -0.47237032\n",
      "  -0.31368628 -0.51994472  0.15422993 -0.21534859 -0.15733668 -0.28796725\n",
      "   0.79898428  0.34778073 -0.6015338  -0.03273052  0.59694123 -0.00612983\n",
      "   0.26689423 -0.18554911 -0.00890278 -0.06793786 -0.06696437  0.20441941\n",
      "  -0.18791699  0.25780846 -0.01521516 -0.70482483  0.03667846  0.39833725\n",
      "  -0.2267043  -0.64233495 -0.226262   -0.45003465  0.65270814  0.95997939\n",
      "   0.76125635  0.02119451  0.17722897 -0.87469961 -0.01474435  0.12827464\n",
      "  -0.02785111 -0.59576562  0.75637721 -0.02038257 -0.13391828 -0.58382352\n",
      "  -0.04736062 -0.10484883  0.2681946   0.31963379 -0.01489081 -1.00029095\n",
      "   0.50998401  0.00482698  0.07299915 -0.91604067  0.31270899 -0.85647099\n",
      "   0.03004364 -0.26333815  0.06864665  0.17334608  0.63565408 -0.30834296\n",
      "  -0.16136634 -0.41256983  0.18245852  0.16113982 -0.34279531 -0.33112925\n",
      "   0.10156159  0.16637732 -0.32099703  1.04044396  0.50866856]]\n",
      "[[-0.14990439 -0.45915754  0.15426136 ... -0.28401414  0.26915414\n",
      "  -0.14400778]\n",
      " [-0.14798522  0.20344717  0.37839261 ...  0.06920986 -0.17949428\n",
      "  -0.20854536]\n",
      " [ 0.27278248  0.63865228  0.27163955 ... -0.39618453  0.10305083\n",
      "   0.17934729]\n",
      " ...\n",
      " [ 0.47267979 -0.59897724 -0.11276928 ...  0.21072283 -0.34824839\n",
      "  -0.38548525]\n",
      " [-0.37083761  0.22599838 -0.08961964 ...  0.24259538  0.01142671\n",
      "  -0.38379194]\n",
      " [-0.43060097  0.30494625 -0.02752906 ... -0.30451048  0.40968282\n",
      "   0.28829023]]\n"
     ]
    }
   ],
   "source": [
    "weight_out = weight_out_val\n",
    "weight_hidden = weight_hidden_val\n",
    "print(weight_out)\n",
    "print(weight_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target outputs: [0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 0\n",
      " 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1\n",
      " 0 1 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1\n",
      " 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0\n",
      " 0 1 1 1 0 0 1 0 0 1 1 1 0]\n",
      "Predicted outputs: [[1.62568546e-04 2.61821418e-02 1.16624211e-05 4.32323207e-11\n",
      "  3.81036506e-04 1.89138412e-09 2.47523499e-11 9.99999973e-01\n",
      "  1.00000000e+00 2.39906328e-13 9.22012318e-10 3.21441066e-05\n",
      "  3.80384543e-06 2.61271434e-02 1.24892727e-09 9.99999774e-01\n",
      "  9.98955432e-01 9.99998563e-01 9.99999944e-01 2.26027207e-06\n",
      "  9.99997692e-01 1.00000000e+00 4.69039341e-07 4.16864385e-10\n",
      "  9.98431710e-01 9.99999991e-01 4.31842766e-11 1.00000000e+00\n",
      "  1.00000000e+00 9.98671199e-13 9.96041299e-01 1.42579845e-12\n",
      "  9.99983685e-01 9.99307646e-01 9.99999910e-01 4.62045548e-05\n",
      "  2.93822224e-04 4.56876168e-07 1.00000000e+00 7.58335102e-12\n",
      "  2.46095016e-10 3.21968046e-09 3.30401976e-06 2.78798384e-02\n",
      "  5.37735241e-07 9.80246914e-01 9.36532719e-01 3.98772321e-12\n",
      "  8.29619591e-03 2.00812224e-10 4.98502131e-04 9.99999858e-01\n",
      "  9.99366387e-01 9.81460396e-01 2.41347203e-02 4.28091617e-02\n",
      "  1.13008651e-06 4.90405717e-05 1.88357460e-11 2.99690915e-02\n",
      "  3.85901914e-12 4.47489986e-07 8.48770159e-14 9.87442471e-01\n",
      "  4.90304381e-07 6.16128533e-09 9.59122111e-01 9.99452862e-01\n",
      "  9.99996891e-01 2.52936697e-11 9.99999786e-01 3.62744581e-04\n",
      "  9.99933449e-01 1.00000000e+00 2.86248827e-13 9.99971109e-01\n",
      "  1.00000000e+00 9.99879334e-01 9.50041034e-11 7.81697959e-11\n",
      "  2.98263555e-10 9.99999991e-01 2.88150347e-06 9.99999230e-01\n",
      "  1.00000000e+00 1.13665213e-10 4.57713226e-09 9.99995673e-01\n",
      "  7.45551188e-09 1.35203677e-03 4.68089783e-11 1.81764878e-03\n",
      "  1.00000000e+00 9.99999976e-01 2.03834371e-02 5.19006883e-08\n",
      "  9.32591141e-07 1.52518366e-07 1.00000000e+00 9.99999479e-01\n",
      "  9.99987416e-01 9.99999972e-01 9.99999996e-01 6.98656724e-12\n",
      "  9.69176153e-07 6.21069388e-10 9.99999941e-01 9.99987529e-01\n",
      "  1.42723095e-11 1.41252054e-12 9.99117149e-01 2.09380614e-10\n",
      "  9.99998928e-01 9.99397003e-01 9.95356162e-01 2.70311130e-02\n",
      "  9.99978438e-01 1.27111990e-11 9.99999986e-01 1.00000000e+00\n",
      "  1.00000000e+00 9.99963742e-01 5.37455187e-09 1.00000000e+00\n",
      "  1.72002782e-06 9.82811041e-01 1.00000000e+00 9.98320293e-01\n",
      "  1.03538786e-10 3.16697090e-02 9.99676458e-01 1.00000000e+00\n",
      "  5.91922501e-05 7.52772267e-11 9.74967422e-01 1.00000000e+00\n",
      "  9.99999997e-01 9.99999999e-01 1.02991291e-11 1.00000000e+00\n",
      "  1.98815161e-04 3.59689800e-02 1.97447847e-10 3.87852559e-06\n",
      "  1.00000000e+00 4.35128678e-12 9.99719955e-01 4.81561030e-07\n",
      "  6.07296185e-04 9.99949008e-01 9.99999973e-01 9.78710298e-01\n",
      "  1.61127058e-04 8.86923386e-06 9.99982113e-01 1.68719708e-10\n",
      "  4.23436395e-14 9.66723859e-01 9.99999745e-01 9.99999945e-01\n",
      "  1.52767743e-04]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAEWCAYAAADl19mgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd5xddZ3/8dd7anpPIJCEhBB6J/QqgiDSdgVpCggrioL6WxusLguIu7i71hVFpBepwppVEFAhSk+oIUBIoWTSeyaTZCYz8/n98f3e5ORy25Q7d+bO5/l43Mc99Xs+59xz7+d+v6fJzHDOOefKSUWpA3DOOec6myc355xzZceTm3POubLjyc0551zZ8eTmnHOu7Hhyc845V3bKNrlJGidpnaTKHNOYpJ26Mq58JD0t6Z+yjMu5TpKulnR3jrLfl3RcZ8VaDPnWocjLvlHSv+YY36bYcu1fks6T9ER74uwqPWF/6e4k7SLpVUn1kr5a6nhykXShpGdKHUdn6ZTkFr8ETZJGpA1/LX7Bx8f+2yVdl6UMk9QQf7wXSPpxrsSUj5l9aGYDzKwllp81aRSilD+6Kenr5DqXmX3JzL4PIOkYSXVFXNY9ZvaJYpXvMitBwv428LSZDTSzn3e0sPg7tCn+Tq6W9JykQzshzqJoa8KUND7mgqqOLrsza27vAeekeiTtBfRtYxn7mNkA4OPAucAXOi8855zrcjsAM9szY44f+Pvj7+QI4CngwXbGVtY6M7ndBZyf6L8AuLM9BZnZO8DfgT3Tx0m6RtL/xO7qWNv7z9jfV9JGSUOT/wAk/QA4EvhF/Mfzi0SRx0maLWmVpBskKcMyTwT+BTgrzv+6pI9JmpGY5s+SXkr0PyPp9Ni9W6w5rpY0U9KpeTbBDpKejU0ZT6RqxOn/aiRNkDQ1TvckYWdPxv05SR9IWiHpu2njKiRdIWluHP+ApGFpy7lA0oeSlqfPn1bW7XHb/THG8qKkiYnxh0maJmlNfD8sMS7fOhwS/52ujtv9mMS4CyXNi/O+J+m8DLH1kbQhsQ2/J6lZ0qDYf52knybW4zpJ/YHHgO3i571O0naxyBpJd8ZlzpQ0Odt2iU6KMS6X9F+SKhKxb/5HK+lnkuZLWivpZUlHJsYdJGl6HLdE0o+zfA5DJf1B0rK4P/9B0pjE+KclfT/TvhXHZ91fMizrdkm/lPRY3D7PStpW0k/jst+RtF9i+qzfgXaUtZ2k38X1fE+J5j6Fms0DmT4jSXcB44D/i8v5tjLU0JWo3cXyHpR0dyxvhqSdJV0paWn8zDLWwCX9FfgYW353dpY0OMa2LG7r76XtE89K+omklcDVuT4DM2sG7gG2lzQysdyTFVrNUjW7vRPjUt/5eklvSfqHXMtIW582fRcl7QbcCBwa1391nPZTCk21a+P2S67n3+L76jjPoXGeiyS9HfeHxyXtkDdgM+vwC3gfOA6YBewGVALzCf9aDBgfp7sduC5LGQbsFLt3BxYDF2eY7lhgRuw+DJgLvJgY93rsHh/LrIr9TwP/lGGZfwCGEHb6ZcCJWeK7Grg70d8H2ED4Ma6K8S4EBhJqrBuA4UA1MIeQHGtijPXALlmW83Rcp51jOU8D12dZp+eBHwO1wFGx3LsT23BdHF4bp2sGjovjvw68AIyJ438N3Ju2nN/EGPYBGoHdssR8O7ASOChui3uA++K4YcAq4HNx3Dmxf3gB67A9sAI4ifBH7PjYPxLoD6xNbUdgNLBHlvj+Bnw6dj8Rt+8nE+P+IX3/BI4B6jLsAxtjPJXAfwAv5PheGOGf9TDC/vUucR8ELgSeSUz7WcL+UgV8g7A/9Ulso8/F7gHAIVmWNxz4NNCPsB8+CPxvgftWzv0ly2e+HDiA8F34K6H15vy4ba4DnorT5vwOtLGsCuBl4KpY1o7APOCEQj4j4m9Voj/T57x5mkR5J8TP5s4Y23fjen0BeC/HPvA0id+dOP/v4+czPu4TFyf2iWbg8risvrl+h+L6Xx+3Xeo3YX9gKXBwXP8L4vrUxvFnAtvF7XgW0ACMzrRPpi23Xd/FTGXGbb5XLGdvYAlweqbfuDjsdML+s1vcLt8Dnsu2zTfPl2+CQl5sSW7fizvTicCTMZC2JLe1hB++uYQduiLDdH3jzjYcuILwhakjfOmvAX6eaSOl72SJZR6R6H8AuCJLfJt3qsSwvwP/CBxC+NF8IK77x4A34jRHEn6oKhLz3QtcnePL8L1E/5eBP6WvE+HHshnon5j2t2zZ8a8iJpjY3x9oYsuX9m3g44nxo4FNsezUcsYkxr8EnJ3jh+7mRP9JwDux+3PAS2nTP0/Y6fOtw3eAu9LmfZzwhe0PrCb8mH/kRyBtnu8DP2fLn5CvEX4UNv9BSd8/yZ7c/pzo3x3YkGO5RuLPUvws/5LvhySOX0VopoeQgK9JxdmG7+W+wKoC962c+0uWz/w3if7LgbcT/XsBqwv5DrSxrIOBD9NiuRK4rZDPiPYltycT404h/AmojP0D4+c8JMf3OfWHppLwJ3H3xPgvEo7JpfaJDzOVk7YPNhH2/RZCgjkmMf5XwPfT5pkFHJ2lvNeA0/Ltk7Tzu5irzMQ0PwV+ErvH89Hk9hiJig4hKa4HdshVbmefLXkX4VjZhbSvSXJ/MxtqZhPN7Htm1po+gZltAKYDRxP+ZU4FngMOj8OmtnGZixPd6wlJslBTCV+OVBxPxxiScWwHzE9blw8I/4Q6EtN2hB+uhrRyk+Pnp3ridCsS43cAHolNDKsJya4F2KaNceSbdru0uFJxbl/AOuwAnJmKMcZ5BOGfZgPhn+eXgEUKTaK7Zokt9TntD8wg/PE6mvCnZI6ZLc+xXvnWs49yH/yen+j+gLDOHyHpG7HZZU1cz8FsaaK9mFDbekehWffkLGX0k/Tr2Ny1lpAUh2jrE7NyfU659pdMliS6N2To36rsPN+BQsvagdBcnNwn/oXc+22+zyif9FiW25aTujbE90J+N0YQalvJfTx9O8wnvwfMbAhhnd8k1HhTdgC+kbZ9xhL3O0nnJ5osVxMO/Ywgv876LiLpYElPxabZNXG+XDHsAPwssdyVgMj9G9q5yc3MPiBU2U8CHu7MstNMJTRt7AdMi/0nEJrF/pZlHuvgMjPNn57cpvLR5LYQGJtqV4/GAQs6GM8iYKjC8aFkucnxY1M9kvoRarsp8wlNc0MSrz5m1tG40i0k7JxJqfXPtw7zCf8WkzH2N7PrAczscTM7nlDrfIfQjJrJc8AuwD8AU83srbicT5H9z1BH95eUsYnucYTtsRWF42vfAT4DDI0/XGsIX2DMbLaZnQOMAn4IPJS2zVK+QVjPg81sEGG/JFVOHvn2l47ozO/AfEIzYHKfGGhmJxU4f/rn2kBoxgUg/hEYSXEsJ7SOJL8P6duh4P0u/in7InC1pNFx8HzgB2nbp5+Z3RuPU/0GuIxwWGAIITkWsn+097uYaX1+C0wBxprZYMJxOeWYfj7wxbRl9zWz53IFXIzr3C4Gjk37N55UqXCQP/WqaccyphLa498ysyZi1Z+w0y/LMs8SQvt8ey0Bxqd9QVM/mgcRmt5mEnbcg9mSZF8kfIG+rXACzDGEpo37OhBL6o/EdOAaSTWSjojlpjwEnCzpiLiNr2Xrz/tG4AepA7OSRko6rSMxZfEosLOkcxVO7jmL0FT0hwLW4W7gFEknSErtN8dIGiNpG0mnxh/5RkJTUcZLJMxsPeE4zVfYksyeI/wwZEtuS4DhkgZ3aO3hWwoneowlNIfen2GagYTm2WVAlaSrgEGpkZI+K2lkrPmsjoMzretAQk1itcLJQf/Whjjz7S8d0ZnfgZeAtZK+o3ACWaWkPSUdWOD86b8D7xJqdp+SVE04tFLbjrjyirW9Bwjfu4Hxu/fPhP28vWW+Q2ge/HYc9BvgS7F2JEn947oNJDQfGmE/Q9LnyXDSXhbt/S4uAcak/c4PBFaa2UZJBxFa+1KWAa1s/RndCFwpaY8Y92BJZ+YLuNOTm5nNNbPpOSa5gvAFTL3+2o7FPEc49pZKIG8RjsNlq7UB/Aw4I55t057rTVKn266Q9Apsbrp5BZgZkyyE40kfmNnSOE0TcCrwScI/t18C58edsqPOJSTSlYQfss1NwTHRfoXwL2kR4RhO8qywnxH+PT0hqZ5wcsnBnRDTVsxsBXAyoVaxgvAlPDnRFJhrHeYDpxGanZYR/sF9i7DfVsQyF8Z5jyYcQ8pmKuEEgJcS/QPJss/Ez+deYF5sDsnYnFiA3xMS62vAH4FbMkzzOOG4wruEZqqNbN08dSIwU9I6wud2tpltzFDOTwnfi+WEz/NPhQZZwP7Sbp35HYgJ4hTC8cT3Ynk3E5pxC/EfwPfiZ/pNM1tD2G9uJtSgGuik9c7i8riMecAzhO19awfL/C/gEkmj4m/vF4BfED7DOYTDRMQWix8RfqOWEI5lPlvIAjrwXfwr4VKIxZJS3/kvA9fG352rCAk/tZz1wA+AZ+NndIiZPUJosbgvNre/SdiXclI8QOecc86VjbK9/ZZzzrney5Obc865suPJzTnnXNnx5Oacc67sdPjOyz3BiBEjbPz48aUOwznnepSXX355uZkV67q/ouoVyW38+PFMn57r6gTnnHPpJKXfXajH8GZJ55xzZceTm3POubLjyc0551zZ8eTmnHOu7Hhyc845V3Y8uTnnnCs7ntycc86VHU9uOTzyah13v9BjL/Nwzrley5NbDlNeW8gD0wt56rtzzrnuxJObc865suPJLQ9/lqtzzvU8ntxykFTqEJxzzrWDJzfnnHNlx5NbHoa3SzrnXE/jyS0Hb5R0zrmeyZObc865suPJLQ8/W9I553oeT245+MmSzjnXMxU1uUk6UdIsSXMkXZFh/FGSXpHULOmMxPCPSXot8doo6fQ47nZJ7yXG7VvMdXDOOdfzVBWrYEmVwA3A8UAdME3SFDN7KzHZh8CFwDeT85rZU8C+sZxhwBzgicQk3zKzh4oV+9axdMVSnHPOdaaiJTfgIGCOmc0DkHQfcBqwObmZ2ftxXGuOcs4AHjOz9cULNRtvl3TOuZ6omM2S2wPJuw7XxWFtdTZwb9qwH0h6Q9JPJNVmmknSJZKmS5q+bNmydiw28Iqbc871PMVMbpmqPW3KFZJGA3sBjycGXwnsChwIDAO+k2leM7vJzCab2eSRI0e2ZbGbfWHZv3Nd/b+2a17nnHOlU8zkVgeMTfSPARa2sYzPAI+Y2abUADNbZEEjcBuh+bMo+rasY6CtK1bxzjnniqSYyW0aMEnSBEk1hObFKW0s4xzSmiRjbQ6FuxqfDrzZCbHm4A2TzjnX0xQtuZlZM3AZoUnxbeABM5sp6VpJpwJIOlBSHXAm8GtJM1PzSxpPqPlNTSv6HkkzgBnACOC6Yq2Dn0/inHM9UzHPlsTMHgUeTRt2VaJ7GqG5MtO875PhBBQzO7Zzo3TOOVdu/A4lecibJZ1zrsfx5JaTt0s651xP5MktD6+5Oedcz+PJLQdDfvst55zrgTy55eENk8451/N4csvJU5tzzvVEntzy8nZJ55zraTy55SKvuznnXE/kyS0H89TmnHM9kie3vLxZ0jnnehpPbs4558qOJzfnnHNlx5NbHn6HEuec63k8ueXkm8c553oi//XOw8+XdM65nseTWw6GnyvpnHM9kSe3XLza5pxzPZIntzz8hBLnnOt5PLnl5FU355zriYqa3CSdKGmWpDmSrsgw/ihJr0hqlnRG2rgWSa/F15TE8AmSXpQ0W9L9kmqKug5ec3POuR6naMlNUiVwA/BJYHfgHEm7p032IXAh8NsMRWwws33j69TE8B8CPzGzScAq4OJODz7ye0s651zPVMya20HAHDObZ2ZNwH3AackJzOx9M3sDaC2kQEkCjgUeioPuAE7vvJDTllesgp1zzhVVMZPb9sD8RH9dHFaoPpKmS3pBUiqBDQdWm1lzvjIlXRLnn75s2bK2xr6FebOkc871NFVFLDtTxactmWKcmS2UtCPwV0kzgLWFlmlmNwE3AUyePLldGcrkdTfnnOuJillzqwPGJvrHAAsLndnMFsb3ecDTwH7AcmCIpFRSblOZbSW8adI553qiYia3acCkeHZjDXA2MCXPPABIGiqpNnaPAA4H3jIzA54CUmdWXgD8vtMjj/yEEuec65mKltzicbHLgMeBt4EHzGympGslnQog6UBJdcCZwK8lzYyz7wZMl/Q6IZldb2ZvxXHfAf5Z0hzCMbhbirUOzjnneqZiHnPDzB4FHk0bdlWiexqhaTF9vueAvbKUOY9wJmbRDWlexg5W1xWLcs4514n8DiU57LjhzVKH4Jxzrh08uTnnnCs7ntycc86VHU9uzjnnyo4nN+ecc2XHk5tzzrmy48nNOedc2cmZ3CRVSvp/XRWMc8451xlyJjczayHtMTXOOedcd1fIHUqelfQL4H6gITXQzF4pWlTOOedcBxSS3A6L79cmhhnhoaHOOedct5M3uZnZx7oiEOecc66z5D1bUtJgST9OPdVa0o8kDe6K4Jxzzrn2KORSgFuBeuAz8bUWuK2YQTnnnHMdUcgxt4lm9ulE/zWSXitWQM4551xHFVJz2yDpiFSPpMOBDcULyTnnnOuYQmpuXwLuTBxnWwVcULyQuqG1C2HQdqWOwjnnXIFyJjdJFcAuZraPpEEAZra2SyLrRmzdUuTJzTnneox8dyhpBS6L3Wt7Y2ILrNQBOOeca4NCmiWflPRNPnqHkpX5ZpR0IvAzoBK42cyuTxt/FPBTYG/gbDN7KA7fF/gVMAhoAX5gZvfHcbcDRwNrYjEXmllRT3AxAxVzAa5LmRlNLa1saGphfVML65ua43sLTc2ttJhhZrS0QqsZra1GixkVEhUSVRWisjK+V4iqior4LqoqRZ/qSmqrKqitqqS2uoI+VZVUVwrJ9yLnukohye2i+P6VxDADdsw1k6RK4AbgeKAOmCZpipm9lZjsQ+BC4Jtps68Hzjez2ZK2A16W9LiZrY7jv5VKhM61thr1G5tZs2ETazduYtX6Jlasa2L5ukZWNDSxYl3j5v7l65pY0dDIxk2tXRqjBLVVFVsnvmR/dRjWJybD2uot4/tUb5luc38icdbGYZvnr67cPH11pT/4w/VOhRxz+6yZPduOsg8C5pjZvFjWfYSbMG9Obmb2fhy31S+Nmb2b6F4oaSkwElhNCXijZGk1t7Ty7NwVvDhvBXWrNrB8XSNrNmwKyWzDJuobm7EsH1J1pRjev5bhA2oYPqCWiSMHMHxADUP61dC3upJ+NZX0q62iX+yuqaqgokJUxlpaRQVUKNTQzKC5tZXW1vDe0mo0t1rivZWm5lYaU69NLZu7N6a64/vGtHEbN7Wyev2mrfpT0za1tD8RV1aIPlVbJ8ctyTAmwqqPJsePJNdEMu2TIZmGBB3eayrDNnSulHImNzNrlfTfwKHtKHt7YH6ivw44uK2FSDoIqAHmJgb/QNJVwF+AK8ysMcN8lwCXAIwbN66ti3XdQEurMeX1Bfz8L3N4b3kD1ZViuyF9GTGglm0G9WHnbQYyuG81g/pUMahvNYPja0i/GoYPqGFE/1oG9a3q8c2BLa1GY3NMePE9JMCWrZNhcwuNm1rZ2BzHbe5uzTrtyoamrYYlp+2IqgpRU1VBTaw91lSG7tR7daU2j6tNm646TpeqedYk3kMZsezKys3l1CTGV1eG8qsqKqiqFNWVFbHJeMtwbyYuf4U0Sz4h6dPAw2bZ/h9nlGnPaVMlSNJo4C7ggnhyC8CVwGJCwrsJ+A5b39Q5LMjspjieyZMnd6jy1aa1dh3W0mr84Y2F/Owvs5m3rIFdtx3IL8/bn4/tMoq+NZWlDq/LVVaIfjVV9KvpumWaWaxpJpJlIjlu3d/CxkSttKm5lU0tifeWVpqaLb63sKnFaIo10vqNzayM0za1tLKpOTX9lvfWIn3/UsdIq2MSrKqsoDomweTwbMmyOk6XSpap/q2S6VZJdUt3ZUXqPXXstiJxDDe+VyaGVWaYrnLr473Jd0/chSW3fwb6A82SNhKSlpnZoDzz1QFjE/1jgIWFBhYvPfgj8D0zeyE13MwWxc5GSbfx0eN1rodqbmnl968t5Ian5zBvWQO7bDOQX523Pyfssa03c3UxSZuP3Q2muqSxtLTaVsluU1rySybFLUnVaG6N74nuTS2tNLdsGd/cYltNu3l8q9HcEscnujc2t8Rp4rSttlX3phhDWGbp/hVvPsGpQky5/AgmjhxQslhKpZCnAgxsZ9nTgEmSJgALgLOBcwuZUVIN8Ahwp5k9mDZutJktUvhrcjrwZjvjK5hX3IqrqbmVh1+p45dPz+XDlevZdduB3HDu/nxyT09qLvxQ962ppC89q9ZuZluSX+vWCbElJtRMx203tWzd37xVfygj03ypcjel9Q/uW9o/J6WSNblJ+qyZ3R27D0+eVCLpMjP7Ra6CzaxZ0mXA44RLAW41s5mSrgWmm9kUSQcSkthQ4BRJ15jZHoQbNB8FDJd0YSwydcr/PZJGEmqQrxHuoFJknt6KYeOmFh6cPp8bp85jweoN7D1mMP968mQ+vusoT2qux5MUmyvpcYm5HCjbYTRJr5jZ/undmfq7u8mTJ9v06dPbPuPV4Y5jmy5+iuqxPWZ1u70NTS389qUP+fXUuSytb+SAHYZy+bE7cfTOI/1YgXPdiKSXzWxyqeNoj1zNksrSnanfubzWNTZz1/MfcPPf57GioYlDdhzGT8/al0MnDvek5pzrVLmSm2XpztRf1nrVyhbBusZmbn3mPW555j3WbNjEUTuP5PJjd+LA8cNKHZpzrkzlSm67SnqDUEubGLuJ/TnvTlJ+PL21x6aWVu576UN+9pfZLF/XxHG7jeKyYyex79ghpQ7NOVfmciW33bosim7OSnhKb0/11DtL+f4f3mLe8gYOmjCMmy/YzZOac67LZE1uZvZBVwbiysPS+o1c839v8cc3FjFxZH9uuWAyx+46yo+pOee6VCEXcTv/YS7IlNcX8t1HZtDY3Mo3jt+ZLx49kZoqv3Gvc67reXJzHba+qZmrp8zkgel17D9uCP995j7s2AvviOCc6z4KSm6S+gLjzGxWkePplvzektktWrOBi26fzjuL13LZx3bi68dNosofs+KcK7G8v0KSTiHcCeRPsX9fSVOKHZjr/t5csIbTb3iW+SvXc9uFB/LNE3bxxOac6xYK+SW6mvBsttUA8RZY44sXUnfUtQ+27AneqFvNOb95gUqJhy49lGN2GVXqkJxzbrNCmiWbzWyNn+3mUmbUreGzN7/IkH7V3HfJoWw/pG+pQ3LOua0UktzelHQuUClpEvBV4LnihtW9+DG3LeavXM/nb3+JgX2qufcLh3hic851S4U0S14O7AE0Ar8F1gBfL2ZQrnuq37iJf7pjOo3Nrdxx0UGMGdqv1CE551xGOWtukiqBa8zsW8B3uyak7sf8PtGYGd968A3mLFvHHZ8/iJ1G+an+zrnuK2fNzcxagAO6KJbuq6Wp1BGU3G9f+pA/zVzMt0/YhSMmjSh1OM45l1Mhx9xejaf+Pwg0pAaa2cNFi6qbqZr5O5h4WKnDKJnZS+r5/h/e4shJI/jCkb3sntnOuR6pkOQ2DFgBHJsYZkCvSW5rNzbRW+sqra3Gt3/3Bn2rK/nRmfv4E7Kdcz1C3uRmZp/vikC6s2X1jb02ud077UNe/XA1PzpzH0YN6lPqcJxzriCF3KGkj6SvSPqlpFtTr0IKl3SipFmS5ki6IsP4oyS9IqlZ0hlp4y6QNDu+LkgMP0DSjFjmz9UFF+D11ksBltZv5PrH3uGwicP5x/23L3U4zjlXsEIuBbgL2BY4AZgKjAHq880Uz7S8AfgksDtwjqTd0yb7ELiQcIlBct5hwL8BBxPujvJvkobG0b8CLgEmxdeJBaxDB/XO7PYfj75DY3Mr152+pz+yxjnXoxSS3HYys38FGszsDuBTwF4FzHcQMMfM5plZE3AfcFpyAjN738ze4KP3tzoBeNLMVprZKuBJ4ERJo4FBZva8mRlwJ3B6AbF0TC+sus2oW8Mjry7g4iMm+B3+nXM9TiHJbVN8Xy1pT2Awhd1bcntgfqK/Lg4rRLZ5t4/d7SnTFcjM+PdH32ZY/xouPWZiqcNxzrk2KyS53RSbBP8VmAK8BfxnAfNlascqtAqUbd6Cy5R0iaTpkqYvW7aswMU6gKdmLeX5eSv42scnMahPdanDcc65Nsub3MzsZjNbZWZTzWxHMxtlZjcWUHYdMDbRPwZYWGBc2eati915yzSzm8xssplNHjlyZIGLdc0trfzHo+8wYUR/zj14XKnDcc65dsl7KYCkqzINN7Nr88w6DZgkaQKwADgbOLfAuB4H/j1xEskngCvNbKWkekmHAC8C5wP/U2CZ7dabjrg9ML2O2UvXceNnD6Dan83mnOuhCvn1aki8WghnP47PN5OZNQOXERLV28ADZjZT0rWSTgWQdKCkOuBM4NeSZsZ5VwLfJyTIacC1cRjApcDNwBxgLvBYYavaAb0ku61vauYnf36XyTsM5YQ9til1OM45126FXMT9o2S/pP8mHHvLy8weBR5NG3ZVonsaWzczJqe7FfjI9XRmNh3Ys5Dld57ekd1ufeY9ltU3cuNnD/BT/51zPVp72p36Ab3rBoO94FKAVQ1N/HrqPI7ffRsO2GFo/hmcc64bK+SY2wy2VF0qgZFAvuNt5aUX1GJueGoODU3NfPuEXUodinPOdVghN04+OdHdDCyJx9N6jzKvuS1YvYE7X/iAT+8/hknbDCx1OM4512GFJLf0W20NSh6PSZzoUcbKO7n99Ml3Afh/x+9c4kicc65zFJLcXiFcc7aKcBH1EMI9ISH86veu429lZvaSen73Sh0XHzGB7Yb0LXU4zjnXKQo5oeRPwClmNsLMhhOaKR82swlm1isSm5Vxs+QP/zSL/jVVfPmYnUodinPOdZpCktuB8ZR+AMzsMeDo4oXkusqzc5bz57eXcOnHJjK0f02pw3HOuU5TSLPkcknfA+4mNEN+lvBkbteDNbe0cu3/vcXYYX256PAJpQ7HOec6VSE1t3MIp/8/AvwvMCoO60XKr1ny3mnzmd/ISnYAABv6SURBVLWknu+etBt9qitLHY5zznWqQu5QshL4GkC81+NqK+eDUJmU2dquWb+JHz8xi0N2HMYJe2xb6nCcc67TZa25SbpK0q6xu1bSXwn3c1wi6biuCrA7sDK7iPv6P73Nmg2buOrkPfw2W865spSrWfIsYFbsviBOO4pwMsm/FzmubqVZ5XOyxQvzVnDvS/P5pyN3ZPftBpU6HOecK4pcya0p0fx4AnCvmbWY2dsUdiJK2dhU0afUIXSKjZtauPLhGYwb1o//d5xfsO2cK1+5klujpD0ljQQ+BjyRGNevuGF1D41WXjn8vx6fxXvLG/j3f9iLvjV+Eolzrnzl+vX+GvAQ4UzJn5jZewCSTgJe7YLYupGef0bJU+8s5ZZn3uOCQ3fgiEkjSh2Oc84VVdbkZmYvArtmGP6RZ7SVr/I42WLRmg1888HX2XXbgVx50m6lDsc554quPc9z63XUg698aGhs5qLbp9PY3Movzt3Pr2lzzvUK5XVQqUh6amrbuKmFL9/zCu8uqefWCw9kp1H+OBvnXO/gyS0Hy9DVU6xvauaLd73M32cv5/p/3Iujdx5Z6pCcc67LFJTcJB0GjE9Ob2Z3FjDficDPCE/wvtnMrk8bXwvcCRxAuF/lWWb2vqTzgG8lJt0b2N/MXpP0NDAa2BDHfcLMlhayHm3VGlttZa3FKL5o5i1bx6V3v8LspfX85xl785nJY0sdknPOdam8yU3SXcBE4DWgJQ42QlLKNV8lcANwPFAHTJM0xczeSkx2MbDKzHaSdDbwQ0KCuwe4J5azF/B7M3stMd95Zja9kBXsiJ5WX6vfuIlbnnmPXz09l341ldxx0UEcOclrbM653qeQmttkYPd23E/yIGCOmc0DkHQfcBqQTG6nAVfH7oeAX0hS2rLOAe5t47I7haXOt+nGJ5Ssb2rmxfdW8ue3lvD71xayrrGZk/cezfc+tTvbDi6Pi8+dc66tCklubwLbAovaWPb2wPxEfx1wcLZpzKxZ0hpgOLA8Mc1ZhCSYdJukFuB3wHXFupHzlkJL3yzZ3NLKe8sbeHtxPbMWr2XW4npmLaln/srQOtu3upIT9tiGi46YwN5jhpQ4WuecK61CktsI4C1JLwGNqYFmdmqe+TJdJJaehHJOI+lgYL2ZvZkYf56ZLZA0kJDcPkeGJlJJlwCXAIwbNy5PqJk9Z3txgl5iTU3X3Tm/tdVYsHoDc5etCwlscT1vL65n7tJ1NLWEJFtZIXYc0Z+9xwzhzAPGss/YIRw8YZif5u+cc1Ehye3qdpZdByTPZBgDLMwyTZ2kKmAwsDIx/mzSmiTNbEF8r5f0W0Lz50eSm5ndBNwEMHny5HbV7G7c9ClOqH2J2S2j6ezHIKxvambesgbmLlvH3NT70nW8t7yBxuYtNcVtBtWy67aDOGrSCHbZdiC7bjuIiaP6U1vlicw557Ip5HluU9tZ9jRgkqQJwAJCojo3bZophCcOPA+cAfw11cQoqQI4EzgqNXFMgEPMbLmkauBk4M/tjC+vlnjM7aX3VnBpO+Y3M5bWNzJ36bqPJLGFazZunq5CMHZYP3Yc0Z8jdhrBxFED2HFEf3beZiBD+5fPEwmcc66rFHK25CHA/wC7ATWE0/obzCzn81LiMbTLgMfjPLea2UxJ1wLTzWwKcAtwl6Q5hBrb2YkijgLqUiekRLXA4zGxVRIS228KW9W222/cUFgC44bmPzHDzJi9dB0vf7CK1z5czbtL65mzdB31G5s3T9O/ppKJowZw0IRhTBw5gImjBjBx5AB2GN7PmxSdc64TFdIs+QtC0nmQcObk+cCkQgrPdB9KM7sq0b2RUDvLNO/TwCFpwxoI18R1ierY9KccFwVsaGrhtufe4/5p8/lgxXoAhvarZpdtB3Lavtux08gB7DRqIDuNGsA2g2r94aDOOdcFCrqI28zmSKo0sxbCmYrPFTmubsHi+S7ZLuL+YEUDn79tGvOWN3DYxOFcevREDt5xOOOH9/Mk5pxzJVRIclsvqQZ4TdJ/Ei4J6F/csLqHIf1rw3vfj26mdY3NfP62aaxa38Q9/3Qwh+/kj5FxzrnuopCnAnwuTncZ0EA4u/HTxQyquzhu99EAHLPLR+/ycdPUucxb3sCvPnuAJzbnnOtmCjlb8gNJfYHRZnZNF8TUbVQo5P6KtIu4GxqbufXZ9/nUXqM5ZMfhpQjNOedcDnlrbpJOIdxX8k+xf19JU4odWLdQEY+bpd0A5c9vL2FdYzMXHDa+62NyzjmXVyHNklcTLpReDRBvYDy+eCF1J6nktnXN7Y9vLGL04D5M3mFoCWJyzjmXTyHJrdnM1hQ9ku5IH732rKXVeGHeCo7eeSQVFX5GpHPOdUcF3ThZ0rlApaRJwFeBXnEpwObT+RM1t3cWr2XtxmYO3nFYiaJyzjmXTyE1t8uBPQg3Tb4XWAt8vZhBdRtKPfJmS3J7cV649eXBE/xEEuec664KOVtyPfDd+OpVTB89oeTNBWsYNbCW7Yb0LVFUzjnn8sma3PKdEVnAI296vC13GUk2S9az6+ict9V0zjlXYrlqbocSHiR6L/AimZ+9Vt609ZO4m1tambNsHUdM8ou2nXOuO8t1zG1b4F+APYGfAccDy81sagceg9OjNDS2ALBkdbgh8vsrGmhqbmWXbQaWMiznnHN5ZE1uZtZiZn8yswsId+efAzwt6fIui67Epn2wGoAn314ChCZJgF229eTmnHPdWc4TSiTVAp8CziFcuP1z4OHih9U99OtTDcDAmvAfYNbieiorxE6jBpQyLOecc3nkOqHkDkKT5GPANWb2ZpdF1U0cOWkUPAuf2GMUEGpuE0b09weLOudcN5er5vY5wlMAdga+mng+mQDL9yTuclBRGZJYdVz1dxavZe8xQ0oYkXPOuUJkTW5mVsgF3mWtYvPZki2sa2xm/soNfOaAsaUNyjnnXF69PoHlVNMPgIqWjby7xE8mcc65nqKoyU3SiZJmSZoj6YoM42sl3R/HvyhpfBw+XtIGSa/F142JeQ6QNCPO83Ml2ks7W3VtP1pN0NTArHim5K7bln1rrHPO9XhFS26SKoEbgE8CuwPnSNo9bbKLgVVmthPwE+CHiXFzzWzf+PpSYvivgEuASfF1YrHWoX+fGtZTC00NvLNoLQNqqxgz1G+75Zxz3V0xa24HAXPMbJ6ZNQH3AaelTXMacEfsfgj4eK6amKTRwCAze97MDLgTOL3zQw/61VSynj7Q1MDbi+vZZduB/pgb55zrAYqZ3LYn3L4rpS4OyziNmTUDa4DU7fYnSHpV0lRJRyamr8tTZqeRxAb6oE2h5rarH29zzrkeoZDnubVXpiqOFTjNImCcma2QdADwv5L2KLDMULB0CaH5knHjxhUcdLrGij40rQ/PcNvNb5jsnHM9QjFrbnVA8rz5McDCbNNIqgIGAyvNrNHMVgCY2cvAXML1dnWxnFxlEue7ycwmm9nkkSNHtnslNlb0o3l9eBD5QRP8AaXOOdcTFDO5TQMmSZogqQY4G0h/jM4U4ILYfQbwVzMzSSPjCSlI2pFw4sg8M1sE1Es6JB6bOx/4fRHXgaY+IxjBGkYOrGWS33bLOed6hKI1S5pZs6TLgMeBSuBWM5sp6VpguplNAW4B7pI0B1hJSIAARwHXSmoGWoAvmdnKOO5S4HagL+HWYI8Vax0A+g7djlHrpnPeweMo4lUHzjnnOpHMMh6yKiuTJ0+26dOnt2ve1qn/TcVT36f1ykVU1Pbr5Micc677kvSymU0udRzt4XcoyaNi0LbhvWFxiSNxzjlXKE9u+QwdH95XzitpGM455wrnyS2fEbuE92XvljYO55xzBfPklk//EdB3KCyfVepInHPOFciTWz4SjNodFve6Z7U651yP5cmtEGMOhEWvQ9P6UkfinHOuAJ7cCjHuUGjdBAtfKXUkzjnnCuDJrRDjDgZVwLynSx2Jc865AnhyK0TfobDD4fBW+t3DnHPOdUee3Aq126nhjMklM0sdiXPOuTw8uRVqrzOgqg+8+OtSR+Kccy4PT26F6jcM9jkb3rgf1i4qdTTOOedy8OTWFod/HawV/nJtqSNxzjmXgye3thg2AQ75Mrz+W3j/mVJH45xzLgtPbm111Ldg2ER4+BJYvzL/9M4557qcJ7e2qh0AZ9wCDcvgnjOhcV2pI3LOOZfGk1t7bLcfnHErLHwVbjsRVr5X6oicc84leHJrr91OgXPug1Ufwi8PgSf/DVZ/WOqonHPOATKzUsdQdJMnT7bp06cXp/A1dfDnq2HGQ6F/u31hwtHhfdTu4fhcZVVxlu2cc0Uk6WUzm1zqONqjqMlN0onAz4BK4GYzuz5tfC1wJ3AAsAI4y8zel3Q8cD1QAzQB3zKzv8Z5ngZGAxtiMZ8ws6W54ihqcktZ/SG8fh/M+QssmA6tzWG4KmHgaBi8PQzaDgZtH/oHbhv6B24LA7aFmn7Fjc8559qoJye3olUpJFUCNwDHA3XANElTzOytxGQXA6vMbCdJZwM/BM4ClgOnmNlCSXsCjwPbJ+Y7z8yKnK3aaMg4OPrb4bVpAyx/F5a+Dctnw9oF4bXoDZj1GDRv/Oj8fQbHpDd6S/JLvQ/fCUbuAhWVXb9ezjnXAxWzvewgYI6ZzQOQdB9wGpBMbqcBV8fuh4BfSJKZvZqYZibQR1KtmTUWMd7OU90XRu8TXunMYONqqF8M9Yu2fl+7MLwvnw3rFm+p/QFU9w9NneMOgYnHwpiDoKqm69bJOed6kGImt+2B+Yn+OuDgbNOYWbOkNcBwQs0t5dPAq2mJ7TZJLcDvgOssQ9uqpEuASwDGjRvXwVXpRFJ4ykDfoTBqt+zTtbbC+hVQvzDUABe8Agtehmd+Cn//UUh244+AnU+AXU+Ggdt03To451w3V8zkpgzD0pNQzmkk7UFoqvxEYvx5ZrZA0kBCcvsc4bjd1oWY3QTcBOGYW9tC7wYqKmDAyPAavU+4ryXAxrXw/t9h7lMw588w+3H44zdg7MGw+6kh0Q3dobSxO+dciRUzudUBYxP9Y4CFWaapk1QFDAZWAkgaAzwCnG9mc1MzmNmC+F4v6beE5s+PJLey1WcQ7Pqp8DKDZe+E58y9/X/w+L+E1/YHwN5nwR7/GJKjc871MsW8zm0aMEnSBEk1wNlA+tM+pwAXxO4zgL+amUkaAvwRuNLMnk1NLKlK0ojYXQ2cDLxZxHXo3qTQtHnMd+DSZ+Crr8Jx10BzEzz2bfjRLuEuKjMegqb1pY7WOee6TLEvBTgJ+CnhUoBbzewHkq4FppvZFEl9gLuA/Qg1trPNbJ6k7wFXArMTxX0CaAD+BlTHMv8M/LOZteSKo0suBehulrwFMx6ANx6EtXXhGN0ep8Pki0LNTplahJ1zbouefCmAX8Rd7lpb4cPnwjV4bz4Mmxpgm71g8udh789A7cBSR+ic66Y8uXVzvTq5JW1cCzMehJdvg8UzQm1urzNCottuv1JH55zrZjy5dXOe3NKYhUsLpt8Kb/4OmjeE5HbA52HPT4cnHzjnej1Pbt2cJ7ccNqyGNx4Itbmlb0HNwNBcOfnzsO1epY7OOVdCnty6OU9uBTCD+S+F2tzMR6ClEbbZMzz9YLdTwk2g/SQU53oVT27dnCe3Nlq/MtTm3vo9fPg8YDB4LOxwOIw/HMYdBsN2DBeaO+fKlie3bs6TWwesWwqzHg13RHn/GVgf74xWMwC22SPU7kbuCkPHhzujDBkX7q3pnOvxenJy8weNudwGjIIDLgwvM1g2C+a/CEvehMVvhrMvG9duPU//UWG+/iPDa8Ao6D8C+gwJd1ipHRQuQUi99xkUkqU/9cA510k8ubnCSTBq1/BKMQu1u9UfwKr3YdUHsOZDWLcMGpbBynnhfVMBd0ipHQR9h4Qk2HdIvMH0MBg2AUbsHF5DdvCHvzrn8vJfCdcxUngiwcBtYOxB2adraoCNa6CxPlxv15h61W8ZtnF1OHtz42rYsAqWvhOaQdev2FJORVVoAh2+U3gN2zEkv0HxYbB+UbpzDk9urqvU9A+v9tiwCpbPgeWzYMVcWDEnvM+bGq7R22o5A+MTz+NTzvsOg36xBthvOPQbFrr7DI4xDfDn4jlXhjy5ue6v71AYe2B4JbW2hiecr5kfHvS6dsGW9zULQhJcvzLcciyXypotia5mQOzuB5W1UFUbxn/kvU/asGpQZahZViTe2zpMFfGl8A5p/crQn96da9pM8/olHq78eHJzPVdFBQwZG165bNoIG1aGRLdhZWjmbKwPTaVN6+J7AzSuS+uvD09YaGlMvDdCS1N4z32/7p4llewgLdmlD8vSX8g0GecpdDltKLfd8X+kY2sZ/wS0ZdoSTX/u/aHpvpfx5ObKX3UfqI5NlZ2ppTmR8DaFZNfaAq3NYK3hvbW5DcNiv1l4YYn+1jz9qenbMm1ivLXGlUpcGrT5MiHL059tWFvnydLfplhyzZMr1izxp4/falAbpi3l9FW1Wcopb57cnGuvyqrwau+xROdc0fgtJpxzzpUdT27OOefKjic355xzZceTm3POubLjyc0551zZ8eTmnHOu7Hhyc845V3Y8uTnnnCs7veJhpZKWAR+0c/YRwPJODKezeFxt43G1jcfVNuUa1w5mNrKzgulKvSK5dYSk6d3xSbQeV9t4XG3jcbWNx9X9eLOkc865suPJzTnnXNnx5JbfTaUOIAuPq208rrbxuNrG4+pm/Jibc865suM1N+ecc2XHk5tzzrmy48ktB0knSpolaY6kK4q8rLGSnpL0tqSZkr4Wh18taYGk1+LrpMQ8V8bYZkk6oVhxS3pf0oy4/Olx2DBJT0qaHd+HxuGS9PO47Dck7Z8o54I4/WxJF3Qwpl0S2+Q1SWslfb1U20vSrZKWSnozMazTtpGkA+JnMCfOqw7E9V+S3onLfkTSkDh8vKQNiW13Y77lZ1vHdsbVaZ+dpAmSXoxx3S+ppgNx3Z+I6X1Jr3Xl9lL234aS71/dmpn5K8MLqATmAjsCNcDrwO5FXN5oYP/YPRB4F9gduBr4Zobpd48x1QITYqyVxYgbeB8YkTbsP4ErYvcVwA9j90nAY4CAQ4AX4/BhwLz4PjR2D+3Ez2oxsEOpthdwFLA/8GYxthHwEnBonOcx4JMdiOsTQFXs/mEirvHJ6dLKybj8bOvYzrg67bMDHgDOjt03Ape2N6608T8CrurK7UX234aS71/d+eU1t+wOAuaY2TwzawLuA04r1sLMbJGZvRK764G3ge1zzHIacJ+ZNZrZe8CcGHNXxX0acEfsvgM4PTH8TgteAIZIGg2cADxpZivNbBXwJHBiJ8XycWCumeW6C01Rt5eZ/Q1YmWGZHd5GcdwgM3vewi/RnYmy2hyXmT1hZs2x9wVgTK4y8iw/2zq2Oa4c2vTZxVrHscBDnRlXLPczwL25yujs7ZXjt6Hk+1d35sktu+2B+Yn+OnInm04jaTywH/BiHHRZbF64NdGMkS2+YsRtwBOSXpZ0SRy2jZktgvDlA0aVIK6Us9n6B6fU2yuls7bR9rG7GDFeRPinnjJB0quSpko6MhFvtuVnW8f26ozPbjiwOpHAO2t7HQksMbPZiWFdur3Sfht6wv5VMp7cssvU5lz06yYkDQB+B3zdzNYCvwImAvsCiwjNIrniK0bch5vZ/sAnga9IOirHtF0ZF/FYyqnAg3FQd9he+bQ1lmJtu+8CzcA9cdAiYJyZ7Qf8M/BbSYOKtfwMOuuzK1a857D1n6gu3V4ZfhuyTppl+d3pO1B0ntyyqwPGJvrHAAuLuUBJ1YSd9x4zexjAzJaYWYuZtQK/ITTF5Iqv0+M2s4XxfSnwSIxhSWzOSDXDLO3quKJPAq+Y2ZIYY8m3V0JnbaM6tm467HCM8WSCk4HzYlMUsdlvRex+mXA8a+c8y8+2jm3WiZ/dckJTXFWGeNsllvWPwP2JeLtse2X6bchRVsn3r+7Ak1t204BJ8ayrGkLT15RiLSy2598CvG1mP04MH52Y7B+A1FlcU4CzJdVKmgBMIhwU7tS4JfWXNDDVTTgZ4c1YZupsqwuA3yfiOj+esXUIsCY2mTwOfELS0Njc9Ik4rKO2+jdd6u2VplO2URxXL+mQuJ+cnyirzSSdCHwHONXM1ieGj5RUGbt3JGyjeXmWn20d2xNXp3x2MVk/BZzRGXFFxwHvmNnm5ruu2l7ZfhtylFXS/avb6OgZKeX8Ipx19C7hH9l3i7ysIwhNAW8Ar8XXScBdwIw4fAowOjHPd2Nss0ic3dSZcRPORHs9vmamyiMc1/gLMDu+D4vDBdwQlz0DmJwo6yLCyQBzgM93wjbrB6wABieGlWR7ERLsImAT4Z/wxZ25jYDJhB/7ucAviHcXamdccwjHXlL72Y1x2k/Hz/h14BXglHzLz7aO7Yyr0z67uN++FNf1QaC2vXHF4bcDX0qbtku2F9l/G0q+f3Xnl99+yznnXNnxZknnnHNlx5Obc865suPJzTnnXNnx5Oacc67seHJzzjlXdjy5ubImabi23LV9sba+63yhd4q/TdIueab5iqTzOinmZxTudJ+K8/78c7Wp/DrFJwE4V678UgDXa0i6GlhnZv+dNlyE70JrSQJLI+kZ4DIze61I5dcBe5rZ6mKU71x34DU31ytJ2knSmwrP4HoFGC3pJknTFZ6ZdVVi2mck7SupStJqSddLel3S85JGxWmuk/T1xPTXS3op1sAOi8P7S/pdnPfeuKx92xDz3ZJ+Jenvkt6V9Mk4vK+kOxSex/WK4r0/Y7w/iev5hqQvJ4r7usINf9+QtHOc/tgY22uxnP4d3MzOlYwnN9eb7Q7cYmb7mdkCwrOxJgP7AMdL2j3DPIOBqWa2D/A84Y4PmcjMDgK+BaQS5eXA4jjv9YS7u2eTfEDm9YnhY4GjgVOAmyTVAl8FmsxsL+BzwF2xyfVSYDtgHzPbm/BImJQlFm74ezPhpr/EWC8xs30JzzXbmCM+57o1T26uN5trZtMS/edIeoVQk9uNkPzSbTCz1CNiXiY8sDKThzNMcwQxwZhZ6nZm2ZxlZvvGV/Lp4A+YWauZzSLcQmtSLPeuWO5Mwk1vdyLcD/FGM2uJ45LPKcsU37PATyVdTni+V0uO+Jzr1jy5ud6sIdUhaRLwNeDYWMv5E9AnwzxNie4WoCrDNACNGabJ9GiRtko/SJ7tkSWp5WU7qP6R+MzsOuCLwABgWtwmzvVIntycCwYB9cBabXlqcWd7hvAkZyTtReaaYT5nxru970xoopwN/A04L5a7GzCacGPcJ4BLE3euH5arYEkTzewNM/sP4FUg5xmiznVn2f51OtfbvAK8Rbgz+jxCE11n+x/gTklvxOW9CazJMu39kjbE7iVmlkq2cwjJbBTh+FiTpP8Bfi1pBuFu9ufH4b8mNFu+IamZ8DDQG3PE902Fp0m3Eu5A/0S719S5EvNLAZzrIgoPvKwys42xye8JYJKZNRc4/93AQ2b2v8WM07ly4DU357rOAOAvMckJ+GKhic051zZec3POOVd2/IQS55xzZceTm3POubLjyc0551zZ8eTmnHOu7Hhyc845V3b+P7m5g/hLhArfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print the traget values and the networks predictions\n",
    "print(\"Target outputs:\", target)\n",
    "print(\"Predicted outputs:\", predicted_out)\n",
    "\n",
    "# Set up the plot of the training error by epoch\n",
    "plt.figure(4)\n",
    "plt.xlabel('Training Epochs')\n",
    "plt.ylabel('Mean Square Error')\n",
    "plt.title('MLP with two hidden nodes with bias and momentum for Real estate')\n",
    "plt.plot(errorv_log)\n",
    "plt.plot(error_log)\n",
    "plt.draw()\n",
    "\n",
    "plt.show()  # keeping the plots alive until you close them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "1ZDoeuUgq0t1",
    "rO-ApF4yzD4A",
    "4Di75BregPPS",
    "AAPuFJx1gsO0",
    "EQLBv6nerI_I"
   ],
   "name": "mlp_bp_ann.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
